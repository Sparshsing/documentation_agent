{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import MarkdownNodeParser, CodeSplitter, SemanticSplitterNodeParser\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.extractors import SummaryExtractor, TitleExtractor, KeywordExtractor, DocumentContextExtractor\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_components.custom_google_genai import CustomGoogleGenAI\n",
    "from utilities import GoogleGenAIDummyTokensizer, HuggingfaceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource module not available on Windows\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.chat_engine import ContextChatEngine\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DIR = 'processed_data/data/google_genai/api'\n",
    "LLM_MODEL_PROVIDER = 'litellm'  # choose from ['litellm', 'ollama', 'gemini', 'groq']\n",
    "LLM_MODEL = \"gemini/gemini-2.5-flash-preview-05-20\" # \"cerebras/llama-3.3-70b\"  #  # \"together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\" # \"cerebras/llama-3.3-70b\"  # \"groq/llama-3.3-70b-versatile\"  # \"cerebras/llama-3.3-70b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = os.environ['GEMINI_API_KEY']\n",
    "GROQ_API_KEY = os.environ['GROQ_API_KEY']\n",
    "COHERE_API_KEY = os.environ['COHERE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = Path(PROCESSED_DIR) / 'config.json'\n",
    "with open(config_file, 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_config(config):\n",
    "    fields_to_verify = ['llm_model_provider', 'vector_store', 'chromadb_path', 'chroma_collection', 'embedding_provider', 'embedding_model', 'tokenizer_provider', 'tokenizer_model_name']\n",
    "    if type(config) != list:\n",
    "        config = [config]\n",
    "    \n",
    "    # Verify each field has consistent values across all configs\n",
    "    for field in fields_to_verify:\n",
    "        if field not in config[0]:\n",
    "            print(f\"Field {field} not found in config\")\n",
    "            return False\n",
    "            \n",
    "        first_value = config[0][field]\n",
    "        mismatches = []\n",
    "        \n",
    "        for i, cfg in enumerate(config[1:], 1):\n",
    "            if field not in cfg:\n",
    "                mismatches.append(f\"Config {i}: Field missing\")\n",
    "            elif cfg[field] != first_value:\n",
    "                mismatches.append(f\"Config {i}: {cfg[field]}\")\n",
    "                \n",
    "        if mismatches:\n",
    "            print(\"Config values are not consistent across all configs\")\n",
    "            print(f\"\\nMismatch found in {field}:\")\n",
    "            print(f\"First value: {first_value}\")\n",
    "            # for mismatch in mismatches:\n",
    "            #     print(mismatch)\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not verify_config(config):\n",
    "        raise ValueError(\"Config is not valid\")\n",
    "else:\n",
    "    if type(config) == list:\n",
    "        config = config[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_model_provider': 'litellm',\n",
       " 'llm_model': 'gemini/gemini-2.5-flash-preview-05-20',\n",
       " 'rate_limit': 7,\n",
       " 'input_dir': 'data/google_genai/api/',\n",
       " 'output_dir': 'processed_data/data/google_genai/api/',\n",
       " 'file_types': ['.md', '.mdx'],\n",
       " 'vector_store': 'chroma',\n",
       " 'chromadb_path': 'processed_data/data/google_genai/api/chromadb',\n",
       " 'chroma_collection': 'contextual_api',\n",
       " 'doctsore_path': 'processed_data/data/google_genai/api/docstore.json',\n",
       " 'embedding_provider': 'GoogleGenAIEmbedding',\n",
       " 'embedding_model': 'models/text-embedding-004',\n",
       " 'tokenizer_provider': 'tiktoken',\n",
       " 'tokenizer_model_name': 'cl100k_base',\n",
       " 'max_node_tokens': 2000,\n",
       " 'metadata_extractors': ['CustomDocumentContextExtractor'],\n",
       " 'datetime': '2025-06-10T09:59:07.250086+00:00',\n",
       " 'run_time': '2025-06-10T10:00:13.096049+00:00',\n",
       " 'run_nodes': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm(llm_model_provider, llm_model):\n",
    "    if llm_model_provider == 'groq':\n",
    "        from llama_index.llms.groq import Groq\n",
    "        return Groq(model=llm_model, api_key=GROQ_API_KEY, max_retries=2, retry_on_rate_limit=True) # Number of retry attempts\n",
    "    elif llm_model_provider == 'gemini':\n",
    "        return CustomGoogleGenAI(\n",
    "            model=llm_model,\n",
    "            api_key=GEMINI_API_KEY, \n",
    "            max_retries=2,  # Number of retry attempts\n",
    "            retry_on_rate_limit=True\n",
    "        )\n",
    "    elif llm_model_provider == 'ollama':\n",
    "        from llama_index.llms.ollama import Ollama\n",
    "        return Ollama(model=llm_model, request_timeout=120.0, context_window=8192, )\n",
    "    elif llm_model_provider == 'litellm':\n",
    "        import litellm\n",
    "        from llama_index.llms.litellm import LiteLLM\n",
    "        litellm.suppress_debug_info = True\n",
    "        return LiteLLM(model=llm_model, max_tokens=8192, max_retries=6)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"LLM provider {llm_model_provider} invalid or not implemented\")\n",
    "\n",
    "\n",
    "def get_embed_model(embedding_provider, embedding_model):\n",
    "    if embedding_provider == 'HuggingFaceEmbedding':\n",
    "        from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "        return HuggingFaceEmbedding(model_name=embedding_model)\n",
    "    elif embedding_provider == 'GoogleGenAIEmbedding':\n",
    "        from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "        return GoogleGenAIEmbedding(model_name=embedding_model, api_key=GEMINI_API_KEY)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Embedding provider {embedding_provider} invalid or not implemented\")\n",
    "    \n",
    "\n",
    "def get_tokenizer(tokenizer_provider, tokenizer_model_name, llm):\n",
    "    if tokenizer_provider == 'gemini':\n",
    "        return GoogleGenAIDummyTokensizer(llm=llm).encode\n",
    "    elif tokenizer_provider == 'huggingface':\n",
    "        return HuggingfaceTokenizer(model=tokenizer_model_name).encode\n",
    "    elif tokenizer_provider == 'tiktoken':\n",
    "        return tiktoken.get_encoding(encoding_name=tokenizer_model_name).encode\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{tokenizer_provider} invalid or not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "choma_path = config['chromadb_path']\n",
    "chroma_colection_name = config['chroma_collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import litellm\n",
    "# litellm.get_max_tokens(\"gemini/gemini-2.5-flash-preview-05-20\")\n",
    "# # litellm.get_max_tokens(\"gemini/gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = get_llm(config['llm_model_provider'], config['llm_model'])\n",
    "\n",
    "# from llama_index.core.base.llms.types import LLMMetadata\n",
    "# llm_metadata = LLMMetadata(\n",
    "#             context_window=32768, num_output=8192, is_chat_model=True, is_function_calling_model=False, model_name=llm.model,\n",
    "#         )\n",
    "\n",
    "# llm.metadata = llm_metadata\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_window': 65535,\n",
       " 'num_output': 8192,\n",
       " 'is_chat_model': True,\n",
       " 'is_function_calling_model': True,\n",
       " 'model_name': 'gemini/gemini-2.5-flash-preview-05-20',\n",
       " 'system_role': <MessageRole.SYSTEM: 'system'>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model info\n",
    "llm.metadata.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = get_embed_model(config['embedding_provider'], config['embedding_model'])\n",
    "tokenizer = get_tokenizer(config['tokenizer_provider'], config['tokenizer_model_name'], llm)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "Settings.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse client is authenticated and ready!\n"
     ]
    }
   ],
   "source": [
    "# setup observability\n",
    "from langfuse import get_client\n",
    "from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "langfuse = get_client()\n",
    " \n",
    "# Verify connection\n",
    "langfuse_available = False\n",
    "if langfuse.auth_check():\n",
    "    langfuse_available = True\n",
    "    LlamaIndexInstrumentor().instrument()\n",
    "    print(\"Langfuse client is authenticated and ready!\")\n",
    "else:\n",
    "    print(\"Authentication failed. Please check your credentials and host.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        return get_ipython() is not None\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "is_jupyter_notebook = is_notebook()\n",
    "\n",
    "    # # for jupyter notebooks - to fix event loop issue\n",
    "if is_jupyter_notebook:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# for jupyter notebook - Start ChromaDB server\n",
    "if is_jupyter_notebook:\n",
    "    import subprocess\n",
    "    process = subprocess.Popen([\"chroma\", \"run\", \"--path\", choma_path])\n",
    "    remote_db = chromadb.HttpClient()\n",
    "    chroma_collection = remote_db.get_or_create_collection(chroma_colection_name)\n",
    "else:\n",
    "    db = chromadb.PersistentClient(path=choma_path)\n",
    "    chroma_collection = db.get_or_create_collection(chroma_colection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector index loaded\n"
     ]
    }
   ],
   "source": [
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "vector_index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    embed_model=embed_model,\n",
    ")\n",
    "print('vector index loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1032"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# node count\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve(index_path, top_k=5)\n",
    "# def query(query, index_name)\n",
    "# def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_index.as_retriever(similarity_top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how to upload files for chat?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_nodes = retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK_references/Python.md-49197-55891\n",
      "0.46616804207958584\n",
      "/\n",
      "io/python-genai/genai.html#genai.caches.Caches.get)\n",
      "      * [`Caches.list()`](https://googleapis.gi\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-13534-14441\n",
      "0.4495083274471379\n",
      "/ Using files \n",
      "## Method: media.upload\n",
      "  * [Endpoint](https://ai.google.dev/api/files#body.HTTP_TEMPLATE)\n",
      "  * [Re\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-33113-33345\n",
      "0.4433611053938853\n",
      "/Files[¶](https://googleapis.github.io/python-genai/#files \"Link to this heading\")\n",
      "## Upload[¶](https://googleapis.github.io/python-genai/#upload \"Link to this heading\")\n",
      "```\n",
      "file1 =\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-11178-13532\n",
      "0.4426483252338536\n",
      "/\n",
      "#  Using files \n",
      "  * On this page\n",
      "  * [Method: media.upload](https://ai.google.dev/api/files#method\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-112891-119659\n",
      "0.4403348517596423\n",
      "/ Generating content \n",
      "curl\"${upload_url}\"\\\n",
      "-H\"Content-Length: ${NUM_BYTES}\"\\\n",
      "-H\"X-Goog-Upload-Offset: 0\"\\\n",
      "-H\"X-Goog-Upl\n",
      "--------------------\n",
      "\n",
      "Capabilities/Live_API.md-10504-11007\n",
      "0.43781577598394533\n",
      "/Send messagesebSockets API reference \n",
      "### Supported client messages\n",
      "See the supported client messages in the following table:\n",
      "Message | \n",
      "--------------------\n",
      "\n",
      "SDK_references/TypeScript.md-6251-7531\n",
      "0.4341712429640184\n",
      "/Google Gen AI SDK for TypeScript and JavaScript[](https://googleapis.github.io/js-genai/release_docs/index.html#google-gen-ai-sdk-for-typescript-and-javascript)\n",
      "## GoogleGenAI overview[](https://googleapis.github.io/js-genai/release_docs/index.html#googlegenai-\n",
      "--------------------\n",
      "\n",
      "All_methods.md-17624-17807\n",
      "0.433163019737985\n",
      "/ All methods \n",
      "## REST Resource: v1beta.media\n",
      "Methods  \n",
      "---  \n",
      "`upload[](https://ai.google.dev/api/files#v1beta.m\n",
      "--------------------\n",
      "\n",
      "SDK_references/Go.md-131815-139027\n",
      "0.4319990348364902\n",
      "/genai\n",
      "####  type [Chat](https://github.com/googleapis/go-genai/blob/v1.10.0/chats.go#L38) [¶](https://pkg.\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-14687-14884\n",
      "0.42944716210438505\n",
      "/Method: media.upload\n",
      "### Request body\n",
      "The request body contains data with the following structure:\n",
      "Fields \n",
      "`file` `obj\n",
      "--------------------\n",
      "\n",
      "Capabilities/Live_API.md-10061-11007\n",
      "0.4291795516175991\n",
      "/ Live API - WebSockets API reference \n",
      "## Send messages\n",
      "To exchange messages over the WebSocket connection, the client must send a JSON ob\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-14443-14685\n",
      "0.42699497941049386\n",
      "/Method: media.upload\n",
      "### Endpoint\n",
      "  * Upload URI, for media upload requests:\n",
      "\n",
      "post  `https://generativelanguage.google\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-13534-18865\n",
      "0.42477896388790626\n",
      "/ Using files \n",
      "## Method: media.upload\n",
      "  * [Endpoint](https://ai.google.dev/api/files#body.HTTP_TEMPLATE)\n",
      "  * [Re\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-33191-33542\n",
      "0.42444605092807275\n",
      "/Method: media.upload\n",
      "### Response body\n",
      "Response for `media.upload`.\n",
      "If successful, the response body contains data with\n",
      "--------------------\n",
      "\n",
      "Capabilities/Caching.md-32565-34569\n",
      "0.4230190640625478\n",
      "/ Caching \n",
      "// Create initial chat with a system instruction.\n",
      "chat,err:=client.Chats.Create(ctx,modelName,&gena\n",
      "--------------------\n",
      "\n",
      "SDK_references/Go.md-367781-375865\n",
      "0.42223682094474585\n",
      "/genai\n",
      "**in the order they are sent**. A conversation using [SendClientContent] is similar to using the [Ch\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-25655-31466\n",
      "0.42181050132171355\n",
      "/ Using files \n",
      "curl\"${BASE_URL}/upload/v1beta/files?key=${GEMINI_API_KEY}\"\\\n",
      "-Dupload-header.tmp\\\n",
      "-H\"X-Goog-Upload\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-33349-33542\n",
      "0.41922207544776535\n",
      "/Files[¶](https://googleapis.github.io/python-genai/#files \"Link to this heading\")\n",
      "## Get[¶](https://googleapis.github.io/python-genai/#get \"Link to this heading\")\n",
      "```\n",
      "file1 = clien\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-31468-33542\n",
      "0.41504501066197186\n",
      "/ Using files \n",
      "curl\"${upload_url}\"\\\n",
      "-H\"Content-Length: ${NUM_BYTES}\"\\\n",
      "-H\"X-Goog-Upload-Offset: 0\"\\\n",
      "-H\"X-Goog-Upl\n",
      "--------------------\n",
      "\n",
      "Capabilities/Live_API.md-14693-17060\n",
      "0.41492202851497606\n",
      "/Messages and eventsets API reference \n",
      "### BidiGenerateContentRealtimeInput\n",
      "User input that is sent in real time.\n",
      "The different modalitie\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_context = 0\n",
    "for node in retrieved_nodes:\n",
    "    print(node.node.node_id)\n",
    "    print(node.score)\n",
    "    print(node.node.metadata['header_path'])\n",
    "    print(node.node.text[:100])\n",
    "    print('-'*20)\n",
    "    print()\n",
    "    total_context += len(Settings.tokenizer(node.node.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total context: 13951 tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total context: {total_context} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# query\n",
    "if not langfuse_available:\n",
    "    response = query_engine.query(query)\n",
    "else:\n",
    "    with langfuse.start_as_current_span(name=\"vector retriever query\"):\n",
    "        response = query_engine.query(query)    \n",
    "    langfuse.flush()\n",
    "\n",
    "    # instrumentor.start()\n",
    "    # with instrumentor.observe(trace_name=\"Query vector Index\", user_id=\"user123\"):\n",
    "    #     response = query_engine.query(query)\n",
    "    # instrumentor.flush()\n",
    "    # instrumentor.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files can be uploaded using the `media.upload` method in the REST API or the `client.files.upload` method available in the Python, Node.js, Go, and TypeScript SDKs. This process creates a `File` resource.\n",
      "\n",
      "Once a file is uploaded, its URI and MIME type can be included as part of a message in a chat session. For example, in the Go SDK, an uploaded file's URI and MIME type can be used to create a `genai.Part` object, which is then passed to a chat method like `chat.SendMessage`.\n",
      "\n",
      "Additionally, for real-time interactions through the Live API, the `Session.SendClientContent` method (or `BidiGenerateContentClientContent` in the API reference) allows sending content objects that can include file data. This enables incorporating files into a conversation where the history is managed by the API server, similar to a standard chat message.\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Based Retriever - BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_info = chroma_collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'metadatas', 'documents', 'data', 'uris', 'included'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_node_content': '{\"id_\": \"__root__.md-0-3432\", \"embedding\": null, \"metadata\": {\"file_path\": \"__root__.md\", \"file_name\": \"__root__.md\", \"file_size\": 11202, \"creation_date\": \"2025-06-06\", \"last_modified_date\": \"2025-06-06\", \"header_path\": \"/\", \"context\": \"This chunk appears to be the navigation menu or sidebar of the Google AI for Developers documentation page, specifically for the Gemini API.\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"__root__.md\", \"node_type\": \"4\", \"metadata\": {\"file_path\": \"__root__.md\", \"file_name\": \"__root__.md\", \"file_size\": 11202, \"creation_date\": \"2025-06-06\", \"last_modified_date\": \"2025-06-06\"}, \"hash\": \"1125ed6c9622fbc84faec0fc0351beccb9cdaa79c76494cf5de1b86667d413ad\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"0de26304-c8d9-461a-9eb6-624f73a9df2e\", \"node_type\": \"1\", \"metadata\": {\"header_path\": \"/\"}, \"hash\": \"63d2b19c3406663f76b2ebcc6f08b277f0eb0e5eea255333d8a54dacca39380d\", \"class_name\": \"RelatedNodeInfo\"}}, \"metadata_template\": \"{key}: {value}\", \"metadata_separator\": \"\\\\n\", \"text\": \"\", \"mimetype\": \"text/plain\", \"start_char_idx\": 0, \"end_char_idx\": 3432, \"metadata_seperator\": \"\\\\n\", \"text_template\": \"[Excerpt from document]\\\\n{metadata_str}\\\\nExcerpt:\\\\n-----\\\\n{content}\\\\n-----\\\\n\", \"class_name\": \"TextNode\"}',\n",
       " '_node_type': 'TextNode',\n",
       " 'context': 'This chunk appears to be the navigation menu or sidebar of the Google AI for Developers documentation page, specifically for the Gemini API.',\n",
       " 'creation_date': '2025-06-06',\n",
       " 'doc_id': '__root__.md',\n",
       " 'document_id': '__root__.md',\n",
       " 'file_name': '__root__.md',\n",
       " 'file_path': '__root__.md',\n",
       " 'file_size': 11202,\n",
       " 'header_path': '/',\n",
       " 'last_modified_date': '2025-06-06',\n",
       " 'ref_doc_id': '__root__.md'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(nodes_info['metadatas'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_nodes = vector_store.get_nodes(nodes_info['ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever = BM25Retriever.from_defaults(nodes=all_nodes, similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retrieved_nodes = bm25_retriever.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capabilities/Generating_content.md-112891-119659\n",
      "6.043334007263184\n",
      "/ Generating content \n",
      "curl\"${upload_url}\"\\\n",
      "-H\"Content-Length: ${NUM_BYTES}\"\\\n",
      "-H\"X-Goog-Upload-Offset: 0\"\\\n",
      "-H\"X-Goog-Upl\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-30933-38003\n",
      "5.922059059143066\n",
      "/ Generating content \n",
      "forfile.State==genai.FileStateUnspecified||file.State!=genai.FileStateActive{\n",
      "fmt.Println(\"Processi\n",
      "--------------------\n",
      "\n",
      "Tuning/Tuning.md-128539-135473\n",
      "5.844338417053223\n",
      "/ Tuning \n",
      "MIME_TYPE=$(file-b--mime-type\"${VIDEO_PATH}\")\n",
      "NUM_BYTES=$(wc-c < \"${VIDEO_PATH}\")\n",
      "DISPLAY_NAME=VID\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-38003-45486\n",
      "4.997640609741211\n",
      "/ Generating content \n",
      "\",\"model\"),\n",
      "}\n",
      "chat,err:=client.Chats.Create(ctx,\"gemini-2.0-flash\",nil,history)\n",
      "iferr!=nil{\n",
      "log.\n",
      "--------------------\n",
      "\n",
      "Tuning/Tuning.md-43445-50537\n",
      "4.770259857177734\n",
      "/ Tuning \n",
      "while(!video.state||video.state.toString()!=='ACTIVE'){\n",
      "console.log('Processing video...');\n",
      "consol\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-49197-55891\n",
      "4.731428146362305\n",
      "/\n",
      "io/python-genai/genai.html#genai.caches.Caches.get)\n",
      "      * [`Caches.list()`](https://googleapis.gi\n",
      "--------------------\n",
      "\n",
      "Capabilities/Caching.md-28429-34409\n",
      "4.695282936096191\n",
      "/Method: cachedContents.create\n",
      "cache,err=client.Caches.Get(ctx,cacheName,&genai.GetCachedContentConfig{})\n",
      "iferr!=nil{\n",
      "log.Fatal(e\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-33113-33345\n",
      "4.562509059906006\n",
      "/Files[¶](https://googleapis.github.io/python-genai/#files \"Link to this heading\")\n",
      "## Upload[¶](https://googleapis.github.io/python-genai/#upload \"Link to this heading\")\n",
      "```\n",
      "file1 =\n",
      "--------------------\n",
      "\n",
      "Capabilities/Caching.md-24637-32563\n",
      "4.542459487915039\n",
      "/ Caching \n",
      "}\n",
      "  ]\n",
      " },\n",
      " \"ttl\": \"300s\"\n",
      "}' > request.json\n",
      "curl-XPOST\"https://generativelanguage.googleapis.com\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-33349-33542\n",
      "4.387411594390869\n",
      "/Files[¶](https://googleapis.github.io/python-genai/#files \"Link to this heading\")\n",
      "## Get[¶](https://googleapis.github.io/python-genai/#get \"Link to this heading\")\n",
      "```\n",
      "file1 = clien\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in bm25_retrieved_nodes:\n",
    "    print(node.node.node_id)\n",
    "    print(node.score)\n",
    "    print(node.node.metadata['header_path'])\n",
    "    print(node.node.text[:100])\n",
    "    print('-'*20)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble query engine\n",
    "bm25_query_engine = RetrieverQueryEngine(\n",
    "    retriever=bm25_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "bm25_response = bm25_query_engine.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use files within a chat, you first need to upload the file. This can be done using a method like `client.files.upload()`, which will provide details about the uploaded file, including its URI and MIME type.\n",
      "\n",
      "Once the file is uploaded and its information is available, you can then include it as part of the content when sending a message in a chat session. For example, when calling `chat.send_message()`, you would specify the text of your message along with the `file_data` containing the `mime_type` and `file_uri` of the uploaded file.\n"
     ]
    }
   ],
   "source": [
    "print(bm25_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hybrid retriever\n",
    "\n",
    "\n",
    "# class HybridRetriever(BaseRetriever):\n",
    "# def __init__(self, vector_retriever, bm25_retriever):\n",
    "# self.vector_retriever = vector_retriever\n",
    "# self.bm25_retriever = bm25_retriever\n",
    "\n",
    "# def _retrieve(self, query, **kwargs):\n",
    "# bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "# vector_nodes = self.vector_retriever.retrieve(query, **kwargs)\n",
    "# all_nodes = []\n",
    "# node_ids = set()\n",
    "# for n in bm25_nodes + vector_nodes:\n",
    "# if n.node.node_id not in node_ids:\n",
    "# all_nodes.append(n)\n",
    "# node_ids.add(n.node.node_id)\n",
    "# return all_nodes\n",
    "\n",
    "# hybrid_retriever = HybridRetriever(vector_retriever, bm25_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_retriever = QueryFusionRetriever(\n",
    "    [\n",
    "        retriever,\n",
    "        bm25_retriever,\n",
    "    ],\n",
    "    num_queries=4,\n",
    "    similarity_top_k = 40,\n",
    "    llm=llm,\n",
    "    retriever_weights=[0.7, 0.3],\n",
    "    mode = \"dist_based_score\",\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not langfuse_available:\n",
    "    hybrid_nodes = await hybrid_retriever.aretrieve(query)\n",
    "else:\n",
    "    with langfuse.start_as_current_span(name=\"hybrid retriever retrieve\"):\n",
    "        hybrid_nodes = await hybrid_retriever.aretrieve(query)\n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capabilities/Generating_content.md-112891-119659\n",
      "0.6377292836136748\n",
      "/ Generating content \n",
      "curl\"${upload_url}\"\\\n",
      "-H\"Content-Length: ${NUM_BYTES}\"\\\n",
      "-H\"X-Goog-Upload-Offset: 0\"\\\n",
      "-H\"X-Goog-Upl\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-49197-55891\n",
      "0.5566185045992408\n",
      "/\n",
      "io/python-genai/genai.html#genai.caches.Caches.get)\n",
      "      * [`Caches.list()`](https://googleapis.gi\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-13534-14441\n",
      "0.44560408168886334\n",
      "/ Using files \n",
      "## Method: media.upload\n",
      "  * [Endpoint](https://ai.google.dev/api/files#body.HTTP_TEMPLATE)\n",
      "  * [Re\n",
      "--------------------\n",
      "\n",
      "SDK_references/Go.md-131815-139027\n",
      "0.4271812618126451\n",
      "/genai\n",
      "####  type [Chat](https://github.com/googleapis/go-genai/blob/v1.10.0/chats.go#L38) [¶](https://pkg.\n",
      "--------------------\n",
      "\n",
      "Capabilities/Live_API.md-10504-11007\n",
      "0.3578793920492212\n",
      "/Send messagesebSockets API reference \n",
      "### Supported client messages\n",
      "See the supported client messages in the following table:\n",
      "Message | \n",
      "--------------------\n",
      "\n",
      "Capabilities/Caching.md-32565-34569\n",
      "0.348034525466761\n",
      "/ Caching \n",
      "// Create initial chat with a system instruction.\n",
      "chat,err:=client.Chats.Create(ctx,modelName,&gena\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-33113-33345\n",
      "0.29787802781866296\n",
      "/Files[¶](https://googleapis.github.io/python-genai/#files \"Link to this heading\")\n",
      "## Upload[¶](https://googleapis.github.io/python-genai/#upload \"Link to this heading\")\n",
      "```\n",
      "file1 =\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-36016-36024\n",
      "0.27809983974744\n",
      "/Method: models.generateContent\n",
      "### Chat\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-11178-13532\n",
      "0.2520385870007634\n",
      "/\n",
      "#  Using files \n",
      "  * On this page\n",
      "  * [Method: media.upload](https://ai.google.dev/api/files#method\n",
      "--------------------\n",
      "\n",
      "SDK_references/Go.md-367781-375865\n",
      "0.2520024166659487\n",
      "/genai\n",
      "**in the order they are sent**. A conversation using [SendClientContent] is similar to using the [Ch\n",
      "--------------------\n",
      "\n",
      "Tuning/Tuning.md-128539-135473\n",
      "0.2516451281250523\n",
      "/ Tuning \n",
      "MIME_TYPE=$(file-b--mime-type\"${VIDEO_PATH}\")\n",
      "NUM_BYTES=$(wc-c < \"${VIDEO_PATH}\")\n",
      "DISPLAY_NAME=VID\n",
      "--------------------\n",
      "\n",
      "SDK_references/TypeScript.md-6251-7531\n",
      "0.24464727167624428\n",
      "/Google Gen AI SDK for TypeScript and JavaScript[](https://googleapis.github.io/js-genai/release_docs/index.html#google-gen-ai-sdk-for-typescript-and-javascript)\n",
      "## GoogleGenAI overview[](https://googleapis.github.io/js-genai/release_docs/index.html#googlegenai-\n",
      "--------------------\n",
      "\n",
      "All_methods.md-17624-17807\n",
      "0.23634453553871465\n",
      "/ All methods \n",
      "## REST Resource: v1beta.media\n",
      "Methods  \n",
      "---  \n",
      "`upload[](https://ai.google.dev/api/files#v1beta.m\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-30933-38003\n",
      "0.23464739901635034\n",
      "/ Generating content \n",
      "forfile.State==genai.FileStateUnspecified||file.State!=genai.FileStateActive{\n",
      "fmt.Println(\"Processi\n",
      "--------------------\n",
      "\n",
      "Capabilities/Caching.md-28429-34409\n",
      "0.2320866655994635\n",
      "/Method: cachedContents.create\n",
      "cache,err=client.Caches.Get(ctx,cacheName,&genai.GetCachedContentConfig{})\n",
      "iferr!=nil{\n",
      "log.Fatal(e\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-32392-32753\n",
      "0.21892444365870062\n",
      "/Chats[¶](https://googleapis.github.io/python-genai/#chats \"Link to this heading\")\n",
      "## Send Message (Asynchronous Streaming)[¶](https://googleapis.github.io/python-genai/#send-message-\n",
      "--------------------\n",
      "\n",
      "Capabilities/Tokens.md-17120-17128\n",
      "0.21348721514402308\n",
      "/Method: models.countTokens\n",
      "### Chat\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-32083-32388\n",
      "0.19785400588624813\n",
      "/Chats[¶](https://googleapis.github.io/python-genai/#chats \"Link to this heading\")\n",
      "## Send Message (Asynchronous Non-Streaming)[¶](https://googleapis.github.io/python-genai/#send-mess\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-31736-32079\n",
      "0.1889089976283066\n",
      "/Chats[¶](https://googleapis.github.io/python-genai/#chats \"Link to this heading\")\n",
      "## Send Message (Synchronous Streaming)[¶](https://googleapis.github.io/python-genai/#send-message-s\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-14687-14884\n",
      "0.18415880564779882\n",
      "/Method: media.upload\n",
      "### Request body\n",
      "The request body contains data with the following structure:\n",
      "Fields \n",
      "`file` `obj\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-14443-14685\n",
      "0.1794814038505761\n",
      "/Method: media.upload\n",
      "### Endpoint\n",
      "  * Upload URI, for media upload requests:\n",
      "\n",
      "post  `https://generativelanguage.google\n",
      "--------------------\n",
      "\n",
      "Capabilities/Live_API.md-10061-11007\n",
      "0.1687200323104677\n",
      "/ Live API - WebSockets API reference \n",
      "## Send messages\n",
      "To exchange messages over the WebSocket connection, the client must send a JSON ob\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-31338-31732\n",
      "0.14481420360758282\n",
      "/Chats[¶](https://googleapis.github.io/python-genai/#chats \"Link to this heading\")\n",
      "## Send Message (Synchronous Non-Streaming)[¶](https://googleapis.github.io/python-genai/#send-messa\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-33191-33542\n",
      "0.14072992902228093\n",
      "/Method: media.upload\n",
      "### Response body\n",
      "Response for `media.upload`.\n",
      "If successful, the response body contains data with\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-33349-33542\n",
      "0.13697819546008932\n",
      "/Files[¶](https://googleapis.github.io/python-genai/#files \"Link to this heading\")\n",
      "## Get[¶](https://googleapis.github.io/python-genai/#get \"Link to this heading\")\n",
      "```\n",
      "file1 = clien\n",
      "--------------------\n",
      "\n",
      "Capabilities/Tokens.md-26306-26322\n",
      "0.134816893812552\n",
      "/Method: models.countTokens\n",
      "### Inline media\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-13534-18865\n",
      "0.1332532864873731\n",
      "/ Using files \n",
      "## Method: media.upload\n",
      "  * [Endpoint](https://ai.google.dev/api/files#body.HTTP_TEMPLATE)\n",
      "  * [Re\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-37609-38546\n",
      "0.10921181245329811\n",
      "/Method: models.generateContent\n",
      "### Go\n",
      "```\n",
      "ctx:=context.Background()\n",
      "client,err:=genai.NewClient(ctx,&genai.ClientConfig{\n",
      "APIKey\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-34286-36012\n",
      "0.10631510890876375\n",
      "/Method: models.generateContent\n",
      "### Shell\n",
      "```\n",
      "MIME_TYPE=$(file-b--mime-type\"${PDF_PATH}\")\n",
      "NUM_BYTES=$(wc-c < \"${PDF_PATH}\")\n",
      "DISP\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-31468-33542\n",
      "0.10553914868439214\n",
      "/ Using files \n",
      "curl\"${upload_url}\"\\\n",
      "-H\"Content-Length: ${NUM_BYTES}\"\\\n",
      "-H\"X-Goog-Upload-Offset: 0\"\\\n",
      "-H\"X-Goog-Upl\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-114548-116235\n",
      "0.09793544008816621\n",
      "/Method: models.streamGenerateContent\n",
      "### Shell\n",
      "```\n",
      "MIME_TYPE=$(file-b--mime-type\"${PDF_PATH}\")\n",
      "NUM_BYTES=$(wc-c < \"${PDF_PATH}\")\n",
      "DISP\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-30941-31336\n",
      "0.07655147527289081\n",
      "/\n",
      "# Chats[¶](https://googleapis.github.io/python-genai/#chats \"Link to this heading\")\n",
      "Create a chat s\n",
      "--------------------\n",
      "\n",
      "SDK_references/Go.md-408323-412613\n",
      "0.06942022700227833\n",
      "/genai\n",
      "##  ![](https://pkg.go.dev/static/shared/icon/insert_drive_file_gm_grey_24dp.svg) Source Files [¶](h\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-25655-31466\n",
      "0.06436178841023012\n",
      "/ Using files \n",
      "curl\"${BASE_URL}/upload/v1beta/files?key=${GEMINI_API_KEY}\"\\\n",
      "-Dupload-header.tmp\\\n",
      "-H\"X-Goog-Upload\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-38003-45486\n",
      "0.06219883687904042\n",
      "/ Generating content \n",
      "\",\"model\"),\n",
      "}\n",
      "chat,err:=client.Chats.Create(ctx,\"gemini-2.0-flash\",nil,history)\n",
      "iferr!=nil{\n",
      "log.\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-90175-91516\n",
      "0.06124725045941198\n",
      "/ Generating content \n",
      "## Method: models.streamGenerateContent\n",
      "  * [Endpoint](https://ai.google.dev/api/generate-content#b\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-119158-119857\n",
      "0.05553303826420443\n",
      "/Method: models.streamGenerateContent\n",
      "### Shell\n",
      "```\n",
      "curlhttps://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamG\n",
      "--------------------\n",
      "\n",
      "Capabilities/Live_API.md-8459-8784\n",
      "0.05509000212085613\n",
      "/ Live API - WebSockets API reference \n",
      "## Sessions\n",
      "A WebSocket connection establishes a session between the client and the Gemini server. \n",
      "--------------------\n",
      "\n",
      "SDK_references/Go.md-14345-21402\n",
      "0.05471519261633488\n",
      "/genai\n",
      "parts\\)\")\n",
      "        * [ (c) SendStream(ctx, parts) ](https://pkg.go.dev/google.golang.org/genai#Chat.\n",
      "--------------------\n",
      "\n",
      "Tuning/Tuning.md-43445-50537\n",
      "0.05458176474366877\n",
      "/ Tuning \n",
      "while(!video.state||video.state.toString()!=='ACTIVE'){\n",
      "console.log('Processing video...');\n",
      "consol\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in hybrid_nodes:\n",
    "    print(node.node.node_id)\n",
    "    print(node.score)\n",
    "    print(node.node.metadata['header_path'])\n",
    "    print(node.node.text[:100])\n",
    "    print('-'*20)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_query_engine = RetrieverQueryEngine(hybrid_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not langfuse_available:\n",
    "    hybrid_resp = hybrid_query_engine.query(query)\n",
    "else:\n",
    "    with langfuse.start_as_current_span(name=\"hybrid retriever retrieve\"):\n",
    "        hybrid_resp = hybrid_query_engine.query(query)\n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To upload files for use in a chat, you first need to upload the file to the API. This process creates a `File` resource.\n",
      "\n",
      "Here's the general approach:\n",
      "1.  **Upload the file**: Use the `media.upload` method or the equivalent SDK function (e.g., `client.files.upload` in Python). This method allows you to upload various media types such as images, audio, text, video, and PDF files.\n",
      "2.  **Obtain the file URI/name**: After a successful upload, the API returns information about the created file, including its URI or name.\n",
      "3.  **Reference the file in your chat message**: When sending a message in a chat session, include the uploaded file's URI and MIME type as part of the content. For example, in Python, you would upload the file and then pass the file object directly into the `send_message` call along with your text.\n",
      "\n",
      "This allows the model to process the file content as part of the conversation.\n"
     ]
    }
   ],
   "source": [
    "print(hybrid_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hybrid_resp.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capabilities/Generating_content.md-112891-119659\n",
      "0.6377292836136748\n",
      "/ Generating content \n",
      "curl\"${upload_url}\"\\\n",
      "-H\"Content-Length: ${NUM_BYTES}\"\\\n",
      "-H\"X-Goog-Upload-Offset: 0\"\\\n",
      "-H\"X-Goog-Upl\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-49197-55891\n",
      "0.5566185045992408\n",
      "/\n",
      "io/python-genai/genai.html#genai.caches.Caches.get)\n",
      "      * [`Caches.list()`](https://googleapis.gi\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-13534-14441\n",
      "0.44560408168886334\n",
      "/ Using files \n",
      "## Method: media.upload\n",
      "  * [Endpoint](https://ai.google.dev/api/files#body.HTTP_TEMPLATE)\n",
      "  * [Re\n",
      "--------------------\n",
      "\n",
      "SDK_references/Go.md-131815-139027\n",
      "0.4271812618126451\n",
      "/genai\n",
      "####  type [Chat](https://github.com/googleapis/go-genai/blob/v1.10.0/chats.go#L38) [¶](https://pkg.\n",
      "--------------------\n",
      "\n",
      "Capabilities/Live_API.md-10504-11007\n",
      "0.3578793920492212\n",
      "/Send messagesebSockets API reference \n",
      "### Supported client messages\n",
      "See the supported client messages in the following table:\n",
      "Message | \n",
      "--------------------\n",
      "\n",
      "Capabilities/Caching.md-32565-34569\n",
      "0.348034525466761\n",
      "/ Caching \n",
      "// Create initial chat with a system instruction.\n",
      "chat,err:=client.Chats.Create(ctx,modelName,&gena\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-33113-33345\n",
      "0.29787802781866296\n",
      "/Files[¶](https://googleapis.github.io/python-genai/#files \"Link to this heading\")\n",
      "## Upload[¶](https://googleapis.github.io/python-genai/#upload \"Link to this heading\")\n",
      "```\n",
      "file1 =\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-36016-36024\n",
      "0.27809983974744\n",
      "/Method: models.generateContent\n",
      "### Chat\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-11178-13532\n",
      "0.2520385870007634\n",
      "/\n",
      "#  Using files \n",
      "  * On this page\n",
      "  * [Method: media.upload](https://ai.google.dev/api/files#method\n",
      "--------------------\n",
      "\n",
      "SDK_references/Go.md-367781-375865\n",
      "0.2520024166659487\n",
      "/genai\n",
      "**in the order they are sent**. A conversation using [SendClientContent] is similar to using the [Ch\n",
      "--------------------\n",
      "\n",
      "Tuning/Tuning.md-128539-135473\n",
      "0.2516451281250523\n",
      "/ Tuning \n",
      "MIME_TYPE=$(file-b--mime-type\"${VIDEO_PATH}\")\n",
      "NUM_BYTES=$(wc-c < \"${VIDEO_PATH}\")\n",
      "DISPLAY_NAME=VID\n",
      "--------------------\n",
      "\n",
      "SDK_references/TypeScript.md-6251-7531\n",
      "0.24464727167624428\n",
      "/Google Gen AI SDK for TypeScript and JavaScript[](https://googleapis.github.io/js-genai/release_docs/index.html#google-gen-ai-sdk-for-typescript-and-javascript)\n",
      "## GoogleGenAI overview[](https://googleapis.github.io/js-genai/release_docs/index.html#googlegenai-\n",
      "--------------------\n",
      "\n",
      "All_methods.md-17624-17807\n",
      "0.23634453553871465\n",
      "/ All methods \n",
      "## REST Resource: v1beta.media\n",
      "Methods  \n",
      "---  \n",
      "`upload[](https://ai.google.dev/api/files#v1beta.m\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-30933-38003\n",
      "0.23464739901635034\n",
      "/ Generating content \n",
      "forfile.State==genai.FileStateUnspecified||file.State!=genai.FileStateActive{\n",
      "fmt.Println(\"Processi\n",
      "--------------------\n",
      "\n",
      "Capabilities/Caching.md-28429-34409\n",
      "0.2320866655994635\n",
      "/Method: cachedContents.create\n",
      "cache,err=client.Caches.Get(ctx,cacheName,&genai.GetCachedContentConfig{})\n",
      "iferr!=nil{\n",
      "log.Fatal(e\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-32392-32753\n",
      "0.21892444365870062\n",
      "/Chats[¶](https://googleapis.github.io/python-genai/#chats \"Link to this heading\")\n",
      "## Send Message (Asynchronous Streaming)[¶](https://googleapis.github.io/python-genai/#send-message-\n",
      "--------------------\n",
      "\n",
      "Capabilities/Tokens.md-17120-17128\n",
      "0.21348721514402308\n",
      "/Method: models.countTokens\n",
      "### Chat\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-32083-32388\n",
      "0.19785400588624813\n",
      "/Chats[¶](https://googleapis.github.io/python-genai/#chats \"Link to this heading\")\n",
      "## Send Message (Asynchronous Non-Streaming)[¶](https://googleapis.github.io/python-genai/#send-mess\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-31736-32079\n",
      "0.1889089976283066\n",
      "/Chats[¶](https://googleapis.github.io/python-genai/#chats \"Link to this heading\")\n",
      "## Send Message (Synchronous Streaming)[¶](https://googleapis.github.io/python-genai/#send-message-s\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-14687-14884\n",
      "0.18415880564779882\n",
      "/Method: media.upload\n",
      "### Request body\n",
      "The request body contains data with the following structure:\n",
      "Fields \n",
      "`file` `obj\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-14443-14685\n",
      "0.1794814038505761\n",
      "/Method: media.upload\n",
      "### Endpoint\n",
      "  * Upload URI, for media upload requests:\n",
      "\n",
      "post  `https://generativelanguage.google\n",
      "--------------------\n",
      "\n",
      "Capabilities/Live_API.md-10061-11007\n",
      "0.1687200323104677\n",
      "/ Live API - WebSockets API reference \n",
      "## Send messages\n",
      "To exchange messages over the WebSocket connection, the client must send a JSON ob\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-31338-31732\n",
      "0.14481420360758282\n",
      "/Chats[¶](https://googleapis.github.io/python-genai/#chats \"Link to this heading\")\n",
      "## Send Message (Synchronous Non-Streaming)[¶](https://googleapis.github.io/python-genai/#send-messa\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-33191-33542\n",
      "0.14072992902228093\n",
      "/Method: media.upload\n",
      "### Response body\n",
      "Response for `media.upload`.\n",
      "If successful, the response body contains data with\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-33349-33542\n",
      "0.13697819546008932\n",
      "/Files[¶](https://googleapis.github.io/python-genai/#files \"Link to this heading\")\n",
      "## Get[¶](https://googleapis.github.io/python-genai/#get \"Link to this heading\")\n",
      "```\n",
      "file1 = clien\n",
      "--------------------\n",
      "\n",
      "Capabilities/Tokens.md-26306-26322\n",
      "0.134816893812552\n",
      "/Method: models.countTokens\n",
      "### Inline media\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-13534-18865\n",
      "0.1332532864873731\n",
      "/ Using files \n",
      "## Method: media.upload\n",
      "  * [Endpoint](https://ai.google.dev/api/files#body.HTTP_TEMPLATE)\n",
      "  * [Re\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-37609-38546\n",
      "0.10921181245329811\n",
      "/Method: models.generateContent\n",
      "### Go\n",
      "```\n",
      "ctx:=context.Background()\n",
      "client,err:=genai.NewClient(ctx,&genai.ClientConfig{\n",
      "APIKey\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-34286-36012\n",
      "0.10631510890876375\n",
      "/Method: models.generateContent\n",
      "### Shell\n",
      "```\n",
      "MIME_TYPE=$(file-b--mime-type\"${PDF_PATH}\")\n",
      "NUM_BYTES=$(wc-c < \"${PDF_PATH}\")\n",
      "DISP\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-31468-33542\n",
      "0.10553914868439214\n",
      "/ Using files \n",
      "curl\"${upload_url}\"\\\n",
      "-H\"Content-Length: ${NUM_BYTES}\"\\\n",
      "-H\"X-Goog-Upload-Offset: 0\"\\\n",
      "-H\"X-Goog-Upl\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-114548-116235\n",
      "0.09793544008816621\n",
      "/Method: models.streamGenerateContent\n",
      "### Shell\n",
      "```\n",
      "MIME_TYPE=$(file-b--mime-type\"${PDF_PATH}\")\n",
      "NUM_BYTES=$(wc-c < \"${PDF_PATH}\")\n",
      "DISP\n",
      "--------------------\n",
      "\n",
      "SDK_references/Python.md-30941-31336\n",
      "0.07655147527289081\n",
      "/\n",
      "# Chats[¶](https://googleapis.github.io/python-genai/#chats \"Link to this heading\")\n",
      "Create a chat s\n",
      "--------------------\n",
      "\n",
      "SDK_references/Go.md-408323-412613\n",
      "0.06942022700227833\n",
      "/genai\n",
      "##  ![](https://pkg.go.dev/static/shared/icon/insert_drive_file_gm_grey_24dp.svg) Source Files [¶](h\n",
      "--------------------\n",
      "\n",
      "Capabilities/Files.md-25655-31466\n",
      "0.06436178841023012\n",
      "/ Using files \n",
      "curl\"${BASE_URL}/upload/v1beta/files?key=${GEMINI_API_KEY}\"\\\n",
      "-Dupload-header.tmp\\\n",
      "-H\"X-Goog-Upload\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-38003-45486\n",
      "0.06219883687904042\n",
      "/ Generating content \n",
      "\",\"model\"),\n",
      "}\n",
      "chat,err:=client.Chats.Create(ctx,\"gemini-2.0-flash\",nil,history)\n",
      "iferr!=nil{\n",
      "log.\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-90175-91516\n",
      "0.06124725045941198\n",
      "/ Generating content \n",
      "## Method: models.streamGenerateContent\n",
      "  * [Endpoint](https://ai.google.dev/api/generate-content#b\n",
      "--------------------\n",
      "\n",
      "Capabilities/Generating_content.md-119158-119857\n",
      "0.05553303826420443\n",
      "/Method: models.streamGenerateContent\n",
      "### Shell\n",
      "```\n",
      "curlhttps://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamG\n",
      "--------------------\n",
      "\n",
      "Capabilities/Live_API.md-8459-8784\n",
      "0.05509000212085613\n",
      "/ Live API - WebSockets API reference \n",
      "## Sessions\n",
      "A WebSocket connection establishes a session between the client and the Gemini server. \n",
      "--------------------\n",
      "\n",
      "SDK_references/Go.md-14345-21402\n",
      "0.05471519261633488\n",
      "/genai\n",
      "parts\\)\")\n",
      "        * [ (c) SendStream(ctx, parts) ](https://pkg.go.dev/google.golang.org/genai#Chat.\n",
      "--------------------\n",
      "\n",
      "Tuning/Tuning.md-43445-50537\n",
      "0.05458176474366877\n",
      "/ Tuning \n",
      "while(!video.state||video.state.toString()!=='ACTIVE'){\n",
      "console.log('Processing video...');\n",
      "consol\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for node in hybrid_resp.source_nodes:\n",
    "    print(node.node.node_id)\n",
    "    print(node.score)\n",
    "    print(node.node.metadata['header_path'])\n",
    "    print(node.node.text[:100])\n",
    "    print('-'*20)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohere_rerank = CohereRerank(\n",
    "    top_n=10, model=\"rerank-v3.5\", api_key=COHERE_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_nodes = cohere_rerank.postprocess_nodes(nodes=hybrid_nodes, query_str=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK_references/Python.md-49197-55891 0.59457535\n",
      "Tuning/Tuning.md-128539-135473 0.5377946\n",
      "Capabilities/Generating_content.md-30933-38003 0.5271467\n",
      "Capabilities/Generating_content.md-112891-119659 0.5192538\n",
      "Tuning/Tuning.md-43445-50537 0.48147875\n",
      "Capabilities/Generating_content.md-38003-45486 0.45551404\n",
      "Capabilities/Caching.md-28429-34409 0.41577685\n",
      "SDK_references/Go.md-14345-21402 0.40584955\n",
      "SDK_references/TypeScript.md-6251-7531 0.39508808\n",
      "Capabilities/Files.md-11178-13532 0.33561262\n"
     ]
    }
   ],
   "source": [
    "for node in sorted_nodes:\n",
    "    print(node.node.node_id, node.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capabilities/Generating_content.md-112891-119659 0.6377292836136748\n",
      "SDK_references/Python.md-49197-55891 0.5566185045992408\n",
      "Capabilities/Files.md-13534-14441 0.44560408168886334\n",
      "SDK_references/Go.md-131815-139027 0.4271812618126451\n",
      "Capabilities/Live_API.md-10504-11007 0.3578793920492212\n",
      "Capabilities/Caching.md-32565-34569 0.348034525466761\n",
      "SDK_references/Python.md-33113-33345 0.29787802781866296\n",
      "Capabilities/Generating_content.md-36016-36024 0.27809983974744\n",
      "Capabilities/Files.md-11178-13532 0.2520385870007634\n",
      "SDK_references/Go.md-367781-375865 0.2520024166659487\n",
      "Tuning/Tuning.md-128539-135473 0.2516451281250523\n",
      "SDK_references/TypeScript.md-6251-7531 0.24464727167624428\n",
      "All_methods.md-17624-17807 0.23634453553871465\n",
      "Capabilities/Generating_content.md-30933-38003 0.23464739901635034\n",
      "Capabilities/Caching.md-28429-34409 0.2320866655994635\n",
      "SDK_references/Python.md-32392-32753 0.21892444365870062\n",
      "Capabilities/Tokens.md-17120-17128 0.21348721514402308\n",
      "SDK_references/Python.md-32083-32388 0.19785400588624813\n",
      "SDK_references/Python.md-31736-32079 0.1889089976283066\n",
      "Capabilities/Files.md-14687-14884 0.18415880564779882\n",
      "Capabilities/Files.md-14443-14685 0.1794814038505761\n",
      "Capabilities/Live_API.md-10061-11007 0.1687200323104677\n",
      "SDK_references/Python.md-31338-31732 0.14481420360758282\n",
      "Capabilities/Files.md-33191-33542 0.14072992902228093\n",
      "SDK_references/Python.md-33349-33542 0.13697819546008932\n",
      "Capabilities/Tokens.md-26306-26322 0.134816893812552\n",
      "Capabilities/Files.md-13534-18865 0.1332532864873731\n",
      "Capabilities/Generating_content.md-37609-38546 0.10921181245329811\n",
      "Capabilities/Generating_content.md-34286-36012 0.10631510890876375\n",
      "Capabilities/Files.md-31468-33542 0.10553914868439214\n",
      "Capabilities/Generating_content.md-114548-116235 0.09793544008816621\n",
      "SDK_references/Python.md-30941-31336 0.07655147527289081\n",
      "SDK_references/Go.md-408323-412613 0.06942022700227833\n",
      "Capabilities/Files.md-25655-31466 0.06436178841023012\n",
      "Capabilities/Generating_content.md-38003-45486 0.06219883687904042\n",
      "Capabilities/Generating_content.md-90175-91516 0.06124725045941198\n",
      "Capabilities/Generating_content.md-119158-119857 0.05553303826420443\n",
      "Capabilities/Live_API.md-8459-8784 0.05509000212085613\n",
      "SDK_references/Go.md-14345-21402 0.05471519261633488\n",
      "Tuning/Tuning.md-43445-50537 0.05458176474366877\n"
     ]
    }
   ],
   "source": [
    "for node in hybrid_nodes:\n",
    "    print(node.node.node_id, node.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers.type import ResponseMode\n",
    "response_synthesizer = get_response_synthesizer(llm=llm, response_mode=ResponseMode.COMPACT)\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle, QueryType\n",
    "query_bundle = QueryBundle(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not langfuse_available:\n",
    "    reranked_response = response_synthesizer.synthesize(query_bundle, sorted_nodes)\n",
    "else:\n",
    "    with langfuse.start_as_current_span(name=\"reranked query\"):\n",
    "        reranked_response = response_synthesizer.synthesize(query_bundle, sorted_nodes)\n",
    "    langfuse.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To upload files for use in a chat, you first use the `files` client or module to upload the file. Once the file is uploaded, you can then include the uploaded file object or its URI as part of the content when sending a message in a chat session.\n",
      "\n",
      "For example:\n",
      "\n",
      "*   **In Python:**\n",
      "    1.  Upload the file: `document = client.files.upload(file=media / \"your_file.txt\")`\n",
      "    2.  Send it in a chat message: `response = chat.send_message(message=[\"Your prompt text here\", document])`\n",
      "\n",
      "*   **In Node.js:**\n",
      "    1.  Upload the file: `const document = await ai.files.upload({ file: filePath, config: { mimeType: \"text/plain\" } });`\n",
      "    2.  Send it in a chat message: `let response = await chat.sendMessage({ message: createUserContent([\"Your prompt text here\", createPartFromUri(document.uri, document.mimeType)]) });`\n",
      "\n",
      "*   **In Go:**\n",
      "    1.  Upload the file: `document, err := client.Files.UploadFromPath(ctx, filepath.Join(getMedia(), \"your_file.txt\"), &genai.UploadFileConfig{ MIMEType: \"text/plain\" })`\n",
      "    2.  Prepare parts with the file data: `parts[1] = genai.Part{ FileData: &genai.FileData{ FileURI: document.URI, MIMEType: document.MIMEType } }`\n",
      "    3.  Send it in a chat message: `resp, err := chat.SendMessage(ctx, parts...)`\n",
      "\n",
      "The `genai.files` module provides methods like `upload()`, `delete()`, `download()`, `get()`, and `list()` for managing files. The `genai.chats` module allows you to create chat sessions and send messages, including those with uploaded file content.\n"
     ]
    }
   ],
   "source": [
    "print(reranked_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid + Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_query_engine = RetrieverQueryEngine(hybrid_retriever, node_postprocessors=[cohere_rerank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not langfuse_available:\n",
    "    hybrid_resp = hybrid_query_engine.query(query)\n",
    "else:\n",
    "    with langfuse.start_as_current_span(name=\"hybrid and rerank query\"):\n",
    "        hybrid_resp = hybrid_query_engine.query(query)\n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hybrid_resp.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To upload files for use in a chat, you first upload the file using the `files` submodule, and then reference the uploaded file when sending a message within a chat session.\n",
      "\n",
      "For example, in Python, you would:\n",
      "1. Upload the file: `document = client.files.upload(file=media / \"your_file.txt\")`\n",
      "2. Send the message in a chat, including the uploaded document: `response = chat.send_message(message=[\"Your message here.\", document])`\n",
      "\n",
      "Similarly, in Node.js, you would:\n",
      "1. Upload the file: `const document = await ai.files.upload({ file: filePath, config: { mimeType: \"text/plain\" }, });`\n",
      "2. Send the message, referencing the file's URI and MIME type: `let response = await chat.sendMessage({ message: createUserContent([\"Your message here.\", createPartFromUri(document.uri, document.mimeType),]), });`\n",
      "\n",
      "In Go, the process involves:\n",
      "1. Uploading the file: `document, err := client.Files.UploadFromPath(ctx, filepath.Join(getMedia(), \"your_file.txt\"), &genai.UploadFileConfig{ MIMEType: \"text/plain\", }, )`\n",
      "2. Creating parts that include the file data: `parts[1] = genai.Part{ FileData: &genai.FileData{ FileURI: document.URI, MIMEType: document.MIMEType, }, }`\n",
      "3. Sending the message with these parts: `resp, err := chat.SendMessage(ctx, parts...)`\n"
     ]
    }
   ],
   "source": [
    "print(hybrid_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReACT Agent Chat Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent, \n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, RetrieverTool\n",
    "from llama_index.core.tools.types import ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine = RetrieverQueryEngine.from_args(retriever=retriever, node_postprocessors=[cohere_rerank])\n",
    "\n",
    "# 3. (Optional) Add a name and description to the query engine tool\n",
    "#    This helps the agent decide when to use it.\n",
    "query_engine_tool = QueryEngineTool(\n",
    "    query_engine=hybrid_query_engine,\n",
    "    metadata= ToolMetadata(\n",
    "            name=\"google_genai_api\",\n",
    "            description=\"API documentation for Google GenAI\",\n",
    "            return_direct=False\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_tool = RetrieverTool(retriever=hybrid_retriever, node_postprocessor=[cohere_rerank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_agent = ReActAgent.from_tools(\n",
    "    tools=[query_engine_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    react_prompt=\"Always use the one of the above tools to answer the question\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools(\n",
    "    tools=[query_engine_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    react_prompt=\"Always use the one of the above tools to answer the question\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_agent = 'how to upload files for chat in google genai?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 1b84d790-a836-4477-94d4-98c08dc73e17. Step input: how to upload files for chat in google genai?\n",
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: Observation: The Google GenAI API does not directly support file uploads for chat. The `google_genai_api` tool is for interacting with the GenAI API itself, not for managing file uploads for a chat interface. File uploads for a chat interface would typically be handled by the application or platform that is *using* the GenAI API, not by the API directly.\n",
      "The Google GenAI API itself does not directly handle file uploads for chat. File uploads for a chat interface would typically be managed by the application or platform that is integrating with the GenAI API, not by the API directly. You would need to refer to the documentation or features of the specific chat application or platform you are using to understand how to upload files within that context.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "if not langfuse_available:\n",
    "    agent_resp = agent.chat(query_agent)\n",
    "else:\n",
    "    with langfuse.start_as_current_span(name=\"React agent query\"):\n",
    "        agent_resp = agent.chat(query_agent)\n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine import ContextChatEngine, CondensePlusContextChatEngine\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=8000)\n",
    "chat_engine = CondensePlusContextChatEngine(retriever=retriever, llm=llm, memory=memory, node_postprocessors=[cohere_rerank], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp1 = chat_engine.chat(query)\n",
    "print(resp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp2 = chat_engine.chat('Can you answer in short?')\n",
    "print(resp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router for Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import PydanticSingleSelector, LLMSingleSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize router query engine (single selection, pydantic)\n",
    "query_engine = RouterQueryEngine(\n",
    "    # selector=PydanticSingleSelector.from_defaults(llm=llm),\n",
    "    selector = LLMSingleSelector.from_defaults(llm=llm),\n",
    "    query_engine_tools=[\n",
    "        query_engine_tool,\n",
    "    ],\n",
    "    llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not langfuse_available:\n",
    "    router_resp = await query_engine.aquery(query)\n",
    "else:\n",
    "    with langfuse.start_as_current_span(name=\"Router query\"):\n",
    "        router_resp = await query_engine.aquery(query)\n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To upload files for use in a chat, you first upload the file using the `files` module. Once the file is uploaded, the resulting file object can then be included directly as part of the content in your chat messages.\n",
      "\n",
      "For instance, in Python, you would use `client.files.upload()` to upload a file, and then pass the uploaded file object within the `message` argument of `chat.send_message()`.\n"
     ]
    }
   ],
   "source": [
    "print(router_resp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
