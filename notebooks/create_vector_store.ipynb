{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Vector Store Creation\n",
    "\n",
    "read: \n",
    "- llamaindex - https://docs.llamaindex.ai/en/stable/understanding/rag/\n",
    "- pdf RAG: https://cookbook.openai.com/examples/parse_pdf_docs_for_rag\n",
    "- sample data - https://www.gutenberg.org/cache/epub/24022/pg24022.txt\n",
    "- sample data: https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import math\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime, timezone\n",
    "import logging\n",
    "import subprocess\n",
    "import hashlib\n",
    "from collections import deque\n",
    "import nest_asyncio\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import MarkdownNodeParser, CodeSplitter, SemanticSplitterNodeParser\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.extractors import SummaryExtractor, TitleExtractor, KeywordExtractor, DocumentContextExtractor\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.llms.litellm import LiteLLM\n",
    "\n",
    "\n",
    "import chromadb\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from core.utilities import create_custom_logger, get_large_files, setup_llm_logs, GoogleGenAIDummyTokensizer, HuggingfaceTokenizer\n",
    "from core.custom_components.custom_extractors import CustomDocumentContextExtractor\n",
    "from core.custom_components.custom_parsers import CustomMarkdownNodeParser\n",
    "from core.custom_components.custom_google_genai import CustomGoogleGenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.environ['GEMINI_API_KEY']\n",
    "GROQ_API_KEY = os.environ['GROQ_API_KEY']\n",
    "TOGETHER_API_KEY = os.environ['TOGETHER_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = 'data/google_genai/api/'\n",
    "OUTPUT_DIR = 'processed_data/' + INPUT_DIR\n",
    "\n",
    "FILE_TYPES = ['.md', '.mdx']\n",
    "# Select Extractors - To add metadata to each node. Some of them may use many LLM calls. Use only if needed.\n",
    "METADATA_EXTRACTORS = ['CustomDocumentContextExtractor']  # choose from ['TitleExtractor', 'SummaryExtractor', 'KeywordExtractor', 'CustomDocumentContextExtractor' etc]\n",
    "\n",
    "CHROMADB_PATH = (Path(OUTPUT_DIR) / 'chromadb').as_posix()\n",
    "CHROMADB_COLLECTION = 'contextual_api'\n",
    "\n",
    "LLM_MODEL_PROVIDER = 'litellm'  # choose from ['litellm', 'ollama', 'gemini', 'groq']\n",
    "LLM_MODEL = \"gemini/gemini-2.5-flash\" # \"cerebras/llama-3.3-70b\"  #  # \"together_ai/meta-llama/Llama-3.3-70B-Instruct-Turbo\" # \"cerebras/llama-3.3-70b\"  # \"groq/llama-3.3-70b-versatile\"  # \"cerebras/llama-3.3-70b\"\n",
    "RATE_LIMIT = 7 # LLM req/min, -1 if no limit\n",
    "\n",
    "RUN_PARALLEL = False  # process nodes in parallel using async\n",
    "\n",
    "# LITELLM_MODEL = \"gemini/gemini-2.5-flash-preview-05-20\"  # \"together_ai/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\" #\n",
    "# GEMINI_MODEL = \"models/gemini-2.0-flash\"  # \"models/gemini-2.0-flash\"\n",
    "# GROQ_MODEL = \"llama-3.3-70b-versatile\"\n",
    "# OLLAMA_MODEL = \"llama3.1:8b-instruct-q8_0\"\n",
    "\n",
    "FASTEMBED_EMBEDDING_MODEL = \"BAAI/bge-base-en-v1.5\"\n",
    "GEMINI_EMBEDDING_MODEL = \"models/text-embedding-004\"\n",
    "\n",
    "# TIKTOKEN_TOKENIZER_MODEL = \"cl100k_base\"\n",
    "# GEMINI_TOKENIZER_MODEL = LLM_MODEL\n",
    "# HUGGINGFACE_TOKENIZER_MODEL = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n",
    "\n",
    "\n",
    "EMBEDDING_PROVIDER = 'GoogleGenAIEmbedding'  # choose from ['FastEmbedEmbedding', 'GoogleGenAIEmbedding', etc]\n",
    "EMBEDDING_MODEL = GEMINI_EMBEDDING_MODEL\n",
    "\n",
    "# Only used for token counting, best use tiktoken unless accuracy is needed\n",
    "TOKENIZER_PROVIDER = 'tiktoken'  # chose from ['gemini', 'huggingface', 'tiktoken' etc]\n",
    "TOKENIZER_MODEL_NAME = 'cl100k_base'  # choose from ['models/gemini-2.5-flash', 'meta-llama/Meta-Llama-3.1-70B-Instruct', 'cl100k_base' etc]\n",
    "MAX_NODE_TOKENS = 2000\n",
    "\n",
    "DOCSTORE_PATH = (Path(OUTPUT_DIR) / 'docstore.json').as_posix()\n",
    "CONFIG_PATH = (Path(OUTPUT_DIR) / 'config.json').as_posix()\n",
    "\n",
    "\n",
    "EXCLUDE_FILES = ['__all_docs__.md']  # files/folders to exclude. eg ['file.txt', 'folder1/', 'folder2/b.txt', 'folder2/folder3/']\n",
    "SKIP_LARGE_FILES = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find and exclude large files over 20KB if enabled\n",
    "def get_large_files_to_exclude(excluded_files):\n",
    "    if SKIP_LARGE_FILES:\n",
    "        large_files = get_large_files(INPUT_DIR, min_size_kb=50, extensions=('.txt', '.md', '.mdx'))\n",
    "        for file_path, size_kb in large_files:\n",
    "            # Convert absolute path to relative path from INPUT_DIR\n",
    "            rel_path = Path(file_path).absolute().relative_to(Path(INPUT_DIR).absolute()).as_posix()\n",
    "            excluded_files.append(rel_path)\n",
    "            print(f\"Excluding large file: {rel_path} ({size_kb:.1f}KB)\")\n",
    "    return excluded_files\n",
    "\n",
    "\n",
    "def get_llm(config):\n",
    "    if config['llm_model_provider'] == 'groq':\n",
    "        return Groq(model=config['llm_model'], api_key=GROQ_API_KEY, max_retries=2, retry_on_rate_limit=True) # Number of retry attempts\n",
    "    elif config['llm_model_provider'] == 'gemini':\n",
    "        return CustomGoogleGenAI(\n",
    "            model=config['llm_model'],\n",
    "            api_key=GEMINI_API_KEY, \n",
    "            max_retries=2,  # Number of retry attempts\n",
    "            retry_on_rate_limit=True\n",
    "        )\n",
    "    elif config['llm_model_provider'] == 'ollama':\n",
    "        return Ollama(model=config['llm_model'], request_timeout=120.0, context_window=8192, )\n",
    "    elif config['llm_model_provider'] == 'litellm':\n",
    "        import litellm\n",
    "        litellm.suppress_debug_info = True\n",
    "        return LiteLLM(model=config['llm_model'], max_tokens=8192, max_retries=6)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"LLM provider {config['llm_model_provider']} invalid or not implemented\")\n",
    "\n",
    "\n",
    "def get_embed_model(config):\n",
    "    if config['embedding_provider'] == 'FastEmbedEmbedding':\n",
    "        from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "        return FastEmbedEmbedding(model_name=config['embedding_model'])\n",
    "    elif config['embedding_provider'] == 'GoogleGenAIEmbedding':\n",
    "        from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "        return GoogleGenAIEmbedding(model_name=config['embedding_model'], api_key=GEMINI_API_KEY)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Embedding provider {config['embedding_provider']} invalid or not implemented\")\n",
    "    \n",
    "\n",
    "def get_tokenizer(config, llm):\n",
    "    if config['tokenizer_provider'] == 'gemini':\n",
    "        return GoogleGenAIDummyTokensizer(llm=llm).encode\n",
    "    elif config['tokenizer_provider'] == 'huggingface':\n",
    "        return HuggingfaceTokenizer(model=config['tokenizer_model_name']).encode\n",
    "    elif config['tokenizer_provider'] == 'tiktoken':\n",
    "        return tiktoken.get_encoding(encoding_name=config['tokenizer_model_name']).encode\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{config['tokenizer_provider']} invalid or not implemented\")\n",
    "\n",
    "\n",
    "def get_metadata_extractors(config, llm, docstore=None):\n",
    "    metadata_extractors = []\n",
    "    for extractor in config['metadata_extractors']:\n",
    "        if extractor == 'TitleExtractor':\n",
    "            metadata_extractors.append(TitleExtractor(llm=llm, show_progress=False))\n",
    "        elif extractor == 'SummaryExtractor':\n",
    "            metadata_extractors.append(SummaryExtractor(llm=llm, show_progress=False))\n",
    "        elif extractor == 'KeywordExtractor':\n",
    "            metadata_extractors.append(KeywordExtractor(llm=llm, show_progress=False))\n",
    "        elif extractor == 'DocumentContextExtractor':\n",
    "            if docstore is None:\n",
    "                raise ValueError(\"docstore with original documents is required for Contextual Extractor\")\n",
    "            context_extractor = DocumentContextExtractor(\n",
    "                # these 2 are mandatory\n",
    "                docstore=docstore,\n",
    "                max_context_length=128000,\n",
    "                # below are optional\n",
    "                llm=llm,  # default to Settings.llm\n",
    "                oversized_document_strategy=\"warn\",\n",
    "                max_output_tokens=100,\n",
    "                key=\"context\",\n",
    "                prompt=DocumentContextExtractor.SUCCINCT_CONTEXT_PROMPT,\n",
    "                show_progress=False\n",
    "            )\n",
    "            metadata_extractors.append(context_extractor)\n",
    "        elif extractor == 'CustomDocumentContextExtractor':\n",
    "            if docstore is None:\n",
    "                raise ValueError(\"docstore with original documents is required for Contextual Extractor\")\n",
    "            context_extractor = CustomDocumentContextExtractor(\n",
    "                # these 2 are mandatory\n",
    "                docstore=docstore,\n",
    "                max_context_length=128000,\n",
    "                # below are optional\n",
    "                llm=llm,  # default to Settings.llm\n",
    "                oversized_document_strategy=\"warn\",\n",
    "                # max_output_tokens=100,\n",
    "                key=\"context\",\n",
    "                prompt=CustomDocumentContextExtractor.ORIGINAL_CONTEXT_PROMPT,\n",
    "                show_progress=False\n",
    "            )\n",
    "            metadata_extractors.append(context_extractor)\n",
    "        else:\n",
    "            raise ValueError(f\"Extractor {extractor} not available\")\n",
    "    return metadata_extractors\n",
    "\n",
    "\n",
    "def get_nodes_from_documents(documents, embed_model, tokenizer, max_tokens):\n",
    "    md_docs = []\n",
    "    txt_docs = []\n",
    "    py_docs = []\n",
    "    for doc in documents:\n",
    "        if Path(doc.metadata['file_name']).suffix.lower() in ('.md', '.mdx'):\n",
    "            md_docs.append(doc)\n",
    "        elif Path(doc.metadata['file_name']).suffix.lower() == '.txt':\n",
    "            txt_docs.append(doc)\n",
    "        elif Path(doc.metadata['file_name']).suffix.lower() == '.py':\n",
    "            py_docs.append(doc)\n",
    "        else:\n",
    "            raise ValueError(f\"Filetype not supported.\")\n",
    "    \n",
    "    nodes = []\n",
    "    if len(md_docs) > 0:\n",
    "        # md_node_parser = MarkdownNodeParser(chunk_size=512, chunk_overlap=32)\n",
    "        md_node_parser = CustomMarkdownNodeParser(max_tokens=max_tokens, max_header_level=2, split_pattern=r'\\*\\*(.*?)\\*\\*', tokenizer=tokenizer)\n",
    "        md_nodes = md_node_parser.get_nodes_from_documents(md_docs)\n",
    "        nodes += md_nodes\n",
    "    if len(txt_docs) > 0:\n",
    "        txt_node_parser = SemanticSplitterNodeParser(buffer_size=3, embed_model=embed_model)\n",
    "        txt_nodes = txt_node_parser.get_nodes_from_documents(txt_docs)\n",
    "        nodes += txt_nodes\n",
    "    if len(py_docs) > 0:\n",
    "        py_node_parser = CodeSplitter(language='python', chunk_lines=70, chunk_lines_overlap=10, max_chars=3000)\n",
    "        py_nodes = py_node_parser.get_nodes_from_documents(py_docs)\n",
    "        nodes += py_nodes\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "\n",
    "\n",
    "def is_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        return get_ipython() is not None\n",
    "    except ImportError:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def setup_application_logging(output_dir):\n",
    "    logging.basicConfig(\n",
    "        level=logging.WARNING,\n",
    "        filename=(Path(output_dir) / \"warnings.log\").as_posix(),              # All logs go here\n",
    "        filemode=\"a\",                    # 'w' to overwrite each run\n",
    "        format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n",
    "    )\n",
    "    \n",
    "    # Create your application logger\n",
    "    logger = logging.getLogger('documentation_agent')  # Use your app name\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.propagate = False  # Prevent propagation to root logger\n",
    "    \n",
    "    # Create file handler\n",
    "    file_handler = logging.FileHandler((Path(output_dir) / \"rag.log\").as_posix())\n",
    "    file_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"))\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_nodes_with_ratelimit(nodes, transformations, run_parallel=True, rate_limit=-1, logger=None):\n",
    "    if logger is None:\n",
    "        logger = logging.getLogger(__name__)\n",
    "    \n",
    "    actual_rate_limit = rate_limit\n",
    "    # decrease rate limit as per the no of llm based transformers\n",
    "    llm_ops = 0\n",
    "    for transformation in transformations:\n",
    "        if hasattr(transformation, 'llm'):\n",
    "            llm_ops += 1\n",
    "    if llm_ops > 1 and actual_rate_limit > 0:\n",
    "        actual_rate_limit = actual_rate_limit // llm_ops\n",
    "\n",
    "    transformed_nodes = []\n",
    "    batch_size_for_pipeline = actual_rate_limit if actual_rate_limit > 0 else 60 \n",
    "    \n",
    "    if not nodes: # Handle empty nodes list\n",
    "        return []\n",
    "\n",
    "    total_batches = math.ceil(len(nodes) / batch_size_for_pipeline)\n",
    "    \n",
    "    for batch_idx in range(total_batches):\n",
    "        batch_start_time = time.time()\n",
    "        # Get current batch of nodes\n",
    "        start_idx = batch_idx * batch_size_for_pipeline\n",
    "        end_idx = min(start_idx + batch_size_for_pipeline, len(nodes)) \n",
    "        batch_nodes = nodes[start_idx:end_idx]\n",
    "        \n",
    "        if not batch_nodes: # Should not happen if total_batches is calculated correctly from non-empty nodes\n",
    "            continue\n",
    "\n",
    "        logger.info(f\"Batch {batch_idx + 1}/{total_batches}, processing {len(batch_nodes)} nodes.\")\n",
    "\n",
    "        pipeline = IngestionPipeline(\n",
    "            transformations=transformations\n",
    "        )\n",
    "        \n",
    "        if run_parallel:\n",
    "            batch_retries = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    processed_batch = await pipeline.arun(nodes=batch_nodes, in_place=False, show_progress=False)\n",
    "                    # processed_batch = pipeline.run(nodes=batch_nodes, in_place=False, show_progress=False)\n",
    "\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing batch {batch_idx + 1}/{total_batches}. Retrying...: {e}\")\n",
    "                    time.sleep(70)\n",
    "                    batch_retries += 1\n",
    "                    if batch_retries > 1:\n",
    "                        logger.error('Aborting batch ...')\n",
    "                        processed_batch = []\n",
    "                        break\n",
    "        else:  # process nodes one by one\n",
    "            processed_batch = []\n",
    "            for i, node in enumerate(batch_nodes):\n",
    "                node_failure = False\n",
    "                node_retries = 0\n",
    "                # for transformation in transformations:\n",
    "                processed_node = node\n",
    "                while True:\n",
    "                    try:\n",
    "                        processed_nodes = await pipeline.arun(nodes=[processed_node], in_place=False, show_progress=False)\n",
    "                        processed_node = processed_nodes[0]\n",
    "                        # processed_node = transformation.process_nodes(nodes=[processed_node], in_place=False, show_progress=False)\n",
    "                        # if hasattr(transformation, 'llm'):\n",
    "                        #     time.sleep(1)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Error processing node {i}. Retrying...: {e}\")\n",
    "                        time.sleep(70)\n",
    "                        node_retries += 1\n",
    "                        if node_retries > 1:\n",
    "                            logger.error(f'Aborting node {i}...')\n",
    "                            node_failure = True\n",
    "                            break\n",
    "                if not node_failure:\n",
    "                    processed_batch.append(processed_node)\n",
    "\n",
    "        transformed_nodes.extend(processed_batch)\n",
    "        batch_end_time = time.time()\n",
    "        elapsed_time = batch_end_time - batch_start_time\n",
    "        logger.info(f\"Batch {batch_idx + 1}/{total_batches} finished in {elapsed_time:.1f}s\")\n",
    "        if actual_rate_limit > 0 and elapsed_time < 60:\n",
    "            await asyncio.sleep(60 - elapsed_time + 1)\n",
    "                            \n",
    "    return transformed_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "except FileExistsError as e:\n",
    "    print(f'Processed_data directory {OUTPUT_DIR} already exists. Please delete that or specify another directory.')\n",
    "    exit(1)\n",
    "print('output dir', OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "config = {\n",
    "    'llm_model_provider': LLM_MODEL_PROVIDER,\n",
    "    'llm_model': LLM_MODEL,\n",
    "    'rate_limit': RATE_LIMIT,\n",
    "    'input_dir': INPUT_DIR,\n",
    "    'output_dir': OUTPUT_DIR,\n",
    "    'file_types': FILE_TYPES,\n",
    "    'vector_store': 'chroma',\n",
    "    'chromadb_path': CHROMADB_PATH,\n",
    "    'chroma_collection': CHROMADB_COLLECTION,\n",
    "    'doctsore_path': DOCSTORE_PATH,\n",
    "    'embedding_provider': EMBEDDING_PROVIDER,\n",
    "    'embedding_model': EMBEDDING_MODEL,\n",
    "    'tokenizer_provider': TOKENIZER_PROVIDER,\n",
    "    'tokenizer_model_name': TOKENIZER_MODEL_NAME,\n",
    "    'max_node_tokens': MAX_NODE_TOKENS,\n",
    "    'metadata_extractors': METADATA_EXTRACTORS,\n",
    "    'datetime': datetime.now(timezone.utc).isoformat(),\n",
    "}\n",
    "\n",
    "# Setup logging\n",
    "logger = setup_application_logging(OUTPUT_DIR)\n",
    "llm_logger = create_custom_logger('LLMlogger', (Path(OUTPUT_DIR) / \"llm_events.log\").as_posix())\n",
    "setup_llm_logs(llm_logger, Settings, show_text=False, short_inputs=False, short_outputs=False)\n",
    "\n",
    "\n",
    "is_jupyter_notebook = is_notebook()\n",
    "\n",
    "# # for jupyter notebooks - to fix event loop issue\n",
    "if is_jupyter_notebook:\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# for jupyter notebook - Start ChromaDB server\n",
    "if is_jupyter_notebook:\n",
    "    process = subprocess.Popen([\"chroma\", \"run\", \"--path\", CHROMADB_PATH])\n",
    "    remote_db = chromadb.HttpClient()\n",
    "    chroma_collection = remote_db.get_or_create_collection(CHROMADB_COLLECTION)\n",
    "else:\n",
    "    db = chromadb.PersistentClient(path=str(CHROMADB_PATH))\n",
    "    chroma_collection = db.get_or_create_collection(CHROMADB_COLLECTION)\n",
    "\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "# Initialize stores\n",
    "# if Path(DOCSTORE_PATH).exists():\n",
    "#     docstore = SimpleDocumentStore.from_persist_path(DOCSTORE_PATH)\n",
    "#     logger.info(f\"Loaded existing docstore with {len(docstore.docs)} documents\")\n",
    "# else:\n",
    "#     docstore = SimpleDocumentStore()\n",
    "\n",
    "# storage_context = StorageContext.from_defaults(vector_store=vector_store, docstore=docstore)\n",
    "\n",
    "llm = get_llm(config)\n",
    "embed_model = get_embed_model(config)\n",
    "tokenizer = get_tokenizer(config, llm)\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.tokenizer = tokenizer\n",
    "\n",
    "# tokenizer = GeminiTokenizer()\n",
    "# Settings.tokenizer = tokenizer\n",
    "max_tokens = config['max_node_tokens']\n",
    "\n",
    "# Step 1: load Documents\n",
    "files_to_exlude = get_large_files_to_exclude(EXCLUDE_FILES)\n",
    "documents = SimpleDirectoryReader(input_dir=INPUT_DIR, exclude=files_to_exlude, recursive=True, filename_as_id=True,\n",
    "                                    required_exts=FILE_TYPES).load_data()\n",
    "for i, document in enumerate(documents):\n",
    "    document.doc_id = Path(document.metadata['file_path']).relative_to(Path(INPUT_DIR).absolute()).as_posix()\n",
    "    document.metadata['file_path'] = document.doc_id\n",
    "print('Document Count', len(documents))\n",
    "\n",
    "original_docs_docstore = SimpleDocumentStore()\n",
    "await original_docs_docstore.async_add_documents(documents)\n",
    "print('docstore created')\n",
    "\n",
    "# Track processed documents and generate batch IDs\n",
    "# processed_node_ids = set(docstore.docs.keys())\n",
    "processed_node_ids = set(chroma_collection.get(include=[])['ids'])\n",
    "print(f\"{len(processed_node_ids)} nodes already processed\")\n",
    "\n",
    "\n",
    "## ! Important. #todo\n",
    "# Implement Logic to split markdown nodes further since Chunk size not followed by MarkdownNode parse\n",
    "try:\n",
    "    doc_batch_size = 1\n",
    "    all_nodes = []\n",
    "    # can be optimised to create docstore only for docs being processed in one iteration\n",
    "    metadata_extractors = get_metadata_extractors(config, llm, original_docs_docstore)\n",
    "    transformations = metadata_extractors + [embed_model]\n",
    "    \n",
    "    for index in range(0, len(documents), doc_batch_size):\n",
    "        t_start = time.time()\n",
    "        logger.info(f\"doc [{index}-{index+doc_batch_size}] start {time.time()} {documents[index].doc_id}\")\n",
    "        # Step 2: Chunk Documents into Nodes\n",
    "        nodes = get_nodes_from_documents(documents=documents[index:index+doc_batch_size],\n",
    "                                        embed_model=embed_model, tokenizer=tokenizer, max_tokens=max_tokens)\n",
    "        # assign node id to nodes\n",
    "        for node in nodes:\n",
    "            node.node_id = f\"{node.ref_doc_id}-{node.start_char_idx}-{node.end_char_idx}\"\n",
    "            # node.node_id = str(hashlib.sha256(f\"{node.ref_doc_id} {node.start_char_idx} {node.end_char_idx} {node.text}\".encode()).hexdigest())\n",
    "        \n",
    "\n",
    "        # logger.info(f\"before process {len(nodes)} nodes\")\n",
    "        nodes = [node for node in nodes if node.node_id not in processed_node_ids]\n",
    "        if len(nodes) == 0:\n",
    "            logger.info(f\"skipping nodes as they are already processed\")\n",
    "            continue\n",
    "\n",
    "        # remove duplicate nodes: sometimes some nodes may have same doc, start, end due to issue in parser, or exactly same content in same doc\n",
    "        # Remove duplicate nodes based on node_id\n",
    "        seen_node_ids = set()\n",
    "        unique_nodes = []\n",
    "        for node in nodes:\n",
    "            if node.node_id not in seen_node_ids:\n",
    "                seen_node_ids.add(node.node_id)\n",
    "                unique_nodes.append(node)\n",
    "        nodes = unique_nodes\n",
    "\n",
    "        logger.info(f\"processing {len(nodes)} nodes\")\n",
    "        # Step 3: extract metadata and embeddings for nodes\n",
    "        nodes = await process_nodes_with_ratelimit(nodes=nodes, transformations=transformations, run_parallel=RUN_PARALLEL, rate_limit = RATE_LIMIT, logger=logger)\n",
    "        \n",
    "        if nodes is None or len(nodes) == 0:\n",
    "            raise Exception('Error processing nodes. Aborting.')\n",
    "\n",
    "        # Step 4: Save the Nodes/Chunks in vector store\n",
    "        total_tokens = sum([len(Settings.tokenizer(node.get_content(metadata_mode=MetadataMode.EMBED))) for node in nodes])\n",
    "        node_ids = await vector_store.async_add(nodes)\n",
    "        # docstore.add_documents(nodes)\n",
    "        # docstore.persist(DOCSTORE_PATH)\n",
    "        logger.info(f\"added {len(nodes)} to vector store. Total tokens = {total_tokens}\")\n",
    "        all_nodes.extend(nodes)\n",
    "        t_end = time.time()\n",
    "        logger.info(f\"time for docs {index} to {index+doc_batch_size} = {round(t_end-t_start)}, nodes: {len(nodes)}\")\n",
    "        # time.sleep(1)\n",
    "\n",
    "finally:\n",
    "    \n",
    "    # Persist the docstore\n",
    "    # docstore.persist(DOCSTORE_PATH)\n",
    "    # logger.info(f\"saving doctore to {DOCSTORE_PATH}\")\n",
    "    \n",
    "    \n",
    "    # Check if config file exists and read existing config\n",
    "    all_configs = []\n",
    "    if os.path.exists(CONFIG_PATH):\n",
    "        try:\n",
    "            with open(CONFIG_PATH, 'r') as fp:\n",
    "                content = fp.read()\n",
    "                if content.strip(): # Ensure file is not empty\n",
    "                    all_configs = json.loads(content) # Changed from json.load(fp)\n",
    "                    if not isinstance(all_configs, list): # If existing config is not a list, wrap it in a list\n",
    "                        logger.warning(f\"Existing config in {CONFIG_PATH} is not a list. Wrapping it in a list.\")\n",
    "                        all_configs = [all_configs]\n",
    "        except json.JSONDecodeError:\n",
    "            logger.error(f\"Could not decode JSON from {CONFIG_PATH}. Starting with an empty config list.\")\n",
    "            all_configs = []\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An unexpected error occurred while reading {CONFIG_PATH}: {e}. Starting with an empty config list.\")\n",
    "            all_configs = []\n",
    "    \n",
    "    current_run_config = config.copy() # Use a copy to avoid modifying the original config dict for the current run\n",
    "    current_run_config['run_time'] = datetime.now(timezone.utc).isoformat()\n",
    "    current_run_config['run_nodes'] = len(all_nodes) if 'all_nodes' in locals() else 0 # ensure all_nodes exists\n",
    "    \n",
    "    all_configs.append(current_run_config)\n",
    "\n",
    "    with open(CONFIG_PATH, 'w') as fp:\n",
    "        json.dump(all_configs, fp, indent=4)\n",
    "    \n",
    "    if 'process' in locals() and process.poll() is None: # Check if process exists and is running\n",
    "        process.terminate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
