{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9bf73b",
   "metadata": {},
   "source": [
    "# RAG using LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb1168b",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6943f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'data/crawl4ai'...\n"
     ]
    }
   ],
   "source": [
    "# download your own data\n",
    "# !git clone https://github.com/unclecode/crawl4ai.git data/crawl4ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e74db4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change these variables to your own\n",
    "input_dir = \"data/crawl4ai/docs/md_v2\"\n",
    "index_name = \"crawl4ai\"\n",
    "\n",
    "EMBEDDING_MODEL = \"models/text-embedding-004\"   # or \"BAAI/bge-base-en-v1.5\"\n",
    "GEMINI_API_KEY = \"\"\n",
    "LLM_MODEL = \"gemini/gemini-2.5-flash-lite\"   # or \"ollama/qwen3:8b\"\n",
    "\n",
    "file_types = [\".md\", \".mdx\"]\n",
    "vector_store_path = \"output/chromadb\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e3f45b",
   "metadata": {},
   "source": [
    "## Step 1: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05685da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "h:\\Coding\\ml\\llm\\agents\\documentation_agent\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.llms.litellm import LiteLLM\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.core.node_parser import MarkdownNodeParser, CodeSplitter, SentenceSplitter\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.extractors import SummaryExtractor, TitleExtractor, KeywordExtractor, DocumentContextExtractor\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../core')\n",
    "from custom_components.custom_extractors import CustomDocumentContextExtractor\n",
    "\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f70e499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Count: 0\n"
     ]
    }
   ],
   "source": [
    "# load chromadb vector store (or another vector stores like Qdrant, PineCone etc)\n",
    "chroma_process = subprocess.Popen([\"chroma\", \"run\", \"--path\", vector_store_path])\n",
    "chroma_client = chromadb.HttpClient()\n",
    "chroma_collection = chroma_client.get_or_create_collection(index_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "print('Vector Store Count:', chroma_collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "156693b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Count 67\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "documents = SimpleDirectoryReader(input_dir=input_dir, exclude=[], recursive=True, filename_as_id=True,\n",
    "                                       required_exts=file_types).load_data()\n",
    "# Note: For PDFs, first convert to text/markdown using specialized pdf converters\n",
    "\n",
    "# set doc-id for easy identification\n",
    "for i, document in enumerate(documents):\n",
    "    document.doc_id = Path(document.metadata['file_path']).relative_to(Path(input_dir).absolute()).as_posix()\n",
    "    document.metadata['file_path'] = document.doc_id\n",
    "\n",
    "# create docstore - can be used to pass original docs to some Components\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(documents)\n",
    "\n",
    "print('Document Count', len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c0957e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "advanced/multi-url-crawling.md\n"
     ]
    }
   ],
   "source": [
    "# Process documents one by one (its also possible to process in batches)\n",
    "document = documents[7]\n",
    "print(document.doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7553b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Advanced Multi-URL Crawling with Dispatchers\r\n",
       "\r\n",
       "> **Heads Up**: Crawl4AI supports advanced dispatchers for **parallel** or **throttled** crawling, providing dynamic rate limiting and memory usage checks. The built-in `arun_many()` function uses these dispatchers to handle concurrency efficiently.\r\n",
       "\r\n",
       "## 1. Introduction\r\n",
       "\r\n",
       "When crawling many URLs:\r\n",
       "\r\n",
       "- **Basic**: Use `arun()` in a loop (simple but less efficient)\r\n",
       "- **Better**: Use `arun_many()`, which efficiently handles multiple URLs with prop"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preview document\n",
    "display(Markdown(document.text[:500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64d505d",
   "metadata": {},
   "source": [
    "## Step 2: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5762c088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Count 16\n"
     ]
    }
   ],
   "source": [
    "# extract nodes from documents (i.e. split document into chunks)\n",
    "# experiment with different node parsers\n",
    "\n",
    "file_extension = Path(document.metadata['file_name']).suffix.lower()\n",
    "if file_extension in ('.md', '.mdx'):\n",
    "    node_parser = MarkdownNodeParser()\n",
    "elif file_extension == '.txt':\n",
    "    node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "elif file_extension == '.py':\n",
    "    node_parser = CodeSplitter(language='python', chunk_lines=70, chunk_lines_overlap=20, max_chars=2000)\n",
    "else:\n",
    "    raise ValueError(f\"Filetype not supported.\")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents([document])\n",
    "\n",
    "# assign predictable node_id to nodes\n",
    "for node in nodes:\n",
    "    node.node_id = f\"{node.ref_doc_id}-{node.hash}\"\n",
    "\n",
    "print('Node Count', len(nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc59c17",
   "metadata": {},
   "source": [
    "## Step 3: Extract Metadata (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "23a1c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding model\n",
    "\n",
    "embed_model = GoogleGenAIEmbedding(model_name=EMBEDDING_MODEL, api_key=GEMINI_API_KEY)\n",
    "# embed_model = FastEmbedEmbedding(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "# load LLM model (required if extracting metadata - Optional)\n",
    "llm = LiteLLM(model=LLM_MODEL, max_tokens=8192, max_retries=6)\n",
    "# llm = Ollama(model=LLM_MODEL, request_timeout=120.0, context_window=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e685fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract metadata for nodes (Uses LLMs) - Optional\n",
    "# helps in improving embedding quality. can also help during retrieval\n",
    "\n",
    "# Warning - May use too many LLM tokens\n",
    "\n",
    "title_extractor = TitleExtractor(llm=llm, show_progress=False)\n",
    "keyword_extractor = KeywordExtractor(llm=llm, show_progress=False)\n",
    "\n",
    "# context extractor - Read Claude blog on Contextual RAG\n",
    "context_extractor = CustomDocumentContextExtractor(\n",
    "                # these 2 are mandatory\n",
    "                docstore=docstore,\n",
    "                max_context_length=8192,\n",
    "                # below are optional\n",
    "                llm=llm,  # default to Settings.llm\n",
    "                oversized_document_strategy=\"warn\",\n",
    "                # max_output_tokens=100,\n",
    "                key=\"context\",\n",
    "                prompt=CustomDocumentContextExtractor.ORIGINAL_CONTEXT_PROMPT,\n",
    "                show_progress=False\n",
    "            )\n",
    "\n",
    "extractors = [keyword_extractor]  # include the ones that you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6200971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: Can be costly if you have a lot of nodes\n",
    "for extractor in extractors:\n",
    "    extractor_results = await extractor.aextract(nodes)\n",
    "    for node, result in zip(nodes, extractor_results):\n",
    "        node.metadata.update(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be303b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_path': 'advanced/multi-url-crawling.md',\n",
       " 'file_name': 'multi-url-crawling.md',\n",
       " 'file_size': 15208,\n",
       " 'creation_date': '2025-07-24',\n",
       " 'last_modified_date': '2025-07-24',\n",
       " 'header_path': '/',\n",
       " 'excerpt_keywords': 'Crawling, Dispatchers, Parallel, Throttled, Concurrency'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62228b7e",
   "metadata": {},
   "source": [
    "## Step 4: Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b74ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get embeddings for nodes\n",
    "# Embeddings are numerical representation for a given text, such that similar documents have similar embeddings\n",
    "# Embeddings are generated for text + metadata\n",
    "\n",
    "# we can get different content from nodes depending on requirement by specifying MetadataMode\n",
    "node_texts = [node.get_content(metadata_mode=MetadataMode.EMBED) for node in nodes]\n",
    "embeddings = await embed_model.aget_text_embedding_batch(node_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08d4bb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path: advanced/multi-url-crawling.md\n",
      "/eader_path: /Advanced Multi-URL Crawling with Dispatchers\n",
      "excerpt_keywords: Crawling, Dispatchers, Concurrency, Rate Limiting, Memory Management\n",
      "\n",
      "## 1. Introduction\n",
      "\n",
      "When crawling many URLs:\n",
      "\n",
      "- **Basic**: Use `arun()` in a loop (simple but less efficient)\n",
      "- **Better**: Use `arun_many()`, which efficiently handles multiple URLs with proper concurrency control\n",
      "- **Best**: Customize dispatcher behavior for your specific needs (memory management, rate limits, etc.)\n",
      "\n",
      "**Why Dispatchers?**  \n",
      "\n",
      "- **Adaptive**: Memory-based dispatchers can pause or slow down based on system resources\n",
      "- **Rate-limiting**: Built-in rate limiting with exponential backoff for 429/503 responses\n",
      "- **Real-time Monitoring**: Live dashboard of ongoing tasks, memory usage, and performance\n",
      "- **Flexibility**: Choose between memory-adaptive or semaphore-based concurrency\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(nodes[1].get_content(metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db2fb558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimensions 768\n"
     ]
    }
   ],
   "source": [
    "print('Embedding dimensions', len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdeb8a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add embeddings to nodes:\n",
    "for node, embedding in zip(nodes, embeddings):\n",
    "    node.embedding = embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e537abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add nodes (with embeddings) to Vector Store\n",
    "node_ids = await vector_store.async_add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7c95383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store Count: 16\n"
     ]
    }
   ],
   "source": [
    "print('Vector Store Count:', chroma_collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cebb483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the chromadb server\n",
    "chroma_process.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac433d19",
   "metadata": {},
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbd13544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers.type import ResponseMode\n",
    "from llama_index.core.schema import QueryBundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04341067",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"how to extract multiple urls with low memory usage?\"\n",
    "top_k = 5  # max relevant nodes to retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b09b900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load chroma vector store\n",
    "chroma_process = subprocess.Popen([\"chroma\", \"run\", \"--path\", vector_store_path])\n",
    "chroma_client = chromadb.HttpClient()\n",
    "chroma_collection = chroma_client.get_or_create_collection(index_name)\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3812a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the retriever\n",
    "vector_index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=top_k)\n",
    "\n",
    "# retrieve nodes\n",
    "retrieved_nodes = await vector_retriever.aretrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38de299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node score: 0.4948026761151271\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### 4.1 Batch Processing (Default)\r\n",
       "\r\n",
       "```python\r\n",
       "async def crawl_batch():\r\n",
       "    browser_config = BrowserConfig(headless=True, verbose=False)\r\n",
       "    run_config = CrawlerRunConfig(\r\n",
       "        cache_mode=CacheMode.BYPASS,\r\n",
       "        stream=False  # Default: get all results at once\r\n",
       "    )\r\n",
       "    \r\n",
       "    dispatcher = MemoryAdaptiveDispatcher(\r\n",
       "        memory_threshold_percent=70.0,\r\n",
       "        check_interval=1.0,\r\n",
       "        max_session_permit=10,\r\n",
       "        monitor=CrawlerMonitor(\r\n",
       "            display_mode=DisplayMode.DETAILED\r\n",
       "        )\r\n",
       "    )\r\n",
       "\r\n",
       "    async with AsyncWebCrawler(config=browser_config) as crawler:\r\n",
       "        # Get all results at once\r\n",
       "        results = await crawler.arun_many(\r\n",
       "            urls=urls,\r\n",
       "            config=run_config,\r\n",
       "            dispatcher=dispatcher\r\n",
       "        )\r\n",
       "        \r\n",
       "        # Process all results after completion\r\n",
       "        for result in results:\r\n",
       "            if result.success:\r\n",
       "                await process_result(result)\r\n",
       "            else:\r\n",
       "                print(f\"Failed to crawl {result.url}: {result.error_message}\")\r\n",
       "```\r\n",
       "\r\n",
       "**Review:**  \r\n",
       "- **Purpose:** Executes a batch crawl with all URLs processed together after crawling is complete.  \r\n",
       "- **Dispatcher:** Uses `MemoryAdaptiveDispatcher` to manage concurrency and system memory.  \r\n",
       "- **Stream:** Disabled (`stream=False`), so all results are collected at once for post-processing.  \r\n",
       "- **Best Use Case:** When you need to analyze results in bulk rather than individually during the crawl.\r\n",
       "\r\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print retrieved nodes scores and text\n",
    "print('Node score:', retrieved_nodes[0].score)\n",
    "display(Markdown(retrieved_nodes[0].node.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e8192",
   "metadata": {},
   "source": [
    "## Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e55ae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # setup langfuse for Observability - Optional\n",
    "\n",
    "# # Enter your Langfuse Keys\n",
    "# LANGFUSE_SECRET_KEY = \"\"\n",
    "# LANGFUSE_PUBLIC_KEY = \"\"\n",
    "# LANGFUSE_HOST = \"https://cloud.langfuse.com\"\n",
    "\n",
    "# from langfuse import get_client\n",
    "# from openinference.instrumentation.llama_index import LlamaIndexInstrumentor\n",
    "\n",
    "# def initialize_langfuse():\n",
    "#     langfuse = get_client()\n",
    "#     langfuse_available = False\n",
    "\n",
    "#     # Verify langfuse connection\n",
    "#     if langfuse.auth_check():\n",
    "#         langfuse_available = True\n",
    "#         LlamaIndexInstrumentor().instrument()\n",
    "#         print(\"Langfuse client is authenticated and ready!\")\n",
    "#         return langfuse, langfuse_available\n",
    "#     else:\n",
    "#         print(\"Authentication failed. Please check your credentials and host.\")\n",
    "#         return langfuse, langfuse_available\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4c36e801",
   "metadata": {},
   "outputs": [],
   "source": [
    "langfuse, langfuse_available = None, False\n",
    "# langfuse, langfuse_available = initialize_langfuse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5c95b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_bundle = QueryBundle(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac8e741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthesize response from retrieved nodes\n",
    "response_synthesizer = get_response_synthesizer(llm=llm, response_mode=ResponseMode.COMPACT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "523a9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not langfuse_available:\n",
    "    response = response_synthesizer.synthesize(query_bundle, retrieved_nodes)\n",
    "else:\n",
    "    with langfuse.start_as_current_span(name=\"Vector DB Query\"):\n",
    "        response = response_synthesizer.synthesize(query_bundle, retrieved_nodes)\n",
    "    langfuse.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0792f0b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To extract multiple URLs with low memory usage, you can utilize the `MemoryAdaptiveDispatcher`. This dispatcher dynamically adjusts concurrency based on system memory, pausing or slowing down crawling when memory resources are constrained. When using this dispatcher, you can configure parameters such as `memory_threshold_percent`, `check_interval`, and `max_session_permit` to fine-tune its behavior. The `arun_many()` function, which supports these dispatchers, is recommended for efficient handling of multiple URLs with proper concurrency control."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efcb2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Source (Doc id: advanced/multi-url-crawling.md-4a280cb47538cc51d2edb2504467bdb1b7985e497b1786019361e74eac2c8929): ### 4.1 Batch Processing (Default)\n",
      "\n",
      "```python\n",
      "async def crawl_batch():\n",
      "    browser_config = B...\n",
      "\n",
      "> Source (Doc id: advanced/multi-url-crawling.md-503f24d9b00dadf6469b8b8fb5cec8fa6b337d62ef3e5a38f41d76b9f85547b1): ## 1. Introduction\n",
      "\n",
      "When crawling many URLs:\n",
      "\n",
      "- **Basic**: Use `arun()` in a loop (simple but...\n",
      "\n",
      "> Source (Doc id: advanced/multi-url-crawling.md-ebd3e0e1f5e67c69c4bca6b4b773f2aa1de176f28796bd899259861fb50b3164): ## 5. Dispatch Results\n",
      "\n",
      "Each crawl result includes dispatch information:\n",
      "\n",
      "```python\n",
      "@datacla...\n",
      "\n",
      "> Source (Doc id: advanced/multi-url-crawling.md-d6265503231a80d6823982dc7df4424c336a83759be172d75108aa71129d0ee1): # Advanced Multi-URL Crawling with Dispatchers\n",
      "\n",
      "> **Heads Up**: Crawl4AI supports advanced disp...\n",
      "\n",
      "> Source (Doc id: advanced/multi-url-crawling.md-1caae7d6b6db998a7720d209dd266d8a2ccdf68de71530263c9dfb4443866f54): ## 6. Summary\n",
      "\n",
      "1.â€‚**Two Dispatcher Types**:\n",
      "\n",
      "   - MemoryAdaptiveDispatcher (default): Dynamic...\n"
     ]
    }
   ],
   "source": [
    "# check the source nodes used for generating the response\n",
    "print(response.get_formatted_sources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5a1225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
