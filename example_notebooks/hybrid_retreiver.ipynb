{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext, VectorStoreIndex\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n",
    "\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "import json\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-retrievers-bm25\n",
    "#pip install PyStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_directory = 'processed_data/llama_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = (Path(process_directory) /'config.json').as_posix()\n",
    "with open(config_path, 'r') as fp:\n",
    "    config = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "choma_path = config['chromadb_path']\n",
    "chroma_colection_name = config['chroma_collection']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenAI(\n",
    "    model=\"models/gemini-2.0-flash\",\n",
    "    api_key=os.environ['GEMINI_API_KEY'], \n",
    ")\n",
    "embed_model =  GoogleGenAIEmbedding(model_name=\"models/text-embedding-004\", api_key=os.environ['GEMINI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler, simple_llm_handler\n",
    "import tiktoken\n",
    "from utilities import GeminiTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.tokenizer = GeminiTokenizer().encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=GeminiTokenizer().encode,\n",
    "    verbose=True,  # set to true to see usage printed to the console\n",
    ")\n",
    "\n",
    "llm_debugger = simple_llm_handler.SimpleLLMHandler()\n",
    "handlers = [token_counter] #, llm_debugger]\n",
    "Settings.callback_manager = CallbackManager(handlers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chroma run --path processed_data/llama_index/chromadb\n"
     ]
    }
   ],
   "source": [
    "chroma_command = f\"chroma run --path {choma_path}\"\n",
    "print(chroma_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_db = chromadb.HttpClient()\n",
    "chroma_collection = remote_db.get_or_create_collection('contextual')\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2084"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUERY = 'How to Implement Callback manager to count tokens of all llm calls made, through any component, maybe extractor or retreiver, or query engine.'\n",
    "QUERY = 'how to use gemini as the llm and for embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_retreiver = index.as_retriever(similarity_top_k=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Token Usage: 12\n"
     ]
    }
   ],
   "source": [
    "context_nodes = index_retreiver.retrieve(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='api_reference/embeddings/gemini.md-0-87', embedding=None, metadata={'file_path': 'api_reference/embeddings/gemini.md', 'file_name': 'gemini.md', 'file_size': 88, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/', 'context': 'This chunk is the entire document.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='api_reference/embeddings/gemini.md', node_type='4', metadata={'file_path': 'api_reference/embeddings/gemini.md', 'file_name': 'gemini.md', 'file_size': 88, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='de1d135c1cd76d13df58c394442ce97650b7d28aa3a20c3c8174ba1d7532f723')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='::: llama_index.embeddings.gemini\\n    options:\\n      members:\\n        - GeminiEmbedding', mimetype='text/plain', start_char_idx=0, end_char_idx=87, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.7104416075854444),\n",
       " NodeWithScore(node=TextNode(id_='api_reference/llms/gemini.md-0-72', embedding=None, metadata={'file_path': 'api_reference/llms/gemini.md', 'file_name': 'gemini.md', 'file_size': 73, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/', 'context': 'This chunk defines the Gemini LLM within the llama_index library.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='api_reference/llms/gemini.md', node_type='4', metadata={'file_path': 'api_reference/llms/gemini.md', 'file_name': 'gemini.md', 'file_size': 73, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='62664c96aadb8535184cc34a8b9782d1ee9c202c160e9cbadb0c65602b5d8580')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='::: llama_index.llms.gemini\\n    options:\\n      members:\\n        - Gemini', mimetype='text/plain', start_char_idx=0, end_char_idx=72, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.6754126524282204)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_nodes[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nodes_info  = chroma_collection.get(include=[])['ids']\n",
    "# index = VectorStoreIndex.from_vector_store(vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_info = chroma_collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'metadatas', 'documents', 'data', 'uris', 'included'])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_node_content': '{\"id_\": \"module_guides/storing/vector_stores.md-7039-10664\", \"embedding\": null, \"metadata\": {\"file_path\": \"module_guides/storing/vector_stores.md\", \"file_name\": \"vector_stores.md\", \"file_size\": 10974, \"creation_date\": \"2025-02-11\", \"last_modified_date\": \"2025-02-11\", \"header_path\": \"/Vector Stores/\", \"context\": \"This section lists example Jupyter notebooks demonstrating how to use various vector stores with LlamaIndex.\\\\n\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"module_guides/storing/vector_stores.md\", \"node_type\": \"4\", \"metadata\": {\"file_path\": \"module_guides/storing/vector_stores.md\", \"file_name\": \"vector_stores.md\", \"file_size\": 10974, \"creation_date\": \"2025-02-11\", \"last_modified_date\": \"2025-02-11\"}, \"hash\": \"5e44dd7d65c4dfe8b05a929de4d036239b67e4aa07356cd4392dac7ea7d620c9\", \"class_name\": \"RelatedNodeInfo\"}, \"2\": {\"node_id\": \"f73af2f9-eed1-4561-aafa-21783083651b\", \"node_type\": \"1\", \"metadata\": {\"file_path\": \"module_guides/storing/vector_stores.md\", \"file_name\": \"vector_stores.md\", \"file_size\": 10974, \"creation_date\": \"2025-02-11\", \"last_modified_date\": \"2025-02-11\", \"header_path\": \"/Vector Stores/\"}, \"hash\": \"496a8a1368f624bd7eaa030b5635599c87e815ff2b2ec109221708dc340ff3cc\", \"class_name\": \"RelatedNodeInfo\"}}, \"metadata_template\": \"{key}: {value}\", \"metadata_separator\": \"\\\\n\", \"text\": \"\", \"mimetype\": \"text/plain\", \"start_char_idx\": 7039, \"end_char_idx\": 10664, \"metadata_seperator\": \"\\\\n\", \"text_template\": \"[Excerpt from document]\\\\n{metadata_str}\\\\nExcerpt:\\\\n-----\\\\n{content}\\\\n-----\\\\n\", \"class_name\": \"TextNode\"}',\n",
       " '_node_type': 'TextNode',\n",
       " 'context': 'This section lists example Jupyter notebooks demonstrating how to use various vector stores with LlamaIndex.\\n',\n",
       " 'creation_date': '2025-02-11',\n",
       " 'doc_id': 'module_guides/storing/vector_stores.md',\n",
       " 'document_id': 'module_guides/storing/vector_stores.md',\n",
       " 'file_name': 'vector_stores.md',\n",
       " 'file_path': 'module_guides/storing/vector_stores.md',\n",
       " 'file_size': 10974,\n",
       " 'header_path': '/Vector Stores/',\n",
       " 'last_modified_date': '2025-02-11',\n",
       " 'ref_doc_id': 'module_guides/storing/vector_stores.md'}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_info['metadatas'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = vector_store.get_nodes(nodes_info['ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2084"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes[1].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "import Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# We can pass in the index, docstore, or list of nodes to create the retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes,\n",
    "    similarity_top_k=20,\n",
    "    # Optional: We can pass in the stemmer and set the language for stopwords\n",
    "    # This is important for removing stopwords and stemming the query + text\n",
    "    # The default is english for both\n",
    "    stemmer=Stemmer.Stemmer(\"english\"),\n",
    "    language=\"english\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_nodes = bm25_retriever.retrieve('what are callback handlers?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_reference/callbacks/langfuse.md-0-98 ----- 5.477563858032227\n",
      "understanding/tracing_and_debugging/tracing_and_debugging.md-525-1300 ----- 5.186171054840088\n",
      "api_reference/callbacks/uptrain.md-0-94 ----- 4.90223503112793\n",
      "api_reference/callbacks/promptlayer.md-0-94 ----- 4.90223503112793\n",
      "api_reference/callbacks/deepeval.md-0-98 ----- 4.90223503112793\n",
      "api_reference/callbacks/arize_phoenix.md-0-108 ----- 4.871725082397461\n",
      "api_reference/callbacks/argilla.md-0-96 ----- 4.871725082397461\n",
      "module_guides/observability/index.md-18792-18893 ----- 4.868949890136719\n",
      "community/integrations/uptrain.md-8404-8713 ----- 4.773504257202148\n",
      "community/integrations/uptrain.md-2707-3132 ----- 4.572553634643555\n",
      "module_guides/observability/index.md-16664-16947 ----- 4.515216827392578\n",
      "module_guides/observability/index.md-15309-15401 ----- 3.982264518737793\n",
      "community/integrations/uptrain.md-5279-5617 ----- 3.734769105911255\n",
      "community/integrations/uptrain.md-5619-6485 ----- 3.7272391319274902\n",
      "community/integrations/uptrain.md-3964-4395 ----- 3.7224268913269043\n",
      "community/integrations/uptrain.md-8715-9132 ----- 3.633030414581299\n",
      "module_guides/observability/callbacks/root.md-1329-2921 ----- 3.5813140869140625\n",
      "community/integrations/uptrain.md-12518-16448 ----- 3.370175361633301\n",
      "community/integrations/uptrain.md-3134-3962 ----- 3.278815746307373\n",
      "community/integrations/uptrain.md-1824-2705 ----- 3.2704269886016846\n"
     ]
    }
   ],
   "source": [
    "for node in bm25_nodes:\n",
    "    print(node.node.node_id, '-----', node.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = QueryFusionRetriever(\n",
    "    [\n",
    "        index_retreiver,\n",
    "        bm25_retriever,\n",
    "    ],\n",
    "    num_queries=4,\n",
    "    similarity_top_k = 50,\n",
    "    llm=llm,\n",
    "    retriever_weights=[0.7, 0.3],\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Prompt Token Usage: 54\n",
      "LLM Completion Token Usage: 22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[QueryBundle(query_str='how to use gemini llm', image_path=None, custom_embedding_strs=None, embedding=None),\n",
       " QueryBundle(query_str='gemini llm embeddings tutorial', image_path=None, custom_embedding_strs=None, embedding=None),\n",
       " QueryBundle(query_str='gemini api for llm and embeddings', image_path=None, custom_embedding_strs=None, embedding=None)]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_queries = retriever._get_queries(QUERY)\n",
    "ret_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how to use gemini llm\n",
      "Embedding Token Usage: 7\n",
      "--------\n",
      "gemini llm embeddings tutorial\n",
      "Embedding Token Usage: 5\n",
      "--------\n",
      "gemini api for llm and embeddings\n",
      "Embedding Token Usage: 7\n",
      "--------\n",
      "how to use gemini as the llm and for embeddings\n",
      "Embedding Token Usage: 12\n"
     ]
    }
   ],
   "source": [
    "for ret_query in ret_queries:\n",
    "    print(ret_query)\n",
    "    emb = embed_model.get_query_embedding(ret_query.query_str)\n",
    "    print('--------')\n",
    "print(QUERY)\n",
    "emb = embed_model.get_query_embedding(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Prompt Token Usage: 54\n",
      "LLM Completion Token Usage: 22\n",
      "Embedding Token Usage: 5\n",
      "Embedding Token Usage: 7\n",
      "Embedding Token Usage: 12\n",
      "Embedding Token Usage: 7\n"
     ]
    }
   ],
   "source": [
    "hybrid_nodes = retriever.retrieve(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(Settings.tokenizer(\"You are a helpful assistant that generates multiple search queries based on a single input query. Generate 3 search queries, one on each line, related to the following input query:\\n\\\n",
    "# Query: what are callback handlers?\\n\\\n",
    "# Queries:\\n\\\n",
    "# \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid_nodes[0].dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_reference/embeddings/gemini.md-0-87 ----- 5.783661365509033\n",
      "api_reference/llms/gemini.md-0-72 ----- 5.5878520011901855\n",
      "understanding/agent/index.md-3073-3437 ----- 5.290548324584961\n",
      "module_guides/models/multi_modal.md-5156-8283 ----- 4.709981441497803\n",
      "module_guides/evaluating/evaluating_evaluators_with_llamadatasets.md-0-1865 ----- 4.363983631134033\n",
      "api_reference/multi_modal_llms/gemini.md-0-94 ----- 4.359795093536377\n",
      "module_guides/models/multi_modal.md-9705-10409 ----- 4.213951110839844\n",
      "getting_started/starter_example_local.md-0-448 ----- 3.927900791168213\n",
      "community/faq/embeddings.md-1121-1349 ----- 3.899174690246582\n",
      "understanding/indexing/indexing.md-1757-2303 ----- 3.8961241245269775\n",
      "community/faq/llms.md-25-633 ----- 3.7066307067871094\n",
      "community/faq/embeddings.md-14-494 ----- 3.7046380043029785\n",
      "understanding/evaluating/cost_analysis/index.md-3404-3688 ----- 3.6233983039855957\n",
      "understanding/indexing/indexing.md-757-1755 ----- 3.4983839988708496\n",
      "module_guides/loading/documents_and_nodes/usage_documents.md-4387-5030 ----- 3.354177951812744\n",
      "understanding/evaluating/cost_analysis/index.md-2430-2843 ----- 3.2800238132476807\n",
      "community/faq/embeddings.md-690-899 ----- 3.2625675201416016\n",
      "module_guides/loading/documents_and_nodes/usage_documents.md-3566-4385 ----- 3.2574386596679688\n",
      "community/faq/embeddings.md-901-1119 ----- 3.254964828491211\n",
      "module_guides/loading/documents_and_nodes/usage_documents.md-3406-3564 ----- 3.2417140007019043\n",
      "community/faq/embeddings.md-496-688 ----- 3.2348756790161133\n",
      "use_cases/multimodal.md-296-1148 ----- 3.201904535293579\n",
      "understanding/index.md-0-366 ----- 3.117405414581299\n",
      "community/integrations/fleet_libraries_context.md-0-272 ----- 3.0823490619659424\n",
      "api_reference/embeddings/huggingface_api.md-0-113 ----- 3.053562641143799\n",
      "understanding/index.md-4660-4750 ----- 3.0523369312286377\n",
      "api_reference/embeddings/ibm.md-0-86 ----- 3.016937255859375\n",
      "getting_started/installation.md-1002-1659 ----- 2.965393304824829\n",
      "understanding/loading/loading.md-2483-3016 ----- 2.9238569736480713\n",
      "getting_started/starter_example_local.md-450-1342 ----- 2.894493579864502\n",
      "api_reference/embeddings/llm_rails.md-0-92 ----- 2.8109354972839355\n",
      "optimizing/basic_strategies/basic_strategies.md-875-1856 ----- 2.7736611366271973\n",
      "module_guides/querying/structured_outputs/index.md-1494-2380 ----- 2.672689914703369\n",
      "understanding/evaluating/cost_analysis/usage_pattern.md-17-1995 ----- 2.6360349655151367\n",
      "community/integrations/fleet_libraries_context.md-274-517 ----- 2.5392255783081055\n",
      "getting_started/customization.md-3300-3689 ----- 2.5026724338531494\n",
      "understanding/using_llms/using_llms.md-2699-3220 ----- 2.4833669662475586\n",
      "module_guides/models/llms.md-1011-1657 ----- 2.3885960578918457\n",
      "community/faq/llms.md-635-824 ----- 2.38704776763916\n",
      "understanding/extraction/lower_level.md-0-326 ----- 2.369847297668457\n",
      "community/integrations/trulens.md-0-151 ----- 2.2794723510742188\n",
      "understanding/extraction/structured_prediction.md-1676-1835 ----- 2.2652926445007324\n",
      "module_guides/supporting_modules/settings.md-641-917 ----- 2.257711410522461\n",
      "module_guides/querying/node_postprocessors/node_postprocessors.md-6756-7013 ----- 2.2394356727600098\n",
      "understanding/agent/rag_agent.md-712-1027 ----- 2.2379114627838135\n",
      "understanding/evaluating/cost_analysis/index.md-2845-3402 ----- 2.2344141006469727\n",
      "api_reference/embeddings/google.md-0-158 ----- 0.68279145058634\n",
      "community/faq/embeddings.md-0-12 ----- 0.6821330581592868\n",
      "community/frequently_asked_questions.md-214-342 ----- 0.6628999118048435\n",
      "api_reference/embeddings/openai.md-0-87 ----- 0.6592379254825121\n"
     ]
    }
   ],
   "source": [
    "for node in hybrid_nodes:\n",
    "    print(node.node.node_id, '-----', node.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_reference/embeddings/gemini.md-0-87 ----- 0.7104416075854444\n",
      "api_reference/llms/gemini.md-0-72 ----- 0.6754126524282204\n",
      "community/faq/embeddings.md-0-12 ----- 0.6651634935775171\n",
      "community/frequently_asked_questions.md-214-342 ----- 0.6553498762103321\n",
      "api_reference/embeddings/google.md-0-158 ----- 0.6419364265094576\n",
      "module_guides/models/embeddings.md-0-12 ----- 0.6232502230085977\n",
      "api_reference/embeddings/vllm.md-0-83 ----- 0.6217949006512549\n",
      "community/faq/embeddings.md-14-494 ----- 0.6190136241012562\n",
      "community/faq/embeddings.md-901-1119 ----- 0.6188554892257974\n",
      "community/frequently_asked_questions.md-41-212 ----- 0.6184940686339753\n",
      "understanding/agent/index.md-3073-3437 ----- 0.617153228292253\n",
      "api_reference/multi_modal_llms/gemini.md-0-94 ----- 0.6167912577714008\n",
      "api_reference/embeddings/openai.md-0-87 ----- 0.6158111103662927\n",
      "api_reference/embeddings/langchain.md-0-93 ----- 0.6150433311487288\n",
      "optimizing/custom_modules.md-907-1017 ----- 0.6124784824019214\n",
      "use_cases/fine_tuning.md-0-13 ----- 0.6124703232395772\n",
      "module_guides/models/llms.md-0-12 ----- 0.6118417558320567\n",
      "optimizing/fine-tuning/fine-tuning.md-0-13 ----- 0.6116081278262465\n",
      "use_cases/fine_tuning.md-2580-2599 ----- 0.6057124079021234\n",
      "module_guides/models/multi_modal.md-9705-10409 ----- 0.6044601950225695\n",
      "understanding/index.md-4660-4750 ----- 0.6042750722200201\n",
      "community/faq/embeddings.md-496-688 ----- 0.6033644776833155\n",
      "module_guides/models/llms/usage_standalone.md-0-77 ----- 0.6031772457044445\n",
      "api_reference/llms/nebius.md-0-75 ----- 0.6013674736960761\n",
      "module_guides/loading/documents_and_nodes/usage_documents.md-3406-3564 ----- 0.6009530056331533\n",
      "understanding/using_llms/using_llms.md-2388-2697 ----- 0.6007711412770104\n",
      "community/faq/llms.md-0-23 ----- 0.5995034900597168\n",
      "module_guides/models/llms/modules.md-0-3232 ----- 0.5991658707601403\n",
      "api_reference/embeddings/llm_rails.md-0-92 ----- 0.5991650850731399\n",
      "module_guides/loading/documents_and_nodes/usage_documents.md-6774-6934 ----- 0.598766731653885\n",
      "optimizing/fine-tuning/fine-tuning.md-2589-2608 ----- 0.5962321055363659\n",
      "module_guides/models/embeddings.md-7326-8817 ----- 0.5935374872632979\n",
      "api_reference/embeddings/huggingface_api.md-0-113 ----- 0.59291746527339\n",
      "community/faq/embeddings.md-1121-1349 ----- 0.5913699428435168\n",
      "community/integrations.md-663-748 ----- 0.591045887419099\n",
      "module_guides/models/llms.md-2576-2764 ----- 0.5907046514043625\n",
      "optimizing/custom_modules.md-784-905 ----- 0.5898249829008538\n",
      "module_guides/models/multi_modal.md-5156-8283 ----- 0.5897387859312246\n",
      "module_guides/models/llms.md-2440-2574 ----- 0.5872248748838779\n",
      "understanding/evaluating/cost_analysis/usage_pattern.md-0-15 ----- 0.5871032228632694\n",
      "api_reference/embeddings/upstage.md-0-89 ----- 0.5867401316392004\n",
      "api_reference/embeddings/elasticsearch.md-0-101 ----- 0.5866572877847135\n",
      "api_reference/embeddings/ollama.md-0-87 ----- 0.58603795587408\n",
      "api_reference/embeddings/clip.md-0-83 ----- 0.585672451477047\n",
      "api_reference/embeddings/xinference.md-0-95 ----- 0.5849963444258699\n"
     ]
    }
   ],
   "source": [
    "for node in context_nodes:\n",
    "    print(node.node.node_id, '-----', node.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_reference/callbacks/langfuse.md-0-98 ----- 5.477563858032227\n",
      "understanding/tracing_and_debugging/tracing_and_debugging.md-525-1300 ----- 5.186171054840088\n",
      "api_reference/callbacks/uptrain.md-0-94 ----- 4.90223503112793\n",
      "api_reference/callbacks/promptlayer.md-0-94 ----- 4.90223503112793\n",
      "api_reference/callbacks/deepeval.md-0-98 ----- 4.90223503112793\n",
      "api_reference/callbacks/arize_phoenix.md-0-108 ----- 4.871725082397461\n",
      "api_reference/callbacks/argilla.md-0-96 ----- 4.871725082397461\n",
      "module_guides/observability/index.md-18792-18893 ----- 4.868949890136719\n",
      "community/integrations/uptrain.md-8404-8713 ----- 4.773504257202148\n",
      "community/integrations/uptrain.md-2707-3132 ----- 4.572553634643555\n",
      "module_guides/observability/index.md-16664-16947 ----- 4.515216827392578\n",
      "module_guides/observability/index.md-15309-15401 ----- 3.982264518737793\n",
      "community/integrations/uptrain.md-5279-5617 ----- 3.734769105911255\n",
      "community/integrations/uptrain.md-5619-6485 ----- 3.7272391319274902\n",
      "community/integrations/uptrain.md-3964-4395 ----- 3.7224268913269043\n",
      "community/integrations/uptrain.md-8715-9132 ----- 3.633030414581299\n",
      "module_guides/observability/callbacks/root.md-1329-2921 ----- 3.5813140869140625\n",
      "community/integrations/uptrain.md-12518-16448 ----- 3.370175361633301\n",
      "community/integrations/uptrain.md-3134-3962 ----- 3.278815746307373\n",
      "community/integrations/uptrain.md-1824-2705 ----- 3.2704269886016846\n"
     ]
    }
   ],
   "source": [
    "for node in bm25_nodes:\n",
    "    print(node.node.node_id, '-----', node.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "api_reference/embeddings/gemini.md-0-87 ----- 5.783661365509033 vector\n",
      "[Excerpt from document]\n",
      "file_path: api_reference/embeddings/gemini.md\n",
      "header_path: /\n",
      "context: This c\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "api_reference/llms/gemini.md-0-72 ----- 5.5878520011901855 vector\n",
      "[Excerpt from document]\n",
      "file_path: api_reference/llms/gemini.md\n",
      "header_path: /\n",
      "context: This chunk d\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "understanding/agent/index.md-3073-3437 ----- 5.290548324584961 vector\n",
      "[Excerpt from document]\n",
      "file_path: understanding/agent/index.md\n",
      "header_path: /Building a basic agent\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "module_guides/models/multi_modal.md-5156-8283 ----- 4.709981441497803 vector\n",
      "[Excerpt from document]\n",
      "file_path: module_guides/models/multi_modal.md\n",
      "header_path: /[Beta] Multi-mo\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: module_guides/evaluating/evaluating_evaluators_with_llamadatasets\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "api_reference/multi_modal_llms/gemini.md-0-94 ----- 4.359795093536377 vector\n",
      "[Excerpt from document]\n",
      "file_path: api_reference/multi_modal_llms/gemini.md\n",
      "header_path: /\n",
      "context: \n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "module_guides/models/multi_modal.md-9705-10409 ----- 4.213951110839844 vector\n",
      "[Excerpt from document]\n",
      "file_path: module_guides/models/multi_modal.md\n",
      "header_path: /[Beta] Multi-mo\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: getting_started/starter_example_local.md\n",
      "header_path: /\n",
      "context: \n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "community/faq/embeddings.md-1121-1349 ----- 3.899174690246582 vector\n",
      "[Excerpt from document]\n",
      "file_path: community/faq/embeddings.md\n",
      "header_path: /Embeddings/\n",
      "context: Th\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: understanding/indexing/indexing.md\n",
      "header_path: /Indexing/Vector \n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: community/faq/llms.md\n",
      "header_path: /Large Language Models/\n",
      "contex\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "community/faq/embeddings.md-14-494 ----- 3.7046380043029785 vector\n",
      "[Excerpt from document]\n",
      "file_path: community/faq/embeddings.md\n",
      "header_path: /Embeddings/\n",
      "context: Th\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: understanding/evaluating/cost_analysis/index.md\n",
      "header_path: /Cos\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: understanding/indexing/indexing.md\n",
      "header_path: /Indexing/Vector \n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: module_guides/loading/documents_and_nodes/usage_documents.md\n",
      "head\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: understanding/evaluating/cost_analysis/index.md\n",
      "header_path: /Cos\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: community/faq/embeddings.md\n",
      "header_path: /Embeddings/\n",
      "context: Th\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: module_guides/loading/documents_and_nodes/usage_documents.md\n",
      "head\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "community/faq/embeddings.md-901-1119 ----- 3.254964828491211 vector\n",
      "[Excerpt from document]\n",
      "file_path: community/faq/embeddings.md\n",
      "header_path: /Embeddings/\n",
      "context: Th\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "module_guides/loading/documents_and_nodes/usage_documents.md-3406-3564 ----- 3.2417140007019043 vector\n",
      "[Excerpt from document]\n",
      "file_path: module_guides/loading/documents_and_nodes/usage_documents.md\n",
      "head\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "community/faq/embeddings.md-496-688 ----- 3.2348756790161133 vector\n",
      "[Excerpt from document]\n",
      "file_path: community/faq/embeddings.md\n",
      "header_path: /Embeddings/\n",
      "context: Th\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: use_cases/multimodal.md\n",
      "header_path: /Multi-modal/Types of Multi-\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: understanding/index.md\n",
      "header_path: /\n",
      "context: Introduction to Ll\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: community/integrations/fleet_libraries_context.md\n",
      "header_path: /\n",
      "\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "api_reference/embeddings/huggingface_api.md-0-113 ----- 3.053562641143799 vector\n",
      "[Excerpt from document]\n",
      "file_path: api_reference/embeddings/huggingface_api.md\n",
      "header_path: /\n",
      "contex\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "understanding/index.md-4660-4750 ----- 3.0523369312286377 vector\n",
      "[Excerpt from document]\n",
      "file_path: understanding/index.md\n",
      "header_path: /Building an LLM application/\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: api_reference/embeddings/ibm.md\n",
      "header_path: /\n",
      "context: API refer\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: getting_started/installation.md\n",
      "header_path: /Installation and Se\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: understanding/loading/loading.md\n",
      "header_path: /Loading Data (Inge\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: getting_started/starter_example_local.md\n",
      "header_path: /Starter Tu\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "api_reference/embeddings/llm_rails.md-0-92 ----- 2.8109354972839355 vector\n",
      "[Excerpt from document]\n",
      "file_path: api_reference/embeddings/llm_rails.md\n",
      "header_path: /\n",
      "context: LLM\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: optimizing/basic_strategies/basic_strategies.md\n",
      "header_path: /Bas\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: module_guides/querying/structured_outputs/index.md\n",
      "header_path: /\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: understanding/evaluating/cost_analysis/usage_pattern.md\n",
      "header_pa\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: community/integrations/fleet_libraries_context.md\n",
      "header_path: /F\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: getting_started/customization.md\n",
      "header_path: /Frequently Asked Q\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: understanding/using_llms/using_llms.md\n",
      "header_path: /Using LLMs/A\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: module_guides/models/llms.md\n",
      "header_path: /Using LLMs/\n",
      "context: T\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: community/faq/llms.md\n",
      "header_path: /Large Language Models/\n",
      "contex\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: understanding/extraction/lower_level.md\n",
      "header_path: /\n",
      "context: I\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: community/integrations/trulens.md\n",
      "header_path: /\n",
      "context: Introdu\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: understanding/extraction/structured_prediction.md\n",
      "header_path: /S\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: module_guides/supporting_modules/settings.md\n",
      "header_path: /Config\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: module_guides/querying/node_postprocessors/node_postprocessors.md\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: understanding/agent/rag_agent.md\n",
      "header_path: /Adding RAG to an a\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: understanding/evaluating/cost_analysis/index.md\n",
      "header_path: /Cos\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "api_reference/embeddings/google.md-0-158 ----- 0.68279145058634 vector\n",
      "[Excerpt from document]\n",
      "file_path: api_reference/embeddings/google.md\n",
      "header_path: /\n",
      "context: Define\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "community/faq/embeddings.md-0-12 ----- 0.6821330581592868 vector\n",
      "[Excerpt from document]\n",
      "file_path: community/faq/embeddings.md\n",
      "header_path: /\n",
      "context: This is the t\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "community/frequently_asked_questions.md-214-342 ----- 0.6628999118048435 vector\n",
      "[Excerpt from document]\n",
      "file_path: community/frequently_asked_questions.md\n",
      "header_path: /Frequently \n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n",
      "api_reference/embeddings/openai.md-0-87 ----- 0.6592379254825121 vector\n",
      "[Excerpt from document]\n",
      "file_path: api_reference/embeddings/openai.md\n",
      "header_path: /\n",
      "context: OpenAI\n",
      "\n",
      "\n",
      "-------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vector_node_ids = [node.node.node_id for node in context_nodes]\n",
    "bm25_node_ids = [node.node.node_id for node in bm25_nodes]\n",
    " \n",
    "for node in hybrid_nodes:\n",
    "    if node.node.node_id in vector_node_ids:\n",
    "        print(node.node.node_id, '-----', node.score, 'vector')\n",
    "    if node.node.node_id in bm25_node_ids:\n",
    "        print(node.node.node_id, '-----', node.score, 'bm25')\n",
    "    print(node.node.get_content(metadata_mode='embed')[:100])\n",
    "    print('\\n\\n-------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10136\n"
     ]
    }
   ],
   "source": [
    "text = '\\n------------------\\n'.join([node.node.get_content(metadata_mode='embed') for node in hybrid_nodes])\n",
    "print(len(Settings.tokenizer(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine.synthesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25_retriever.persist('./bm25retreiver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Prompt Token Usage: 54\n",
      "LLM Completion Token Usage: 24\n",
      "Embedding Token Usage: 12\n",
      "Embedding Token Usage: 9\n",
      "Embedding Token Usage: 5\n",
      "Embedding Token Usage: 7\n",
      "LLM Prompt Token Usage: 10122\n",
      "LLM Completion Token Usage: 119\n",
      "To use Gemini, you can access it via an API. You can also use Gemini for multi-modal applications.\n",
      "\n",
      "The `llama_index.llms.gemini` module defines the Gemini LLM.\n",
      "\n",
      "The `llama_index.embeddings.gemini` module includes `GeminiEmbedding`.\n",
      "\n",
      "There are example notebooks for using Gemini with multi-modal applications. These notebooks demonstrate how to integrate Multi-Modal LLM models, Multi-Modal embeddings, Multi-Modal vector stores, Retrievers, and Query engines for composing Multi-Modal Retrieval Augmented Generation (RAG) orchestration.\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(QUERY)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Tokens:  379 \n",
      " LLM Prompt Tokens:  29586 \n",
      " LLM Completion Tokens:  523 \n",
      " Total LLM Token Count:  30109\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Embedding Tokens: \",\n",
    "    token_counter.total_embedding_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Prompt Tokens: \",\n",
    "    token_counter.prompt_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"LLM Completion Tokens: \",\n",
    "    token_counter.completion_llm_token_count,\n",
    "    \"\\n\",\n",
    "    \"Total LLM Token Count: \",\n",
    "    token_counter.total_llm_token_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30109"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter.total_llm_token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank for better results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "cohere_rerank = CohereRerank(\n",
    "    top_n=20, model=\"rerank-v3.5\", api_key=os.environ['COHERE_API_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked_nodes = cohere_rerank.postprocess_nodes(nodes=hybrid_nodes, query_str=QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='understanding/evaluating/cost_analysis/usage_pattern.md-17-1995', embedding=None, metadata={'file_path': 'understanding/evaluating/cost_analysis/usage_pattern.md', 'file_name': 'usage_pattern.md', 'file_size': 1996, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Usage Pattern/', 'context': 'This section details a step-by-step guide on how to use the `TokenCountingHandler` callback to estimate LLM and embedding token counts during index construction and querying.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='understanding/evaluating/cost_analysis/usage_pattern.md', node_type='4', metadata={'file_path': 'understanding/evaluating/cost_analysis/usage_pattern.md', 'file_name': 'usage_pattern.md', 'file_size': 1996, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='ba80d81d0c4b685bf85de1ca44fff44229b2901ed0ce4f02650cca77c162fe42'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='b1c03c6f-fdf0-44b4-944f-f48363f40e98', node_type='1', metadata={'file_path': 'understanding/evaluating/cost_analysis/usage_pattern.md', 'file_name': 'usage_pattern.md', 'file_size': 1996, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/'}, hash='db264eee4008de4e019f094607d916c24420149a218fdeb8648c0b030de63745')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Estimating LLM and Embedding Token Counts\\n\\nIn order to measure LLM and Embedding token counts, you\\'ll need to\\n\\n1. Setup `MockLLM` and `MockEmbedding` objects\\n\\n```python\\nfrom llama_index.core.llms import MockLLM\\nfrom llama_index.core import MockEmbedding\\n\\nllm = MockLLM(max_tokens=256)\\nembed_model = MockEmbedding(embed_dim=1536)\\n```\\n\\n2. Setup the `TokenCountingCallback` handler\\n\\n```python\\nimport tiktoken\\nfrom llama_index.core.callbacks import CallbackManager, TokenCountingHandler\\n\\ntoken_counter = TokenCountingHandler(\\n    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\\n)\\n\\ncallback_manager = CallbackManager([token_counter])\\n```\\n\\n3. Add them to the global `Settings`\\n\\n```python\\nfrom llama_index.core import Settings\\n\\nSettings.llm = llm\\nSettings.embed_model = embed_model\\nSettings.callback_manager = callback_manager\\n```\\n\\n4. Construct an Index\\n\\n```python\\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\n\\ndocuments = SimpleDirectoryReader(\\n    \"./docs/examples/data/paul_graham\"\\n).load_data()\\n\\nindex = VectorStoreIndex.from_documents(documents)\\n```\\n\\n5. Measure the counts!\\n\\n```python\\nprint(\\n    \"Embedding Tokens: \",\\n    token_counter.total_embedding_token_count,\\n    \"\\\\n\",\\n    \"LLM Prompt Tokens: \",\\n    token_counter.prompt_llm_token_count,\\n    \"\\\\n\",\\n    \"LLM Completion Tokens: \",\\n    token_counter.completion_llm_token_count,\\n    \"\\\\n\",\\n    \"Total LLM Token Count: \",\\n    token_counter.total_llm_token_count,\\n    \"\\\\n\",\\n)\\n\\n# reset counts\\ntoken_counter.reset_counts()\\n```\\n\\n6. Run a query, measure again\\n\\n```python\\nquery_engine = index.as_query_engine()\\n\\nresponse = query_engine.query(\"query\")\\n\\nprint(\\n    \"Embedding Tokens: \",\\n    token_counter.total_embedding_token_count,\\n    \"\\\\n\",\\n    \"LLM Prompt Tokens: \",\\n    token_counter.prompt_llm_token_count,\\n    \"\\\\n\",\\n    \"LLM Completion Tokens: \",\\n    token_counter.completion_llm_token_count,\\n    \"\\\\n\",\\n    \"Total LLM Token Count: \",\\n    token_counter.total_llm_token_count,\\n    \"\\\\n\",\\n)\\n```', mimetype='text/plain', start_char_idx=17, end_char_idx=1995, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.64773196),\n",
       " NodeWithScore(node=TextNode(id_='module_guides/supporting_modules/settings.md-2553-2926', embedding=None, metadata={'file_path': 'module_guides/supporting_modules/settings.md', 'file_name': 'settings.md', 'file_size': 3980, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Configuring Settings/', 'context': 'This section describes how to configure global callback managers within the LlamaIndex `Settings` object to monitor events during the indexing and querying process.  It shows how to use `TokenCountingHandler` and `CallbackManager`.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='module_guides/supporting_modules/settings.md', node_type='4', metadata={'file_path': 'module_guides/supporting_modules/settings.md', 'file_name': 'settings.md', 'file_size': 3980, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='c1e10601562291b0deae1182ae0a28bcdc3552c4817537c3feab69cae0458b75'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='39a04470-faa3-4536-bc37-6fc773760464', node_type='1', metadata={'file_path': 'module_guides/supporting_modules/settings.md', 'file_name': 'settings.md', 'file_size': 3980, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Configuring Settings/'}, hash='001ef1f48d0d576209de50e397327f89ac35bc7a5ab65cf332785a770078442d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='1b32116c-01e4-4300-bd30-f657d94d0b4c', node_type='1', metadata={'header_path': '/Configuring Settings/'}, hash='5c95126f57398fa1e21ea1c673b04aab7e90900df79b02b3483fc83ce4e88567')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Callbacks\\n\\nYou can set a global callback manager, which can be used to observe and consume events generated throughout the llama-index code\\n\\n```python\\nfrom llama_index.core.callbacks import TokenCountingHandler, CallbackManager\\nfrom llama_index.core import Settings\\n\\ntoken_counter = TokenCountingHandler()\\nSettings.callback_manager = CallbackManager([token_counter])\\n```', mimetype='text/plain', start_char_idx=2553, end_char_idx=2926, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.62852204),\n",
       " NodeWithScore(node=TextNode(id_='module_guides/observability/callbacks/token_counting_migration.md-0-2412', embedding=None, metadata={'file_path': 'module_guides/observability/callbacks/token_counting_migration.md', 'file_name': 'token_counting_migration.md', 'file_size': 2413, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/', 'context': 'This section details the deprecation of the old token counting method and introduces the new `TokenCountingHandler` callback, providing a code example demonstrating its usage with an OpenAI model.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='module_guides/observability/callbacks/token_counting_migration.md', node_type='4', metadata={'file_path': 'module_guides/observability/callbacks/token_counting_migration.md', 'file_name': 'token_counting_migration.md', 'file_size': 2413, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='bd2891d74df4e81a6220cd4dca889faf9a25bec30734242e62fa9b3ee99bf106')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='# Token Counting - Migration Guide\\n\\nThe existing token counting implementation has been **deprecated**.\\n\\nWe know token counting is important to many users, so this guide was created to walkthrough a (hopefully painless) transition.\\n\\nPreviously, token counting was kept track of on the `llm_predictor` and `embed_model` objects directly, and optionally printed to the console. This implementation used a static tokenizer for token counting (gpt-2), and the `last_token_usage` and `total_token_usage` attributes were not always kept track of properly.\\n\\nGoing forward, token counting as moved into a callback. Using the `TokenCountingHandler` callback, you now have more options for how tokens are counted, the lifetime of the token counts, and even creating separate token counters for different indexes.\\n\\nHere is a minimum example of using the new `TokenCountingHandler` with an OpenAI model:\\n\\n```python\\nimport tiktoken\\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\\nfrom llama_index.core.callbacks import CallbackManager, TokenCountingHandler\\nfrom llama_index.core import Settings\\n\\n# you can set a tokenizer directly, or optionally let it default\\n# to the same tokenizer that was used previously for token counting\\n# NOTE: The tokenizer should be a function that takes in text and returns a list of tokens\\ntoken_counter = TokenCountingHandler(\\n    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode,\\n    verbose=False,  # set to true to see usage printed to the console\\n)\\n\\nSettings.callback_manager = CallbackManager([token_counter])\\n\\ndocument = SimpleDirectoryReader(\"./data\").load_data()\\n\\n# if verbose is turned on, you will see embedding token usage printed\\nindex = VectorStoreIndex.from_documents(\\n    documents,\\n)\\n\\n# otherwise, you can access the count directly\\nprint(token_counter.total_embedding_token_count)\\n\\n# reset the counts at your discretion!\\ntoken_counter.reset_counts()\\n\\n# also track prompt, completion, and total LLM tokens, in addition to embeddings\\nresponse = index.as_query_engine().query(\"What did the author do growing up?\")\\nprint(\\n    \"Embedding Tokens: \",\\n    token_counter.total_embedding_token_count,\\n    \"\\\\n\",\\n    \"LLM Prompt Tokens: \",\\n    token_counter.prompt_llm_token_count,\\n    \"\\\\n\",\\n    \"LLM Completion Tokens: \",\\n    token_counter.completion_llm_token_count,\\n    \"\\\\n\",\\n    \"Total LLM Token Count: \",\\n    token_counter.total_llm_token_count,\\n)\\n```', mimetype='text/plain', start_char_idx=0, end_char_idx=2412, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.5942929),\n",
       " NodeWithScore(node=TextNode(id_='api_reference/callbacks/token_counter.md-0-104', embedding=None, metadata={'file_path': 'api_reference/callbacks/token_counter.md', 'file_name': 'token_counter.md', 'file_size': 105, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/', 'context': 'This document describes the `llama_index.core.callbacks.token_counting` module.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='api_reference/callbacks/token_counter.md', node_type='4', metadata={'file_path': 'api_reference/callbacks/token_counter.md', 'file_name': 'token_counter.md', 'file_size': 105, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='d7b8f2c6953bbc3342e876776526e49ec18c253f65f0d9da30e9b9ec7f7a6a91')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='::: llama_index.core.callbacks.token_counting\\n    options:\\n      members:\\n        - TokenCountingHandler', mimetype='text/plain', start_char_idx=0, end_char_idx=104, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.52568597),\n",
       " NodeWithScore(node=TextNode(id_='module_guides/models/llms/usage_custom.md-5772-8958', embedding=None, metadata={'file_path': 'module_guides/models/llms/usage_custom.md', 'file_name': 'usage_custom.md', 'file_size': 8959, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Customizing LLMs within LlamaIndex Abstractions/', 'context': 'This section details how to use a custom LLM model with LlamaIndex by implementing the `LLM` or `CustomLLM` class, providing a complete example with code and explanations.  It also notes the need for potentially adjusting internal prompts and mentions resources for default and custom prompts.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='module_guides/models/llms/usage_custom.md', node_type='4', metadata={'file_path': 'module_guides/models/llms/usage_custom.md', 'file_name': 'usage_custom.md', 'file_size': 8959, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='a0d0eff2de47d1e485755f09d025064441ce8bb7850fec7692868fdcd4d4deb3'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8bc34a86-7a43-49f2-b8c2-fa485698af03', node_type='1', metadata={'file_path': 'module_guides/models/llms/usage_custom.md', 'file_name': 'usage_custom.md', 'file_size': 8959, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Customizing LLMs within LlamaIndex Abstractions/'}, hash='bcf4e140dadaed9794f27554dd3ae545b94e43fd198b1222fbc14b51800503a4')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Example: Using a Custom LLM Model - Advanced\\n\\nTo use a custom LLM model, you only need to implement the `LLM` class (or `CustomLLM` for a simpler interface)\\nYou will be responsible for passing the text to the model and returning the newly generated tokens.\\n\\nThis implementation could be some local model, or even a wrapper around your own API.\\n\\nNote that for a completely private experience, also setup a [local embeddings model](../embeddings.md).\\n\\nHere is a small boilerplate example:\\n\\n```python\\nfrom typing import Optional, List, Mapping, Any\\n\\nfrom llama_index.core import SimpleDirectoryReader, SummaryIndex\\nfrom llama_index.core.callbacks import CallbackManager\\nfrom llama_index.core.llms import (\\n    CustomLLM,\\n    CompletionResponse,\\n    CompletionResponseGen,\\n    LLMMetadata,\\n)\\nfrom llama_index.core.llms.callbacks import llm_completion_callback\\nfrom llama_index.core import Settings\\n\\n\\nclass OurLLM(CustomLLM):\\n    context_window: int = 3900\\n    num_output: int = 256\\n    model_name: str = \"custom\"\\n    dummy_response: str = \"My response\"\\n\\n    @property\\n    def metadata(self) -> LLMMetadata:\\n        \"\"\"Get LLM metadata.\"\"\"\\n        return LLMMetadata(\\n            context_window=self.context_window,\\n            num_output=self.num_output,\\n            model_name=self.model_name,\\n        )\\n\\n    @llm_completion_callback()\\n    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\\n        return CompletionResponse(text=self.dummy_response)\\n\\n    @llm_completion_callback()\\n    def stream_complete(\\n        self, prompt: str, **kwargs: Any\\n    ) -> CompletionResponseGen:\\n        response = \"\"\\n        for token in self.dummy_response:\\n            response += token\\n            yield CompletionResponse(text=response, delta=token)\\n\\n\\n# define our LLM\\nSettings.llm = OurLLM()\\n\\n# define embed model\\nSettings.embed_model = \"local:BAAI/bge-base-en-v1.5\"\\n\\n\\n# Load the your data\\ndocuments = SimpleDirectoryReader(\"./data\").load_data()\\nindex = SummaryIndex.from_documents(documents)\\n\\n# Query and print response\\nquery_engine = index.as_query_engine()\\nresponse = query_engine.query(\"<query_text>\")\\nprint(response)\\n```\\n\\nUsing this method, you can use any LLM. Maybe you have one running locally, or running on your own server. As long as the class is implemented and the generated tokens are returned, it should work out. Note that we need to use the prompt helper to customize the prompt sizes, since every model has a slightly different context length.\\n\\nThe decorator is optional, but provides observability via callbacks on the LLM calls.\\n\\nNote that you may have to adjust the internal prompts to get good performance. Even then, you should be using a sufficiently large LLM to ensure it\\'s capable of handling the complex queries that LlamaIndex uses internally, so your mileage may vary.\\n\\nA list of all default internal prompts is available [here](https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/prompts/default_prompts.py), and chat-specific prompts are listed [here](https://github.com/run-llama/llama_index/blob/main/llama_index/prompts/chat_prompts.py). You can also implement [your own custom prompts](../prompts/index.md).', mimetype='text/plain', start_char_idx=5772, end_char_idx=8958, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.44217938),\n",
       " NodeWithScore(node=TextNode(id_='module_guides/observability/callbacks/index.md-1329-2667', embedding=None, metadata={'file_path': 'module_guides/observability/callbacks/index.md', 'file_name': 'index.md', 'file_size': 2668, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Callbacks/', 'context': 'This section lists and describes the currently supported LlamaIndex callbacks, including links to their example notebooks and documentation.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='module_guides/observability/callbacks/index.md', node_type='4', metadata={'file_path': 'module_guides/observability/callbacks/index.md', 'file_name': 'index.md', 'file_size': 2668, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='54b31933e831c3d56e020b6080d6a3cf9afc928cef63f8a59176e51c631e67db'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='ab023c43-7e2d-4a12-8743-c7b08aab3ecc', node_type='1', metadata={'file_path': 'module_guides/observability/callbacks/index.md', 'file_name': 'index.md', 'file_size': 2668, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Callbacks/'}, hash='83ab119ef011ee8648f5643ffe8d6c6eb0935175fd35a07a0de58559f99bac98')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Modules\\n\\nCurrently supported callbacks are as follows:\\n\\n- [TokenCountingHandler](../../../examples/callbacks/TokenCountingHandler.ipynb) -> Flexible token counting for prompt, completion, and embedding token usage. See [the migration details](../callbacks/token_counting_migration.md)\\n- [LlamaDebugHanlder](../../../examples/callbacks/LlamaDebugHandler.ipynb) -> Basic tracking and tracing for events. Example usage can be found in the notebook below.\\n- [WandbCallbackHandler](../../../examples/callbacks/WandbCallbackHandler.ipynb) -> Tracking of events and traces using the Wandb Prompts frontend. More details are in the notebook below or at [Wandb](https://docs.wandb.ai/guides/prompts/quickstart)\\n- [AimCallback](../../../examples/callbacks/AimCallback.ipynb) -> Tracking of LLM inputs and outputs. Example usage can be found in the notebook below.\\n- [OpenInferenceCallbackHandler](../../../examples/callbacks/OpenInferenceCallback.ipynb) -> Tracking of AI model inferences. Example usage can be found in the notebook below.\\n- [OpenAIFineTuningHandler](https://github.com/jerryjliu/llama_index/blob/main/experimental/openai_fine_tuning/openai_fine_tuning.ipynb) -> Records all LLM inputs and outputs. Then, provides a function `save_finetuning_events()` to save inputs and outputs in a format suitable for fine-tuning with OpenAI.', mimetype='text/plain', start_char_idx=1329, end_char_idx=2667, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.4272061),\n",
       " NodeWithScore(node=TextNode(id_='module_guides/observability/callbacks/index.md-13-1327', embedding=None, metadata={'file_path': 'module_guides/observability/callbacks/index.md', 'file_name': 'index.md', 'file_size': 2668, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Callbacks/', 'context': 'This section introduces the concept of callbacks in LlamaIndex, explaining their purpose, functionality, and the types of events they can track.  It precedes a list of available callback modules.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='module_guides/observability/callbacks/index.md', node_type='4', metadata={'file_path': 'module_guides/observability/callbacks/index.md', 'file_name': 'index.md', 'file_size': 2668, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='54b31933e831c3d56e020b6080d6a3cf9afc928cef63f8a59176e51c631e67db'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='43ed8db5-6a92-454c-b0b0-f7acbcdf699b', node_type='1', metadata={'file_path': 'module_guides/observability/callbacks/index.md', 'file_name': 'index.md', 'file_size': 2668, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/'}, hash='fbf224ec8fe87a68029a2efbced611db532cd93024dc168a6ac647b94c3382ff'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='3fd00b6e-0070-4303-af74-38ed2a2f35f4', node_type='1', metadata={'header_path': '/Callbacks/'}, hash='50a0d5202e87f6332b7701b4737704636d9d2478250e3228f8c91f49b6cd3328')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Concept\\n\\nLlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library.\\nUsing the callback manager, as many callbacks as needed can be added.\\n\\nIn addition to logging data related to events, you can also track the duration and number of occurrences\\nof each event.\\n\\nFurthermore, a trace map of events is also recorded, and callbacks can use this data\\nhowever they want. For example, the `LlamaDebugHandler` will, by default, print the trace of events\\nafter most operations.\\n\\n**Callback Event Types**\\nWhile each callback may not leverage each event type, the following events are available to be tracked:\\n\\n- `CHUNKING` -> Logs for the before and after of text splitting.\\n- `NODE_PARSING` -> Logs for the documents and the nodes that they are parsed into.\\n- `EMBEDDING` -> Logs for the number of texts embedded.\\n- `LLM` -> Logs for the template and response of LLM calls.\\n- `QUERY` -> Keeps track of the start and end of each query.\\n- `RETRIEVE` -> Logs for the nodes retrieved for a query.\\n- `SYNTHESIZE` -> Logs for the result for synthesize calls.\\n- `TREE` -> Logs for the summary and level of summaries generated.\\n- `SUB_QUESTION` -> Log for a generated sub question and answer.\\n\\nYou can implement your own callback to track and trace these events, or use an existing callback.', mimetype='text/plain', start_char_idx=13, end_char_idx=1327, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.42405477),\n",
       " NodeWithScore(node=TextNode(id_='module_guides/observability/callbacks/root.md-13-1327', embedding=None, metadata={'file_path': 'module_guides/observability/callbacks/root.md', 'file_name': 'root.md', 'file_size': 2922, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Callbacks/', 'context': 'This section introduces the concept of callbacks in LlamaIndex, explaining their purpose, functionality, and the types of events they can track.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='module_guides/observability/callbacks/root.md', node_type='4', metadata={'file_path': 'module_guides/observability/callbacks/root.md', 'file_name': 'root.md', 'file_size': 2922, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='0747d45ec9c85da7ba40d75bccaad04f5a18e2fe3047ccf6c603c6f3a750a66e'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='5feb0c6d-bb17-46ee-81ed-185f7375497a', node_type='1', metadata={'file_path': 'module_guides/observability/callbacks/root.md', 'file_name': 'root.md', 'file_size': 2922, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/'}, hash='8a552d613415c3ea60925f15fbcb400a1551ab1abbc0a8b78e896e897676289e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e279b6e1-28df-4f40-b8ff-f56e10f3e401', node_type='1', metadata={'header_path': '/Callbacks/'}, hash='7c3e568e4f302434924d539e5a2e544cb36cd9f505b5c46edcaee6e15eb04f85')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Concept\\n\\nLlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library.\\nUsing the callback manager, as many callbacks as needed can be added.\\n\\nIn addition to logging data related to events, you can also track the duration and number of occurrences\\nof each event.\\n\\nFurthermore, a trace map of events is also recorded, and callbacks can use this data\\nhowever they want. For example, the `LlamaDebugHandler` will, by default, print the trace of events\\nafter most operations.\\n\\n**Callback Event Types**\\nWhile each callback may not leverage each event type, the following events are available to be tracked:\\n\\n- `CHUNKING` -> Logs for the before and after of text splitting.\\n- `NODE_PARSING` -> Logs for the documents and the nodes that they are parsed into.\\n- `EMBEDDING` -> Logs for the number of texts embedded.\\n- `LLM` -> Logs for the template and response of LLM calls.\\n- `QUERY` -> Keeps track of the start and end of each query.\\n- `RETRIEVE` -> Logs for the nodes retrieved for a query.\\n- `SYNTHESIZE` -> Logs for the result for synthesize calls.\\n- `TREE` -> Logs for the summary and level of summaries generated.\\n- `SUB_QUESTION` -> Log for a generated sub question and answer.\\n\\nYou can implement your own callback to track and trace these events, or use an existing callback.', mimetype='text/plain', start_char_idx=13, end_char_idx=1327, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.42062396),\n",
       " NodeWithScore(node=TextNode(id_='module_guides/observability/callbacks/root.md-1329-2921', embedding=None, metadata={'file_path': 'module_guides/observability/callbacks/root.md', 'file_name': 'root.md', 'file_size': 2922, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Callbacks/', 'context': 'This section lists and describes the currently supported callback handlers in LlamaIndex, providing links to further documentation and examples for each.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='module_guides/observability/callbacks/root.md', node_type='4', metadata={'file_path': 'module_guides/observability/callbacks/root.md', 'file_name': 'root.md', 'file_size': 2922, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='0747d45ec9c85da7ba40d75bccaad04f5a18e2fe3047ccf6c603c6f3a750a66e'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='a35fbb04-0156-40b9-920d-e90901bec2b6', node_type='1', metadata={'file_path': 'module_guides/observability/callbacks/root.md', 'file_name': 'root.md', 'file_size': 2922, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Callbacks/'}, hash='663c6adaa77fb1f0310372a183ee8d0685fc3f413997a013dc0ecce7be56224c')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Modules\\n\\nCurrently supported callbacks are as follows:\\n\\n- [LangfuseCallbackHandler](../../../examples/callbacks/LangfuseCallbackHandler.ipynb) -> Tracking of events and traces using the open-source platform Langfuse. More details are in the linked notebook or in the [Langfuse docs](https://langfuse.com/docs)\\n- [TokenCountingHandler](../../../examples/callbacks/TokenCountingHandler.ipynb) -> Flexible token counting for prompt, completion, and embedding token usage. See [the migration details](../callbacks/token_counting_migration.md)\\n- [LlamaDebugHanlder](../../../examples/callbacks/LlamaDebugHandler.ipynb) -> Basic tracking and tracing for events. Example usage can be found in the notebook below.\\n- [WandbCallbackHandler](../../../examples/callbacks/WandbCallbackHandler.ipynb) -> Tracking of events and traces using the Wandb Prompts frontend. More details are in the notebook below or at [Wandb](https://docs.wandb.ai/guides/prompts/quickstart)\\n- [AimCallback](../../../examples/callbacks/AimCallback.ipynb) -> Tracking of LLM inputs and outputs. Example usage can be found in the notebook below.\\n- [OpenInferenceCallbackHandler](../../../examples/callbacks/OpenInferenceCallback.ipynb) -> Tracking of AI model inferences. Example usage can be found in the notebook below.\\n- [OpenAIFineTuningHandler](https://github.com/jerryjliu/llama_index/blob/main/experimental/openai_fine_tuning/openai_fine_tuning.ipynb) -> Records all LLM inputs and outputs. Then, provides a function `save_finetuning_events()` to save inputs and outputs in a format suitable for fine-tuning with OpenAI.', mimetype='text/plain', start_char_idx=1329, end_char_idx=2921, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.41506538),\n",
       " NodeWithScore(node=TextNode(id_='understanding/evaluating/cost_analysis/index.md-2430-2843', embedding=None, metadata={'file_path': 'understanding/evaluating/cost_analysis/index.md', 'file_name': 'index.md', 'file_size': 3782, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Cost Analysis/', 'context': 'This section describes how LlamaIndex uses token predictors (MockLLM and MockEmbedding) to estimate LLM and embedding costs during index building and querying, leveraging the TokenCountingHandler callback for token counting.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='understanding/evaluating/cost_analysis/index.md', node_type='4', metadata={'file_path': 'understanding/evaluating/cost_analysis/index.md', 'file_name': 'index.md', 'file_size': 3782, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='58f1cbec6a1bee779ddc3a46bb86948bf667d1649109ff5c629c824af236d7ba'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='292d8ac5-3ead-4416-bf04-e5004207829c', node_type='1', metadata={'file_path': 'understanding/evaluating/cost_analysis/index.md', 'file_name': 'index.md', 'file_size': 3782, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Cost Analysis/Concept/'}, hash='439cfec5e88c8175fdb379e2b6c1a39ede3a5f82b154dde65f1f99b3fab945df'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='05d4aa01-e942-4dbb-b35d-2b805efc46c8', node_type='1', metadata={'header_path': '/Cost Analysis/Usage Pattern/'}, hash='8876f342490d8c64dbd7b3b14743ea9baf24b43964f31e7912709bcba4300c58')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Usage Pattern\\n\\nLlamaIndex offers token **predictors** to predict token usage of LLM and embedding calls.\\nThis allows you to estimate your costs during 1) index construction, and 2) index querying, before\\nany respective LLM calls are made.\\n\\nTokens are counted using the `TokenCountingHandler` callback. See the [example notebook](../../../examples/callbacks/TokenCountingHandler.ipynb) for details on the setup.', mimetype='text/plain', start_char_idx=2430, end_char_idx=2843, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.41108778),\n",
       " NodeWithScore(node=TextNode(id_='module_guides/querying/response_synthesizers/index.md-5703-7289', embedding=None, metadata={'file_path': 'module_guides/querying/response_synthesizers/index.md', 'file_name': 'index.md', 'file_size': 9414, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Response Synthesizer/', 'context': 'This section details how to create custom response synthesizers by inheriting from `BaseSynthesizer`, showing the `__init__` function and abstract methods (`get_response`, `aget_response`) required for custom implementations.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='module_guides/querying/response_synthesizers/index.md', node_type='4', metadata={'file_path': 'module_guides/querying/response_synthesizers/index.md', 'file_name': 'index.md', 'file_size': 9414, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='1ba0e0b4b5783a5470349408c043ad37f72a068d643933b32ccdb6b7f8907416'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='c8d7aa6b-65ce-460b-8f67-a514840e2d71', node_type='1', metadata={'file_path': 'module_guides/querying/response_synthesizers/index.md', 'file_name': 'index.md', 'file_size': 9414, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Response Synthesizer/'}, hash='863bd1b6aefb608c0643523751a861a9caf885243c74256ba32963bc3636960d'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='cb7aead2-d534-47e1-b102-0381c466a398', node_type='1', metadata={'header_path': '/Response Synthesizer/'}, hash='d442ac141e81df22fdba3224eb5a1ad94cc2d27a122b33013d75e22bc137771e')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Custom Response Synthesizers\\n\\nEach response synthesizer inherits from `llama_index.response_synthesizers.base.BaseSynthesizer`. The base API is extremely simple, which makes it easy to create your own response synthesizer.\\n\\nMaybe you want to customize which template is used at each step in `tree_summarize`, or maybe a new research paper came out detailing a new way to generate a response to a query, you can create your own response synthesizer and plug it into any query engine or use it on it\\'s own.\\n\\nBelow we show the `__init__()` function, as well as the two abstract methods that every response synthesizer must implement. The basic requirements are to process a query and text chunks, and return a string (or string generator) response.\\n\\n```python\\nfrom llama_index.core import Settings\\n\\n\\nclass BaseSynthesizer(ABC):\\n    \"\"\"Response builder class.\"\"\"\\n\\n    def __init__(\\n        self,\\n        llm: Optional[LLM] = None,\\n        streaming: bool = False,\\n    ) -> None:\\n        \"\"\"Init params.\"\"\"\\n        self._llm = llm or Settings.llm\\n        self._callback_manager = Settings.callback_manager\\n        self._streaming = streaming\\n\\n    @abstractmethod\\n    def get_response(\\n        self,\\n        query_str: str,\\n        text_chunks: Sequence[str],\\n        **response_kwargs: Any,\\n    ) -> RESPONSE_TEXT_TYPE:\\n        \"\"\"Get response.\"\"\"\\n        ...\\n\\n    @abstractmethod\\n    async def aget_response(\\n        self,\\n        query_str: str,\\n        text_chunks: Sequence[str],\\n        **response_kwargs: Any,\\n    ) -> RESPONSE_TEXT_TYPE:\\n        \"\"\"Get response.\"\"\"\\n        ...\\n```', mimetype='text/plain', start_char_idx=5703, end_char_idx=7289, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.24603528),\n",
       " NodeWithScore(node=TextNode(id_='module_guides/observability/index.md-12084-12581', embedding=None, metadata={'file_path': 'module_guides/observability/index.md', 'file_name': 'index.md', 'file_size': 22196, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Observability/Other Partner `One-Click` Integrations (Legacy Modules)/', 'context': 'This section describes Langfuse, a partner integration for LlamaIndex observability, using the legacy CallbackManager.  It details how to use Langfuse to track and monitor LlamaIndex application performance and traces.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='module_guides/observability/index.md', node_type='4', metadata={'file_path': 'module_guides/observability/index.md', 'file_name': 'index.md', 'file_size': 22196, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='18eaa8a211138b6264551e73499d4587d0689761a2baac6cfd085a52746039e5'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='55cca7cb-8c38-4c14-81fc-45ef01c9cad6', node_type='1', metadata={'file_path': 'module_guides/observability/index.md', 'file_name': 'index.md', 'file_size': 22196, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Observability/'}, hash='0369d5e62545900895332398b5eef826fc515d41b1d146c7524eeaf57d1fd68a'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='f3f0571a-3aea-40ff-8355-b8d5187a9885', node_type='1', metadata={'header_path': '/Observability/Other Partner `One-Click` Integrations (Legacy Modules)/Langfuse/'}, hash='b02b5ff59c748c3a03499630b9ff5964e69df6fff5ad6fac458c498dd9a4ae84')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='### Langfuse\\n\\n[Langfuse](https://langfuse.com/docs) is an open source LLM engineering platform to help teams collaboratively debug, analyze and iterate on their LLM Applications. With the Langfuse integration, you can seamlessly track and monitor performance, traces, and metrics of your LlamaIndex application. Detailed [traces](https://langfuse.com/docs/tracing) of the LlamaIndex context augmentation and the LLM querying processes are captured and can be inspected directly in the Langfuse UI.', mimetype='text/plain', start_char_idx=12084, end_char_idx=12581, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.23675482),\n",
       " NodeWithScore(node=TextNode(id_='module_guides/supporting_modules/settings.md-2126-2551', embedding=None, metadata={'file_path': 'module_guides/supporting_modules/settings.md', 'file_name': 'settings.md', 'file_size': 3980, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Configuring Settings/', 'context': 'This section describes how to configure the `tokenizer` attribute within the `Settings` object, specifying tokenizers for OpenAI and open-source LLMs.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='module_guides/supporting_modules/settings.md', node_type='4', metadata={'file_path': 'module_guides/supporting_modules/settings.md', 'file_name': 'settings.md', 'file_size': 3980, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='c1e10601562291b0deae1182ae0a28bcdc3552c4817537c3feab69cae0458b75'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='805188e3-3c8c-4ca7-9107-fdddfc986737', node_type='1', metadata={'file_path': 'module_guides/supporting_modules/settings.md', 'file_name': 'settings.md', 'file_size': 3980, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Configuring Settings/'}, hash='cb707f12e9fcf6dc3d4c8c9eb0965f92fb492e254240ed9e0a937ecde78b0e0c'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='27fb7de3-a759-4571-8f3e-f264ff644227', node_type='1', metadata={'header_path': '/Configuring Settings/'}, hash='11ed2cc1a85f014bb0bc46f3c53c36bcf1cf81b9a22e7e301f23c353b01ef5d0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Tokenizer\\n\\nThe tokenizer is used to count tokens. This should be set to something that matches the LLM you are using.\\n\\n```python\\nfrom llama_index.core import Settings\\n\\n# openai\\nimport tiktoken\\n\\nSettings.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\\n\\n# open-source\\nfrom transformers import AutoTokenizer\\n\\nSettings.tokenzier = AutoTokenizer.from_pretrained(\\n    \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\\n)\\n```', mimetype='text/plain', start_char_idx=2126, end_char_idx=2551, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.22970837),\n",
       " NodeWithScore(node=TextNode(id_='understanding/tracing_and_debugging/tracing_and_debugging.md-525-1300', embedding=None, metadata={'file_path': 'understanding/tracing_and_debugging/tracing_and_debugging.md', 'file_name': 'tracing_and_debugging.md', 'file_size': 1932, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Tracing and Debugging/', 'context': \"This section details how to use LlamaIndex callbacks for debugging and tracing application behavior, including setting a simple handler and creating custom handlers.  It's part of a larger discussion on debugging and tracing LlamaIndex applications.\\n\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='understanding/tracing_and_debugging/tracing_and_debugging.md', node_type='4', metadata={'file_path': 'understanding/tracing_and_debugging/tracing_and_debugging.md', 'file_name': 'tracing_and_debugging.md', 'file_size': 1932, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='f193152e7e5ae5b3d93f515be95936e74439273be5e830ba341cbf215ac907a9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='06efcf49-d538-4b67-9c55-b1c442704488', node_type='1', metadata={'file_path': 'understanding/tracing_and_debugging/tracing_and_debugging.md', 'file_name': 'tracing_and_debugging.md', 'file_size': 1932, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Tracing and Debugging/'}, hash='c53d4b034777abc0a211cec53ccc5371eb2f1509bfe6e9740487488b074c6eed'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='a31d7adc-1912-43f1-9584-47821ad1db22', node_type='1', metadata={'header_path': '/Tracing and Debugging/'}, hash='3a5e1e10b46eb33c77504b8a2ab8e40d1038578b42e64c8d7b074e0eaf57c6d8')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Callback handler\\n\\nLlamaIndex provides callbacks to help debug, track, and trace the inner workings of the library. Using the callback manager, as many callbacks as needed can be added.\\n\\nIn addition to logging data related to events, you can also track the duration and number of occurrences\\nof each event.\\n\\nFurthermore, a trace map of events is also recorded, and callbacks can use this data however they want. For example, the `LlamaDebugHandler` will, by default, print the trace of events after most operations.\\n\\nYou can get a simple callback handler like this:\\n\\n```python\\nimport llama_index.core\\n\\nllama_index.core.set_global_handler(\"simple\")\\n```\\n\\nYou can also learn how to [build you own custom callback handler](../../module_guides/observability/callbacks/index.md).', mimetype='text/plain', start_char_idx=525, end_char_idx=1300, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.19511858),\n",
       " NodeWithScore(node=TextNode(id_='getting_started/concepts.md-3187-5145', embedding=None, metadata={'file_path': 'getting_started/concepts.md', 'file_name': 'concepts.md', 'file_size': 5146, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/High-Level Concepts/', 'context': 'This section outlines four main use cases for LlamaIndex applications: Agents, Workflows, Structured Data Extraction, and Query/Chat Engines,  providing brief descriptions and links to further documentation.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='getting_started/concepts.md', node_type='4', metadata={'file_path': 'getting_started/concepts.md', 'file_name': 'concepts.md', 'file_size': 5146, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='ee7bacd4b793908ad67f95551df94705bd304859e91050445d46723ac9494cc8'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7acba5ba-0bac-40a0-a144-537baa50f4f8', node_type='1', metadata={'file_path': 'getting_started/concepts.md', 'file_name': 'concepts.md', 'file_size': 5146, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/High-Level Concepts/'}, hash='4ce037aa807c0816505949156d07ddac42dd9cb644aeab20aa75db65aad6a5bd')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='## Use cases\\n\\nThere are endless use cases for data-backed LLM applications but they can be roughly grouped into four categories:\\n\\n[**Agents**](../module_guides/deploying/agents/index.md):\\nAn agent is an automated decision-maker powered by an LLM that interacts with the world via a set of [tools](../module_guides/deploying/agents/tools.md). Agents can take an arbitrary number of steps to complete a given task, dynamically deciding on the best course of action rather than following pre-determined steps. This gives it additional flexibility to tackle more complex tasks.\\n\\n[**Workflows**](../module_guides/workflow/index.md):\\nA Workflow in LlamaIndex is a specific event-driven abstraction that allows you to orchestrate a sequence of steps and LLMs calls. Workflows can be used to implement any agentic application, and are a core component of LlamaIndex.\\n\\n[**Structured Data Extraction**](../use_cases/extraction.md)\\nPydantic extractors allow you to specify a precise data structure to extract from your data and use LLMs to fill in the missing pieces in a type-safe way. This is useful for extracting structured data from unstructured sources like PDFs, websites, and more, and is key to automating workflows.\\n\\n[**Query Engines**](../module_guides/deploying/query_engine/index.md):\\nA query engine is an end-to-end flow that allows you to ask questions over your data. It takes in a natural language query, and returns a response, along with reference context retrieved and passed to the LLM.\\n\\n[**Chat Engines**](../module_guides/deploying/chat_engines/index.md):\\nA chat engine is an end-to-end flow for having a conversation with your data (multiple back-and-forth instead of a single question-and-answer).\\n\\n!!! tip\\n    * Tell me how to [customize things](./customization.md)\\n    * Continue learning with our [understanding LlamaIndex](../understanding/index.md) guide\\n    * Ready to dig deep? Check out the [component guides](../module_guides/index.md)', mimetype='text/plain', start_char_idx=3187, end_char_idx=5145, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.15429981),\n",
       " NodeWithScore(node=TextNode(id_='module_guides/deploying/query_engine/streaming.md-1088-2054', embedding=None, metadata={'file_path': 'module_guides/deploying/query_engine/streaming.md', 'file_name': 'streaming.md', 'file_size': 2055, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Streaming/', 'context': 'This section details how to use the `StreamingResponse` object returned by a query engine configured for streaming, showing how to iterate over the generated tokens or directly print the streaming response.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='module_guides/deploying/query_engine/streaming.md', node_type='4', metadata={'file_path': 'module_guides/deploying/query_engine/streaming.md', 'file_name': 'streaming.md', 'file_size': 2055, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='c357e826a2c9c1375c3923bb0a637033be7da89e6ab7edbf25c6d818b23804ef'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='6f66dd11-eef3-487f-bdf6-f564c1e780af', node_type='1', metadata={'file_path': 'module_guides/deploying/query_engine/streaming.md', 'file_name': 'streaming.md', 'file_size': 2055, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Streaming/'}, hash='13a99a092eb7a06f75009f66c3db2853da4d9f9a8755c6990219ff9b7c22ac15')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='### Streaming Response\\n\\nAfter properly configuring both the LLM and the query engine,\\ncalling `query` now returns a `StreamingResponse` object.\\n\\n```python\\nstreaming_response = query_engine.query(\\n    \"What did the author do growing up?\",\\n)\\n```\\n\\nThe response is returned immediately when the LLM call _starts_, without having to wait for the full completion.\\n\\n> Note: In the case where the query engine makes multiple LLM calls, only the last LLM call will be streamed and the response is returned when the last LLM call starts.\\n\\nYou can obtain a `Generator` from the streaming response and iterate over the tokens as they arrive:\\n\\n```python\\nfor text in streaming_response.response_gen:\\n    # do something with text as they arrive.\\n    pass\\n```\\n\\nAlternatively, if you just want to print the text as they arrive:\\n\\n```\\nstreaming_response.print_response_stream()\\n```\\n\\nSee an [end-to-end example](../../../examples/customization/streaming/SimpleIndexDemo-streaming.ipynb)', mimetype='text/plain', start_char_idx=1088, end_char_idx=2054, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.13990116),\n",
       " NodeWithScore(node=TextNode(id_='understanding/tracing_and_debugging/tracing_and_debugging.md-1302-1931', embedding=None, metadata={'file_path': 'understanding/tracing_and_debugging/tracing_and_debugging.md', 'file_name': 'tracing_and_debugging.md', 'file_size': 1932, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Tracing and Debugging/', 'context': \"This section describes LlamaIndex's advanced observability features, offering one-click integration with partner tools for production-level debugging and monitoring of LLM applications.\\n\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='understanding/tracing_and_debugging/tracing_and_debugging.md', node_type='4', metadata={'file_path': 'understanding/tracing_and_debugging/tracing_and_debugging.md', 'file_name': 'tracing_and_debugging.md', 'file_size': 1932, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='f193152e7e5ae5b3d93f515be95936e74439273be5e830ba341cbf215ac907a9'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='8043c5d1-caf8-45cc-b414-023630b6b1a6', node_type='1', metadata={'file_path': 'understanding/tracing_and_debugging/tracing_and_debugging.md', 'file_name': 'tracing_and_debugging.md', 'file_size': 1932, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Tracing and Debugging/'}, hash='31340e4cacbbb67ceac6f34d060982d706852f248f77906b442b10a23d01b931')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"## Observability\\n\\nLlamaIndex provides **one-click observability** to allow you to build principled LLM applications in a production setting.\\n\\nThis feature allows you to seamlessly integrate the LlamaIndex library with powerful observability/evaluation tools offered by our partners. Configure a variable once, and you'll be able to do things like the following:\\n\\n- View LLM/prompt inputs/outputs\\n- Ensure that the outputs of any component (LLMs, embeddings) are performing as expected\\n- View call traces for both indexing and querying\\n\\nTo learn more, check out our [observability docs](../../module_guides/observability/index.md)\", mimetype='text/plain', start_char_idx=1302, end_char_idx=1931, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.13961938),\n",
       " NodeWithScore(node=TextNode(id_='api_reference/query_pipeline/llm.md-0-146', embedding=None, metadata={'file_path': 'api_reference/query_pipeline/llm.md', 'file_name': 'llm.md', 'file_size': 147, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/', 'context': 'LLM components in LlamaIndex core.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='api_reference/query_pipeline/llm.md', node_type='4', metadata={'file_path': 'api_reference/query_pipeline/llm.md', 'file_name': 'llm.md', 'file_size': 147, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='b402b5b78809c56204a8fb36b4cbae997718313174fe1834214e8c968b48eef0')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='::: llama_index.core.llms.llm.BaseLLMComponent\\n\\n::: llama_index.core.llms.llm.LLMCompleteComponent\\n\\n::: llama_index.core.llms.llm.LLMChatComponent', mimetype='text/plain', start_char_idx=0, end_char_idx=146, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.13655108),\n",
       " NodeWithScore(node=TextNode(id_='module_guides/models/llms/usage_custom.md-0-618', embedding=None, metadata={'file_path': 'module_guides/models/llms/usage_custom.md', 'file_name': 'usage_custom.md', 'file_size': 8959, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/', 'context': \"This introductory section outlines the document's purpose: explaining how to customize Large Language Models (LLMs) within the LlamaIndex framework, focusing on changing the underlying LLM, output tokens, and other parameters.\\n\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='module_guides/models/llms/usage_custom.md', node_type='4', metadata={'file_path': 'module_guides/models/llms/usage_custom.md', 'file_name': 'usage_custom.md', 'file_size': 8959, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='a0d0eff2de47d1e485755f09d025064441ce8bb7850fec7692868fdcd4d4deb3'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='e568f456-6e28-4e97-b4de-e79376a690b1', node_type='1', metadata={'header_path': '/Customizing LLMs within LlamaIndex Abstractions/'}, hash='78e1bd818fb27b46f225b020bfc725202f5bdd4b9d29348f056c7acf7bf2f151')}, metadata_template='{key}: {value}', metadata_separator='\\n', text=\"# Customizing LLMs within LlamaIndex Abstractions\\n\\nYou can plugin these LLM abstractions within our other modules in LlamaIndex (indexes, retrievers, query engines, agents) which allow you to build advanced workflows over your data.\\n\\nBy default, we use OpenAI's `gpt-3.5-turbo` model. But you may choose to customize\\nthe underlying LLM being used.\\n\\nBelow we show a few examples of LLM customization. This includes\\n\\n- changing the underlying LLM\\n- changing the number of output tokens (for OpenAI, Cohere, or AI21)\\n- having more fine-grained control over all parameters for any LLM, from context window to chunk overlap\", mimetype='text/plain', start_char_idx=0, end_char_idx=618, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.13524376),\n",
       " NodeWithScore(node=TextNode(id_='understanding/evaluating/cost_analysis/index.md-2845-3402', embedding=None, metadata={'file_path': 'understanding/evaluating/cost_analysis/index.md', 'file_name': 'index.md', 'file_size': 3782, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Cost Analysis/Usage Pattern/', 'context': \"This section details how to use `MockLLM` to predict token usage for LLM calls in LlamaIndex, enabling cost estimation before actual LLM calls are made.  It's part of a larger discussion on cost analysis and token prediction within the LlamaIndex framework.\\n\"}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='understanding/evaluating/cost_analysis/index.md', node_type='4', metadata={'file_path': 'understanding/evaluating/cost_analysis/index.md', 'file_name': 'index.md', 'file_size': 3782, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11'}, hash='58f1cbec6a1bee779ddc3a46bb86948bf667d1649109ff5c629c824af236d7ba'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2980b492-68c3-452b-b244-b46e4ba6f171', node_type='1', metadata={'file_path': 'understanding/evaluating/cost_analysis/index.md', 'file_name': 'index.md', 'file_size': 3782, 'creation_date': '2025-02-11', 'last_modified_date': '2025-02-11', 'header_path': '/Cost Analysis/'}, hash='7ebfe902ef0f49afd8d0b0ec36cbd0965ea8cbf2e324a5a879863268636d815e'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='9909c02b-0dae-4d57-be81-62d95406a77c', node_type='1', metadata={'header_path': '/Cost Analysis/Usage Pattern/'}, hash='c19d19978b80afbcbdadf9dfcbf2ebceac3c2e05e18e7ca3317a8fcee66da493')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='### Using MockLLM\\n\\nTo predict token usage of LLM calls, import and instantiate the MockLLM as shown below. The `max_tokens` parameter is used as a \"worst case\" prediction, where each LLM response will contain exactly that number of tokens. If `max_tokens` is not specified, then it will simply predict back the prompt.\\n\\n```python\\nfrom llama_index.core.llms import MockLLM\\nfrom llama_index.core import Settings\\n\\n# use a mock llm globally\\nSettings.llm = MockLLM(max_tokens=256)\\n```\\n\\nYou can then use this predictor during both index construction and querying.', mimetype='text/plain', start_char_idx=2845, end_char_idx=3402, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.1326608)]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Customizing LLMs within LlamaIndex Abstractions\n",
      "\n",
      "You can plugin these LLM abstractions within our other modules in LlamaIndex (indexes, retrievers, query engines, agents) which allow you to build advanced workflows over your data.\n",
      "\n",
      "By default, we use OpenAI's `gpt-3.5-turbo` model. But you may choose to customize\n",
      "the underlying LLM being used.\n",
      "\n",
      "Below we show a few examples of LLM customization. This includes\n",
      "\n",
      "- changing the underlying LLM\n",
      "- changing the number of output tokens (for OpenAI, Cohere, or AI21)\n",
      "- having more fine-grained control over all parameters for any LLM, from context window to chunk overlap\n"
     ]
    }
   ],
   "source": [
    "print(reranked_nodes[-2].node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_rerank = RetrieverQueryEngine(retriever, node_postprocessors=[cohere_rerank])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Prompt Token Usage: 72\n",
      "LLM Completion Token Usage: 29\n",
      "Embedding Token Usage: 7\n",
      "Embedding Token Usage: 30\n",
      "Embedding Token Usage: 9\n",
      "Embedding Token Usage: 10\n",
      "LLM Prompt Token Usage: 7245\n",
      "LLM Completion Token Usage: 433\n"
     ]
    }
   ],
   "source": [
    "response_rerank = query_engine_rerank.query(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To count tokens from LLM calls across different components like extractors, retrievers, or query engines, you can use the `TokenCountingHandler` callback along with the `CallbackManager`. Here's a step-by-step guide:\n",
      "\n",
      "1.  Set up the `TokenCountingCallback` handler:\n",
      "\n",
      "    ```python\n",
      "    import tiktoken\n",
      "    from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
      "\n",
      "    token_counter = TokenCountingHandler(\n",
      "        tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
      "    )\n",
      "\n",
      "    callback_manager = CallbackManager([token_counter])\n",
      "    ```\n",
      "\n",
      "2.  Add the callback manager to the global `Settings`:\n",
      "\n",
      "    ```python\n",
      "    from llama_index.core import Settings\n",
      "\n",
      "    Settings.callback_manager = callback_manager\n",
      "    ```\n",
      "\n",
      "After setting up the `TokenCountingHandler` and adding it to the global `Settings`, LlamaIndex will automatically track token usage for LLM calls made by any component. You can then access the counts directly:\n",
      "\n",
      "```python\n",
      "print(\n",
      "    \"Embedding Tokens: \",\n",
      "    token_counter.total_embedding_token_count,\n",
      "    \"\\n\",\n",
      "    \"LLM Prompt Tokens: \",\n",
      "    token_counter.prompt_llm_token_count,\n",
      "    \"\\n\",\n",
      "    \"LLM Completion Tokens: \",\n",
      "    token_counter.completion_llm_token_count,\n",
      "    \"\\n\",\n",
      "    \"Total LLM Token Count: \",\n",
      "    token_counter.total_llm_token_count,\n",
      "    \"\\n\",\n",
      ")\n",
      "\n",
      "# reset counts\n",
      "token_counter.reset_counts()\n",
      "```\n",
      "\n",
      "You can also set the `tokenizer` attribute within the `Settings` object. This should match the LLM you are using.\n",
      "\n",
      "```python\n",
      "from llama_index.core import Settings\n",
      "import tiktoken\n",
      "\n",
      "Settings.tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(response_rerank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Prompt Token Usage: 69\n",
      "LLM Completion Token Usage: 41\n",
      "Embedding Token Usage: 27\n",
      "Embedding Token Usage: 14\n",
      "Embedding Token Usage: 12\n",
      "Embedding Token Usage: 12\n",
      "LLM Prompt Token Usage: 5596\n",
      "LLM Completion Token Usage: 701\n"
     ]
    }
   ],
   "source": [
    "resp = query_engine_rerank.query('how to do chat with query fusion retreiver and cohere rerank? Provide code. I want to chat, not just query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While the provided documents do not contain a specific example that combines a chat engine with query fusion and Cohere rerank, they do offer guidance on how to achieve this.\n",
      "\n",
      "To create a chat engine with query fusion and Cohere rerank, you would need to:\n",
      "\n",
      "1.  **Set up a Query Engine:** Construct a query engine that utilizes a query fusion retriever and the CohereRerank node postprocessor. The query engine serves as the foundation for answering questions over your data.\n",
      "2.  **Configure a Chat Engine:**  Then configure a chat engine, and there are multiple chat modes available. You can configure the chat engine in a high-level or low-level approach.\n",
      "3.  **Choose a Chat Mode**: You can select a chat mode like `context` or `condense_plus_context` to incorporate retrieved context into the chat.\n",
      "\n",
      "Here's a conceptual outline based on the provided information:\n",
      "\n",
      "```python\n",
      "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
      "from llama_index.core.retrievers import BaseRetriever\n",
      "from llama_index.core.query_engine import RetrieverQueryEngine\n",
      "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
      "from llama_index.llms.openai import OpenAI\n",
      "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
      "from llama_index.core import Document\n",
      "\n",
      "# Assuming you have set up your documents\n",
      "documents = [Document(text=\"Paul Graham is an essay writer.\"),\n",
      "             Document(text=\"Paul Graham founded YC.\")]\n",
      "\n",
      "# Build index\n",
      "index = VectorStoreIndex.from_documents(documents)\n",
      "\n",
      "# Define a custom retriever with query fusion\n",
      "class QueryFusionRetriever(BaseRetriever):\n",
      "    \"\"\"Custom retriever that performs query fusion.\"\"\"\n",
      "\n",
      "    def __init__(self, index):\n",
      "        self._index = index\n",
      "        super().__init__()\n",
      "\n",
      "    def _retrieve(self, query_bundle):\n",
      "        \"\"\"Retrieve nodes given query bundle.\"\"\"\n",
      "        # Perform query fusion logic here\n",
      "        # this is just a placeholder\n",
      "        nodes = self._index.as_retriever().retrieve(query_bundle)\n",
      "        return nodes\n",
      "\n",
      "# Initialize the custom retriever\n",
      "retriever = QueryFusionRetriever(index=index)\n",
      "\n",
      "# Define a reranker\n",
      "reranker = CohereRerank(api_key=\"YOUR COHERE API KEY\", top_n=2)\n",
      "\n",
      "# Build query engine\n",
      "response_synthesizer = get_response_synthesizer()\n",
      "query_engine = RetrieverQueryEngine(\n",
      "    retriever=retriever,\n",
      "    node_postprocessors=[reranker],\n",
      "    response_synthesizer=response_synthesizer,\n",
      ")\n",
      "\n",
      "# Build chat engine\n",
      "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
      "    query_engine=query_engine,\n",
      "    llm=OpenAI(model=\"gpt-3.5-turbo\")\n",
      ")\n",
      "\n",
      "# Start chatting\n",
      "while True:\n",
      "    query = input(\"Enter your query: \")\n",
      "    response = chat_engine.chat(query)\n",
      "    print(response)\n",
      "```\n",
      "\n",
      "Note: This is a conceptual example. You will need to fill in the specifics of the query fusion logic within the `QueryFusionRetriever` class.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Source (Doc id: module_guides/deploying/query_engine/index.md-16-593): ## Concept\n",
      "\n",
      "Query engine is a generic interface that allows you to ask question over your data.\n",
      "\n",
      "...\n",
      "\n",
      "> Source (Doc id: api_reference/retrievers/query_fusion.md-0-90): ::: llama_index.core.retrievers\n",
      "    options:\n",
      "      members:\n",
      "        - QueryFusionRetriever\n",
      "\n",
      "> Source (Doc id: api_reference/postprocessor/cohere_rerank.md-0-94): ::: llama_index.postprocessor.cohere_rerank\n",
      "    options:\n",
      "      members:\n",
      "        - CohereRerank\n",
      "\n",
      "> Source (Doc id: module_guides/querying/node_postprocessors/node_postprocessors.md-2547-2974): ## CohereRerank\n",
      "\n",
      "Uses the \"Cohere ReRank\" functionality to re-order nodes, and returns the top N ...\n",
      "\n",
      "> Source (Doc id: module_guides/querying/node_postprocessors/index.md-624-1542): ## Usage Pattern\n",
      "\n",
      "An example of using a node postprocessors is below:\n",
      "\n",
      "```python\n",
      "from llama_index...\n",
      "\n",
      "> Source (Doc id: module_guides/querying/retriever/retrievers.md-655-1482): ### Advanced Retrieval and Search\n",
      "\n",
      "These guides contain advanced retrieval techniques. Some are c...\n",
      "\n",
      "> Source (Doc id: module_guides/querying/retriever/retrievers.md-2349-3176): ### Composed Retrievers\n",
      "\n",
      "These are retrieval techniques that are composed on top of other retriev...\n",
      "\n",
      "> Source (Doc id: module_guides/querying/pipeline/usage_pattern.md-893-2160): ### Defining a DAG\n",
      "\n",
      "Many pipelines will require you to setup a DAG (for instance, if you want to ...\n",
      "\n",
      "> Source (Doc id: optimizing/building_rag_from_scratch.md-3495-3734): ### Building RAG Fusion Retriever from Scratch\n",
      "\n",
      "Here we show you how to build an advanced retriev...\n",
      "\n",
      "> Source (Doc id: understanding/putting_it_all_together/agents.md-2153-3636): ## Agentic Components within LlamaIndex\n",
      "\n",
      "LlamaIndex provides core modules capable of automated re...\n",
      "\n",
      "> Source (Doc id: optimizing/fine-tuning/fine-tuning.md-6455-7105): ## Cohere Custom Reranker\n",
      "\n",
      "By training a custom reranker with CohereAI, we can attempt to improve...\n",
      "\n",
      "> Source (Doc id: use_cases/fine_tuning.md-6568-7215): ## Cohere Custom Reranker\n",
      "\n",
      "By training a custom reranker with CohereAI, we can attempt to improve...\n",
      "\n",
      "> Source (Doc id: api_reference/packs/cohere_citation_chat.md-0-109): ::: llama_index.packs.cohere_citation_chat\n",
      "    options:\n",
      "      members:\n",
      "        - CohereCitationCh...\n",
      "\n",
      "> Source (Doc id: optimizing/basic_strategies/basic_strategies.md-3298-4380): ## Hybrid Search\n",
      "\n",
      "Hybrid search is a common term for retrieval that involves combining results fr...\n",
      "\n",
      "> Source (Doc id: optimizing/advanced_retrieval/advanced_retrieval.md-33-887): ## Main Advanced Retrieval Strategies\n",
      "\n",
      "There are a variety of more advanced retrieval strategies ...\n",
      "\n",
      "> Source (Doc id: module_guides/deploying/chat_engines/usage_pattern.md-1125-2324): #### Available Chat Modes\n",
      "\n",
      "- `best` - Turn the query engine into a tool, for use with a `ReAct` d...\n",
      "\n",
      "> Source (Doc id: module_guides/querying/node_postprocessors/node_postprocessors.md-10253-11452): ## All Notebooks\n",
      "\n",
      "- [Sentence Optimizer](../../../examples/node_postprocessor/OptimizerDemo.ipynb...\n",
      "\n",
      "> Source (Doc id: module_guides/indexing/llama_cloud_index.md-5217-6111): ### Composite Retrieval related parameters\n",
      "There are a few parameters that are specific to tuning...\n",
      "\n",
      "> Source (Doc id: understanding/querying/querying.md-802-1764): ## Stages of querying\n",
      "\n",
      "However, there is more to querying than initially meets the eye. Querying ...\n",
      "\n",
      "> Source (Doc id: module_guides/deploying/chat_engines/usage_pattern.md-464-566): ## Configuring a Chat Engine\n",
      "\n",
      "Configuring a chat engine is very similar to configuring a query en...\n"
     ]
    }
   ],
   "source": [
    "print(resp.get_formatted_sources())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.chat_engine import ContextChatEngine, CondensePlusContextChatEngine\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = CondensePlusContextChatEngine(retriever=retriever, llm=llm, memory=memory, node_postprocessors=[cohere_rerank], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condensed question: how to use a react chat engine, given that I have a retriever? provide code\n",
      "LLM Prompt Token Usage: 59\n",
      "LLM Completion Token Usage: 22\n",
      "Embedding Token Usage: 7\n",
      "Embedding Token Usage: 17\n",
      "Embedding Token Usage: 6\n",
      "Embedding Token Usage: 6\n",
      "LLM Prompt Token Usage: 4022\n",
      "LLM Completion Token Usage: 1447\n"
     ]
    }
   ],
   "source": [
    "res = chat_engine.chat('how to use a react chat engine, given that I have a retriever? provide code', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I can help with that! Here's how you can use a React Chat Engine with a retriever in LlamaIndex.\n",
      "\n",
      "Firstly, it's important to understand the relationship between Chat Engines, Query Engines, and Retrievers. A Retriever is responsible for fetching the most relevant context given a user query. It can be built on top of indexes, but can also be defined independently. It's a key building block in both Query Engines and Chat Engines. Chat Engines are stateful interfaces for having conversations with your data, keeping track of conversation history to answer questions with past context in mind.\n",
      "\n",
      "While the documents don't provide a direct, copy-and-paste code example for integrating a React frontend *directly* with a retriever and chat engine, they do provide the key components and concepts. Here's a breakdown of how you can approach this, combining the information from the documents:\n",
      "\n",
      "1.  **Backend (LlamaIndex)**:\n",
      "    *   You'll need to set up a LlamaIndex chat engine in your backend. The chat engine will use the retriever to fetch relevant context.\n",
      "    *   The documents show how to initialize a chat engine from an index.  Since you have a retriever, you'll likely need to adapt this.  You might need to create a custom query engine that uses your retriever and then integrate that into the chat engine.\n",
      "    *   The high-level API allows you to configure a chat engine from an index in one line of code, like this:\n",
      "\n",
      "        ```python\n",
      "        chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
      "        ```\n",
      "\n",
      "        Different `chat_mode` options are available, including `\"best\"`, `\"condense_question\"`, `\"context\"`, `\"condense_plus_context\"`, `\"simple\"`, `\"react\"`, and `\"openai\"`.  The `best` mode turns the query engine into a tool for use with a `ReAct` or `OpenAI` data agent.\n",
      "\n",
      "    *   To enable streaming responses, use the `stream_chat` endpoint:\n",
      "\n",
      "        ```python\n",
      "        chat_engine = index.as_chat_engine()\n",
      "        streaming_response = chat_engine.stream_chat(\"Tell me a joke.\")\n",
      "        for token in streaming_response.response_gen:\n",
      "            print(token, end=\"\")\n",
      "        ```\n",
      "\n",
      "2.  **Frontend (React)**:\n",
      "    *   The React frontend will need to handle user input and display messages.\n",
      "    *   The `ChatView` component (`frontend/src/chat/ChatView.tsx`) is responsible for handling and displaying a chat interface. It establishes a WebSocket connection to communicate in real-time with the server, sending and receiving messages.\n",
      "    *   Key features of the `ChatView` component include: establishing and managing the WebSocket connection with the server, displaying messages from the user and the server in a chat-like format, handling user input to send messages to the server, and updating the messages state and UI based on received messages from the server.\n",
      "\n",
      "**Conceptual Code Snippets (Combining Backend & Frontend)**\n",
      "\n",
      "*   **Backend (Python - LlamaIndex with Retriever):**\n",
      "\n",
      "    ```python\n",
      "    from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
      "    from llama_index.core.retrievers import BaseRetriever\n",
      "    from llama_index.core.query_engine import RetrieverQueryEngine\n",
      "\n",
      "    # Assume you have a retriever instance\n",
      "    # my_retriever = ...\n",
      "\n",
      "    # Configure response synthesizer\n",
      "    response_synthesizer = get_response_synthesizer()\n",
      "\n",
      "    # Assemble query engine\n",
      "    query_engine = RetrieverQueryEngine(\n",
      "        retriever=my_retriever,\n",
      "        response_synthesizer=response_synthesizer,\n",
      "    )\n",
      "\n",
      "    # Build chat engine from the query engine\n",
      "    chat_engine = query_engine.as_chat_engine(chat_mode=\"context\")\n",
      "\n",
      "    # Example usage\n",
      "    response = chat_engine.chat(\"Tell me a joke\")\n",
      "    print(response)\n",
      "    ```\n",
      "\n",
      "*   **Frontend (React - ChatView Component):**\n",
      "\n",
      "    ```typescript\n",
      "    // frontend/src/chat/ChatView.tsx\n",
      "    import React, { useState, useEffect } from 'react';\n",
      "\n",
      "    function ChatView() {\n",
      "        const [messages, setMessages] = useState([]);\n",
      "        const [inputValue, setInputValue] = useState('');\n",
      "        const [ws, setWs] = useState(null);\n",
      "\n",
      "        useEffect(() => {\n",
      "            const newWs = new WebSocket('ws://your-backend-url'); // Replace with your backend WebSocket URL\n",
      "\n",
      "            newWs.onopen = () => {\n",
      "                console.log('Connected to WebSocket');\n",
      "            };\n",
      "\n",
      "            newWs.onmessage = (event) => {\n",
      "                const message = JSON.parse(event.data);\n",
      "                setMessages(prevMessages => [...prevMessages, message]);\n",
      "            };\n",
      "\n",
      "            newWs.onclose = () => {\n",
      "                console.log('Disconnected from WebSocket');\n",
      "            };\n",
      "\n",
      "            setWs(newWs);\n",
      "\n",
      "            return () => {\n",
      "                newWs.close();\n",
      "            };\n",
      "        }, []);\n",
      "\n",
      "        const sendMessage = () => {\n",
      "            if (ws && inputValue) {\n",
      "                ws.send(JSON.stringify({ text: inputValue }));\n",
      "                setInputValue('');\n",
      "            }\n",
      "        };\n",
      "\n",
      "        return (\n",
      "            <div>\n",
      "                {/* Display messages */}\n",
      "                {messages.map((message, index) => (\n",
      "                    <div key={index}>{message.text}</div>\n",
      "                ))}\n",
      "\n",
      "                {/* Input and send button */}\n",
      "                <input value={inputValue} onChange={e => setInputValue(e.target.value)} />\n",
      "                <button onClick={sendMessage}>Send</button>\n",
      "            </div>\n",
      "        );\n",
      "    }\n",
      "\n",
      "    export default ChatView;\n",
      "    ```\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **WebSockets:**  The React component uses WebSockets for real-time communication.  You'll need a WebSocket server in your backend (e.g., using FastAPI, Django Channels, or Node.js with Socket.IO) to handle the WebSocket connections and pass messages between the React frontend and the LlamaIndex chat engine.\n",
      "*   **Adaptation:** The code snippets are conceptual. You'll need to adapt them to your specific retriever implementation, backend framework, and frontend structure.\n",
      "*   **Error Handling:** Implement proper error handling in both the frontend and backend.\n",
      "*   **Streaming:** If you want to stream responses, you'll need to handle the streaming data in both the backend (sending the tokens) and the frontend (receiving and displaying the tokens).\n",
      "\n",
      "I hope this helps you get started! Let me know if you have any other questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Prompt Token Usage: 1560\n",
      "LLM Completion Token Usage: 30\n",
      "Condensed question: How can I create a React Agent-based chat engine in LlamaIndex from a retriever, providing code examples for the agent's React functionality?\n",
      "\n",
      "LLM Prompt Token Usage: 71\n",
      "LLM Completion Token Usage: 26\n",
      "Embedding Token Usage: 30\n",
      "Embedding Token Usage: 9\n",
      "Embedding Token Usage: 6\n",
      "Embedding Token Usage: 8\n",
      "LLM Prompt Token Usage: 8195\n",
      "LLM Completion Token Usage: 1146\n"
     ]
    }
   ],
   "source": [
    "res2 = chat_engine.chat('I meant how to use REACT agent based chat engine in llama index. Please provide code to react a react Agent based chat engine. I want to create it from a retriever and not from an index directly.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, I understand now. You want to use a `ReActAgent` as your chat engine, building it from a retriever instead of directly from an index. Here's how you can do that, combining the information from the documents:\n",
      "\n",
      "```python\n",
      "from llama_index.core.agent import ReActAgent\n",
      "from llama_index.llms.openai import OpenAI\n",
      "from llama_index.core.tools import QueryEngineTool\n",
      "from llama_index.core.query_engine import RetrieverQueryEngine\n",
      "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
      "from llama_index.core.prompts import PromptTemplate\n",
      "\n",
      "# 1. Define your Retriever\n",
      "# Assume you have a retriever instance (e.g., from a VectorStoreIndex)\n",
      "# my_retriever = ...\n",
      "\n",
      "# 2. Create a Query Engine from the Retriever\n",
      "#    This wraps the retriever in a query engine interface.\n",
      "query_engine = RetrieverQueryEngine.from_args(retriever=my_retriever)\n",
      "\n",
      "# 3. (Optional) Add a name and description to the query engine tool\n",
      "#    This helps the agent decide when to use it.\n",
      "query_engine_tool = QueryEngineTool(\n",
      "    query_engine=query_engine,\n",
      "    metadata={\n",
      "        \"name\": \"my_retriever_tool\",\n",
      "        \"description\": \"Useful for answering questions about the documents I have access to.\",\n",
      "    },\n",
      ")\n",
      "\n",
      "# 4. Initialize the LLM\n",
      "#    Choose an LLM that supports function calling (e.g., gpt-3.5-turbo-0613 or gpt-4)\n",
      "llm = OpenAI(model=\"gpt-3.5-turbo-0613\")\n",
      "\n",
      "# 5. (Optional) Define a custom prompt\n",
      "#    You can customize the agent's behavior with a custom prompt.\n",
      "#    See https://docs.llamaindex.ai/en/stable/core_modules/agent_modules/agents/root.html#customization\n",
      "react_template = \"\"\"\\\n",
      "Given a user question, an incomplete plan, and a set of tools, take the following steps to answer the question as best as possible. Between each step, carefully consider your plan and adjust it as needed.\n",
      "\n",
      "Always use the following format:\n",
      "\n",
      "```\n",
      "Thought: Describe your plan at this step.\n",
      "Action:\n",
      "```\n",
      "The action should be a tool name and an input.\n",
      "```\n",
      "Action Input: The input to the tool.\n",
      "```\n",
      "After the action, you will receive an observation.\n",
      "Do not repeat the same action.\n",
      "\n",
      "Here are the tools you have access to:\n",
      "{tool_desc}\n",
      "\n",
      "Here is the incomplete plan:\n",
      "{plan}\n",
      "\n",
      "Begin!\n",
      "\n",
      "Question: {question}\n",
      "{agent_scratchpad}\"\"\"\n",
      "\n",
      "react_prompt = PromptTemplate(react_template)\n",
      "\n",
      "# 6. Initialize the ReActAgent\n",
      "#    Pass the query engine tool and the LLM to the agent.\n",
      "#    Set verbose=True to see the agent's thought process.\n",
      "#    You can also pass a custom prompt here.\n",
      "agent = ReActAgent.from_tools(\n",
      "    tools=[query_engine_tool],\n",
      "    llm=llm,\n",
      "    verbose=True,\n",
      "    react_prompt=react_prompt,\n",
      ")\n",
      "\n",
      "# 7. Use the agent to chat\n",
      "response = agent.chat(\"What is the capital of France?\")\n",
      "print(response)\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "*   **Clearer Steps:** The code is broken down into numbered steps for better understanding.\n",
      "*   **Retriever to Query Engine:**  It explicitly shows how to create a `RetrieverQueryEngine` from your retriever.  This is crucial because the `ReActAgent` works with tools, and a `QueryEngine` is a convenient way to wrap your retriever as a tool.\n",
      "*   **`QueryEngineTool`:**  It uses `QueryEngineTool` to properly format the query engine as a tool for the agent.  The `metadata` argument is important for giving the agent information about the tool's purpose.\n",
      "*   **LLM Selection:**  It emphasizes the need for an LLM that supports function calling (like `gpt-3.5-turbo-0613` or `gpt-4`).  These models are designed to work well with agents and tools.\n",
      "*   **Custom Prompt (Optional):** It includes an example of how to define a custom prompt for the agent.  This allows you to fine-tune the agent's behavior.  The link to the documentation provides more details on prompt customization.\n",
      "*   **Verbose Mode:**  `verbose=True` is set so you can see the agent's thought process, which is very helpful for debugging.\n",
      "*   **`from_args` for `RetrieverQueryEngine`:** Uses `RetrieverQueryEngine.from_args` for a cleaner and more flexible initialization.\n",
      "*   **Tool Description:**  Adds a description to the `QueryEngineTool` to help the agent understand when to use it.\n",
      "*   **Import Statements:** Includes all necessary import statements.\n",
      "\n",
      "This revised response provides a complete and correct solution for creating a `ReActAgent` from a retriever in LlamaIndex.  It addresses the user's specific request and provides clear, executable code with explanations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Prompt Token Usage: 2717\n",
      "LLM Completion Token Usage: 8\n",
      "Condensed question: What is the capital of Assam?\n",
      "\n",
      "LLM Prompt Token Usage: 49\n",
      "LLM Completion Token Usage: 25\n",
      "Embedding Token Usage: 8\n",
      "Embedding Token Usage: 8\n",
      "Embedding Token Usage: 4\n",
      "Embedding Token Usage: 10\n",
      "LLM Prompt Token Usage: 16255\n",
      "LLM Completion Token Usage: 28\n"
     ]
    }
   ],
   "source": [
    "res3 = chat_engine.chat('what is the capital of assam?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I cannot answer that question. The provided documents do not contain information about the capital of Assam.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condensed question: what did I ask you before?\n",
      "LLM Prompt Token Usage: 49\n",
      "LLM Completion Token Usage: 29\n",
      "Embedding Token Usage: 7\n",
      "Embedding Token Usage: 8\n",
      "Embedding Token Usage: 8\n",
      "Embedding Token Usage: 10\n",
      "LLM Prompt Token Usage: 9284\n",
      "LLM Completion Token Usage: 197\n"
     ]
    }
   ],
   "source": [
    "res4 = chat_engine.chat('what did I ask you before?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not have access to past conversations, so I don't know what you asked me before. However, the documentation does mention tools for storing chat history.\n",
      "\n",
      "Specifically, the `PostgresChatStore` allows you to store chat history remotely using PostgreSQL. Here's how you can initialize and use it with `ChatMemoryBuffer`:\n",
      "\n",
      "```python\n",
      "from llama_index.storage.chat_store.postgres import PostgresChatStore\n",
      "from llama_index.core.memory import ChatMemoryBuffer\n",
      "\n",
      "chat_store = PostgresChatStore.from_uri(\n",
      "    uri=\"postgresql+asyncpg://postgres:password@127.0.0.1:5432/database\",\n",
      ")\n",
      "\n",
      "chat_memory = ChatMemoryBuffer.from_defaults(\n",
      "    token_limit=3000,\n",
      "    chat_store=chat_store,\n",
      "    chat_store_key=\"user1\",\n",
      ")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(res4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReACT Agent Chat Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.tools.types import ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine.from_args(retriever=retriever, node_postprocessors=[cohere_rerank])\n",
    "\n",
    "# 3. (Optional) Add a name and description to the query engine tool\n",
    "#    This helps the agent decide when to use it.\n",
    "query_engine_tool = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata= ToolMetadata(\n",
    "            name=\"my_retriever_tool\",\n",
    "            description=\"Useful for answering questions about the documents I have access to.\",\n",
    "            return_direct=False\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools(\n",
    "    tools=[query_engine_tool],\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    # react_prompt=react_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 85c3d896-43dd-4b61-a42e-2c12a8236bb4. Step input: How to Implement Callback manager to count tokens of all llm calls made, through any component, maybe extractor or retreiver, or query engine.\n",
      "LLM Prompt Token Usage: 523\n",
      "LLM Completion Token Usage: 70\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to find information about implementing a callback manager to count tokens of all LLM calls.\n",
      "Action: my_retriever_tool\n",
      "Action Input: {'input': 'implement callback manager to count tokens of all llm calls'}\n",
      "\u001b[0mLLM Prompt Token Usage: 53\n",
      "LLM Completion Token Usage: 28\n",
      "Embedding Token Usage: 9\n",
      "Embedding Token Usage: 11\n",
      "Embedding Token Usage: 8\n",
      "Embedding Token Usage: 8\n",
      "LLM Prompt Token Usage: 6450\n",
      "LLM Completion Token Usage: 210\n",
      "\u001b[1;3;34mObservation: To count tokens from LLM calls, you can implement a callback manager using the `TokenCountingHandler`. Here's how:\n",
      "\n",
      "1.  Instantiate `TokenCountingHandler`, optionally setting a tokenizer.\n",
      "2.  Create a `CallbackManager` and add the `TokenCountingHandler` to it.\n",
      "3.  Set the `callback_manager` in `Settings`.\n",
      "\n",
      "Here's a code snippet demonstrating this:\n",
      "\n",
      "```python\n",
      "import tiktoken\n",
      "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
      "from llama_index.core import Settings\n",
      "\n",
      "token_counter = TokenCountingHandler(\n",
      "    tokenizer=tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode\n",
      ")\n",
      "\n",
      "Settings.callback_manager = CallbackManager([token_counter])\n",
      "```\n",
      "\n",
      "After setting up the callback manager, you can access token counts, including embedding tokens, LLM prompt tokens, and LLM completion tokens. You can also reset the counts at any time.\n",
      "\n",
      "\u001b[0m> Running step 734870ad-177a-46ff-9c45-b1f8b9cf3b17. Step input: None\n",
      "LLM Prompt Token Usage: 805\n",
      "LLM Completion Token Usage: 146\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: To implement a callback manager to count tokens of all LLM calls, you can use the `TokenCountingHandler`. First, instantiate `TokenCountingHandler`, optionally setting a tokenizer. Then, create a `CallbackManager` and add the `TokenCountingHandler` to it. Finally, set the `callback_manager` in `Settings`. A code snippet demonstrating this is provided in the previous response. After setting up the callback manager, you can access token counts, including embedding tokens, LLM prompt tokens, and LLM completion tokens. You can also reset the counts at any time.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "resp1 = agent.chat(QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt tokens 99135 op tokens 5241\n"
     ]
    }
   ],
   "source": [
    "# totla llm counts till now\n",
    "print('prompt tokens', token_counter.prompt_llm_token_count, 'op tokens', token_counter.completion_llm_token_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_components.custom_google_genai import GoogleGenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = GoogleGenAI(\n",
    "            model=\"models/gemini-2.0-flash\",\n",
    "            api_key=os.environ['GEMINI_API_KEY'], \n",
    "            max_retries=2,  # Number of retry attempts\n",
    "            retry_on_rate_limit=True,\n",
    "            additional_kwargs={\"stream_options\": {\"include_usage\": True}}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoogleGenAI(callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x0000017089826150>, system_prompt=None, messages_to_prompt=<function messages_to_prompt at 0x00000170FE71A0C0>, completion_to_prompt=<function default_completion_to_prompt at 0x00000170FFE1C7C0>, output_parser=None, pydantic_program_mode=<PydanticProgramMode.DEFAULT: 'default'>, query_wrapper_prompt=None, model='models/gemini-2.0-flash', temperature=0.1, context_window=None, is_function_calling_model=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.base.llms.types import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = llm.chat([ChatMessage(role=\"user\", content=\"Hello\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.llama_index import LlamaIndexInstrumentor\n",
    " \n",
    "instrumentor = LlamaIndexInstrumentor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trace ID is not set. Creating generation client with new trace id.\n"
     ]
    }
   ],
   "source": [
    "instrumentor.start()\n",
    " \n",
    "# ... your LlamaIndex index creation ...\n",
    " \n",
    "# response = index.as_query_engine().query(\"What is the use of callbacks?\")\n",
    "response = llm.chat([ChatMessage(role=\"user\", content=\"Hello\")])\n",
    " \n",
    "# Flush events to langfuse\n",
    " \n",
    "instrumentor.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.chat([ChatMessage(role=\"user\", content=\"Hello\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': {'parts': [{'video_metadata': None,\n",
       "    'thought': None,\n",
       "    'code_execution_result': None,\n",
       "    'executable_code': None,\n",
       "    'file_data': None,\n",
       "    'function_call': None,\n",
       "    'function_response': None,\n",
       "    'inline_data': None,\n",
       "    'text': 'Hello there! How can I help you today?\\n'}],\n",
       "  'role': 'model'},\n",
       " 'citation_metadata': None,\n",
       " 'finish_message': None,\n",
       " 'token_count': None,\n",
       " 'finish_reason': <FinishReason.STOP: 'STOP'>,\n",
       " 'avg_logprobs': -0.06534120711413297,\n",
       " 'grounding_metadata': None,\n",
       " 'index': None,\n",
       " 'logprobs_result': None,\n",
       " 'safety_ratings': None,\n",
       " 'usage_metadata': {'cache_tokens_details': None,\n",
       "  'cached_content_token_count': None,\n",
       "  'candidates_token_count': 11,\n",
       "  'candidates_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,\n",
       "    'token_count': 11}],\n",
       "  'prompt_token_count': 1,\n",
       "  'prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,\n",
       "    'token_count': 1}],\n",
       "  'thoughts_token_count': None,\n",
       "  'tool_use_prompt_token_count': None,\n",
       "  'tool_use_prompt_tokens_details': None,\n",
       "  'total_token_count': 12,\n",
       "  'traffic_type': None},\n",
       " 'usage': {'prompt_tokens': 1, 'completion_tokens': 11, 'total_tokens': 12}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
