{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"..\")  # to run notebook as if in the base folder, so filepaths dont need to be changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSED_DIR = \"./processed_data/langchain_docs_docs\"\n",
    "MODEL = \"models/gemini-2.0-flash\"# \"llama-3.3-70b-versatile\"\n",
    "RATE_LIMIT = 10  # LLM req/min, -1 if no limit\n",
    "# COLLECTION = 'simple_md_nodes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import math\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter, MarkdownNodeParser\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.extractors import TitleExtractor, SummaryExtractor\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.schema import TextNode, MetadataMode\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "from llama_index.llms.google_genai import GoogleGenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview import tokenization\n",
    "import chromadb\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from utilities import get_logger, setup_llm_logs\n",
    "from utilities import create_custom_logger, get_large_files, setup_llm_logs, GeminiTokenizer, HuggingfaceTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory to store processed data like vecor store, doctore etc\n",
    "# PROCESSED_DIR = (Path('processed_data') / Path(INPUT_DIR).parts[-1]).as_posix()\n",
    "# Path(PROCESSED_DIR).mkdir(exist_ok=True)\n",
    "# PROCESSED_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = Path(PROCESSED_DIR) / 'config.json'\n",
    "with open(config_dir, 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_model_provider': 'gemini',\n",
       " 'llm_model': 'models/gemini-1.5-flash',\n",
       " 'rate_limit': 15,\n",
       " 'input_dir': 'data/langchain/docs/docs/',\n",
       " 'output_dir': 'processed_data/langchain_docs_docs',\n",
       " 'file_types': ['.md', '.mdx'],\n",
       " 'vector_store': 'chroma',\n",
       " 'chromadb_path': 'processed_data/langchain_docs_docs/chromadb',\n",
       " 'chroma_collection': 'contextual',\n",
       " 'doctsore_path': 'processed_data/langchain_docs_docs/docstore.json',\n",
       " 'embedding_provider': 'GeminiEmbedding',\n",
       " 'embedding_model': 'models/text-embedding-004',\n",
       " 'tokenizer_provider': 'gemini',\n",
       " 'tokenizer_model_name': 'gemini-1.5-flash-002',\n",
       " 'max_node_tokens': 2000,\n",
       " 'metadata_extractors': ['CustomDocumentContextExtractor'],\n",
       " 'datetime': '2025-02-21T14:37:17.151125+00:00',\n",
       " 'run_1_time': '2025-02-21T17:23:40.691277+00:00',\n",
       " 'run_1_nodes': 1567,\n",
       " 'run_2_time': '2025-02-22T07:25:50.461930+00:00',\n",
       " 'run_2_nodes': 0,\n",
       " 'run_3_time': '2025-02-22T17:18:28.706884+00:00',\n",
       " 'run_3_nodes': 1063}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = Groq(model=MODEL, api_key=os.environ['GROQ_API_KEY'], max_retries=3,  # Number of retry attempts\n",
    "#     retry_on_rate_limit=True)\n",
    "llm = GoogleGenAI(\n",
    "    model=MODEL,\n",
    "    api_key=os.environ['GEMINI_API_KEY'], \n",
    "    max_retries=3,  # Number of retry attempts\n",
    "    retry_on_rate_limit=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_model(config):\n",
    "    if config['embedding_provider'] == 'HuggingFaceEmbedding':\n",
    "        from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "        return HuggingFaceEmbedding(model_name=config['embedding_model'])\n",
    "    elif config['embedding_provider'] == 'GoogleGenAIEmbedding':\n",
    "        from llama_index.embeddings.google_genai import GoogleGenAIEmbedding\n",
    "        return GoogleGenAIEmbedding(model_name=config['embedding_model'], api_key=os.environ['GEMINI_API_KEY'])\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Embedding provider {config['embedding_provider']} invalid or not implemented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = get_embed_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(config):\n",
    "    if config['tokenizer_provider'] == 'gemini':\n",
    "        return GeminiTokenizer(model=config['tokenizer_model_name']).encode\n",
    "    elif config['tokenizer_provider'] == 'huggingface':\n",
    "        return HuggingfaceTokenizer(model=config['tokenizer_model_name']).encode\n",
    "    elif config['tokenizer_provider'] == 'tiktoken':\n",
    "        return tiktoken.get_encoding(encoding_name=config['tokenizer_model_name']).encode\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{config['tokenizer_provider']} invalid or not implemented\")\n",
    "    \n",
    "tokenizer = GeminiTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.context_window = 32000\n",
    "Settings.embed_model = embed_model\n",
    "Settings.tokenizer = tokenizer.encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = get_logger(logger_name='LLMEvents', logfile_path='logs/rag_query.log')\n",
    "# setup_llm_logs(logger=logger, tokenizer=tokenizer, short_inputs=False, short_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler, simple_llm_handler\n",
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tokenizer.encode,\n",
    "    verbose=True,  # set to true to see usage printed to the console\n",
    ")\n",
    "llm_debugger = simple_llm_handler.SimpleLLMHandler()\n",
    "handlers = [token_counter, llm_debugger]\n",
    "Settings.callback_manager = CallbackManager(handlers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index from vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'llm_model_provider': 'gemini',\n",
       " 'llm_model': 'models/gemini-1.5-flash',\n",
       " 'rate_limit': 15,\n",
       " 'input_dir': 'data/langchain/docs/docs/',\n",
       " 'output_dir': 'processed_data/langchain_docs_docs',\n",
       " 'file_types': ['.md', '.mdx'],\n",
       " 'vector_store': 'chroma',\n",
       " 'chromadb_path': 'processed_data/langchain_docs_docs/chromadb',\n",
       " 'chroma_collection': 'contextual',\n",
       " 'doctsore_path': 'processed_data/langchain_docs_docs/docstore.json',\n",
       " 'embedding_provider': 'GeminiEmbedding',\n",
       " 'embedding_model': 'models/text-embedding-004',\n",
       " 'tokenizer_provider': 'gemini',\n",
       " 'tokenizer_model_name': 'gemini-1.5-flash-002',\n",
       " 'max_node_tokens': 2000,\n",
       " 'metadata_extractors': ['CustomDocumentContextExtractor'],\n",
       " 'datetime': '2025-02-21T14:37:17.151125+00:00',\n",
       " 'run_1_time': '2025-02-21T17:23:40.691277+00:00',\n",
       " 'run_1_nodes': 1567,\n",
       " 'run_2_time': '2025-02-22T07:25:50.461930+00:00',\n",
       " 'run_2_nodes': 0,\n",
       " 'run_3_time': '2025-02-22T17:18:28.706884+00:00',\n",
       " 'run_3_nodes': 1063}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "chromadb_path = config['chromadb_path']\n",
    "chromadb_found = False\n",
    "if os.path.exists(chromadb_path):\n",
    "    chromadb_found = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Index...\n",
      "collections ['contextual']\n",
      "index loaded\n"
     ]
    }
   ],
   "source": [
    "if chromadb_found:\n",
    "    print('loading Index...')\n",
    "    COLLECTION = config['chroma_collection']\n",
    "    \n",
    "    process = subprocess.Popen([\"chroma\", \"run\", \"--path\", chromadb_path])\n",
    "    chroma_client = chromadb.HttpClient()\n",
    "    print('collections', chroma_client.list_collections())\n",
    "\n",
    "    # remote_db = chromadb.PersistentClient(path=chromadb_path)\n",
    "\n",
    "    chroma_collection = chroma_client.get_collection(COLLECTION)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    vector_index = VectorStoreIndex.from_vector_store(\n",
    "        vector_store,\n",
    "        embed_model=embed_model,\n",
    "    )\n",
    "    print('index loaded')\n",
    "else:\n",
    "    print(f\"ChromaDB path {chromadb_path} does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index from docstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docstore does not exist\n"
     ]
    }
   ],
   "source": [
    "docstore_path = f\"{PROCESSED_DIR}/docstore_only_emb.json\"\n",
    "doctsore_found = False\n",
    "if Path(docstore_path).exists():\n",
    "    doctsore_found = True\n",
    "    print(f\"Loading docstore from {docstore_path}\")\n",
    "else:\n",
    "    print('docstore does not exist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if doctsore_found:\n",
    "    # Load the document store\n",
    "    docstore = SimpleDocumentStore.from_persist_path(docstore_path)\n",
    "    nodes = list(docstore.docs.values())\n",
    "    print(len(nodes), 'nodes')\n",
    "    for node in nodes[:10]:\n",
    "        display(Markdown(node.text))\n",
    "        print('------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(chromadb_path) and docstore:\n",
    "    storage_context_doctore = StorageContext.from_defaults(docstore=docstore)\n",
    "    # summary_index = SummaryIndex(nodes, storage_context=storage_context_doctore)\n",
    "    vector_index = VectorStoreIndex(nodes=nodes, storage_context=storage_context_doctore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(chromadb_path) and not docstore:\n",
    "    raise Exception('Neither Chromadb nor docstore exist. Please check the paths and try again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Query and Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_index.storage_context.persist('mystore2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nodes info\n",
    "nodes_info = chroma_collection.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ids', 'embeddings', 'metadatas', 'documents', 'data', 'uris', 'included'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how to use gemini as the llm in langchain?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "retr = vector_index.as_retriever(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Token Usage: 13\n"
     ]
    }
   ],
   "source": [
    "retr_nodes = retr.retrieve(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "integrations/providers/google.mdx-4864-5207\n",
      "LLMs section:  Describes how to access Google AI Gemini models using the GoogleGenerativeAI class within the Langchain framework.\n",
      "\n",
      "### Google Generative AI\n",
      "\n",
      "Access GoogleAI `Gemini` models such as `gemini-pro` and `gemini-pro-vision` through the `GoogleGenerativeAI` class.\n",
      "\n",
      "Install python package.\n",
      "\n",
      "```bash\n",
      "pip install langchain-google-genai\n",
      "```\n",
      "\n",
      "See a [usage example](/docs/integrations/llms/google_ai).\n",
      "\n",
      "```python\n",
      "from langchain_google_genai import GoogleGenerativeAI\n",
      "```\n",
      "--------------------\n",
      "integrations/providers/google.mdx-4855-4862\n",
      "Section outlining different Large Language Models (LLMs) available for integration with Langchain, including Google's Gemini and models accessible through Vertex AI.\n",
      "\n",
      "## LLMs\n",
      "--------------------\n",
      "integrations/providers/google.mdx-2099-2416\n",
      "This section describes how to access Google's Gemini chat models via the Vertex AI platform using the `langchain-google-vertexai` package.\n",
      "\n",
      "### Vertex AI\n",
      "\n",
      "Access chat models like `Gemini` via Google Cloud.\n",
      "\n",
      "We need to install `langchain-google-vertexai` python package.\n",
      "\n",
      "```bash\n",
      "pip install langchain-google-vertexai\n",
      "```\n",
      "\n",
      "See a [usage example](/docs/integrations/chat/google_vertex_ai_palm).\n",
      "\n",
      "```python\n",
      "from langchain_google_vertexai import ChatVertexAI\n",
      "```\n",
      "--------------------\n",
      "integrations/providers/google.mdx-891-2097\n",
      "This section details how to use the Google Generative AI (Gemini) models, including `gemini-pro` and `gemini-pro-vision`, via the `ChatGoogleGenerativeAI` class, showing code examples for text and image inputs.\n",
      "\n",
      "### Google Generative AI\n",
      "\n",
      "Access GoogleAI `Gemini` models such as `gemini-pro` and `gemini-pro-vision` through the `ChatGoogleGenerativeAI` class.\n",
      "\n",
      "```bash\n",
      "pip install -U langchain-google-genai\n",
      "```\n",
      "\n",
      "Configure your API key.\n",
      "\n",
      "```bash\n",
      "export GOOGLE_API_KEY=your-api-key\n",
      "```\n",
      "\n",
      "```python\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "\n",
      "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
      "llm.invoke(\"Sing a ballad of LangChain.\")\n",
      "```\n",
      "\n",
      "Gemini vision model supports image inputs when providing a single chat message.\n",
      "\n",
      "```python\n",
      "from langchain_core.messages import HumanMessage\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "\n",
      "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
      "\n",
      "message = HumanMessage(\n",
      "    content=[\n",
      "        {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"What's in this image?\",\n",
      "        },  # You can optionally provide text parts\n",
      "        {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"},\n",
      "    ]\n",
      ")\n",
      "llm.invoke([message])\n",
      "```\n",
      "\n",
      "The value of image_url can be any of the following:\n",
      "\n",
      "- A public image URL\n",
      "- A gcs file (e.g., \"gcs://path/to/file.png\")\n",
      "- A local file path\n",
      "- A base64 encoded image (e.g., data:image/png;base64,abcd124)\n",
      "- A PIL image\n",
      "--------------------\n",
      "integrations/providers/pygmalionai.mdx-291-419\n",
      "This section describes how to use the Aphrodite Large Language Model (LLM) with the Langchain framework.\n",
      "\n",
      "## LLMs\n",
      "\n",
      "See a [usage example](/docs/integrations/llms/aphrodite).\n",
      "\n",
      "```python\n",
      "from langchain_community.llms import Aphrodite\n",
      "```\n",
      "--------------------\n",
      "integrations/providers/cerebriumai.mdx-543-676\n",
      "This section details how to use the CerebriumAI LLM with Langchain, providing a code example.\n",
      "\n",
      "## LLMs\n",
      "\n",
      "See a [usage example](/docs/integrations/llms/cerebriumai).\n",
      "\n",
      "\n",
      "```python\n",
      "from langchain_community.llms import CerebriumAI\n",
      "```\n",
      "--------------------\n",
      "integrations/providers/huggingface.mdx-508-515\n",
      "This section describes Langchain integrations with Hugging Face Large Language Models (LLMs).\n",
      "\n",
      "## LLMs\n",
      "--------------------\n",
      "integrations/providers/google.mdx-6131-6365\n",
      "This code snippet shows how to use the Gemma large language model from the Vertex AI Model Garden using the `langchain-google-vertexai` library.\n",
      "\n",
      "### Gemma on Vertex AI Model Garden\n",
      "\n",
      "We need to install `langchain-google-vertexai` python package.\n",
      "\n",
      "```bash\n",
      "pip install langchain-google-vertexai\n",
      "```\n",
      "\n",
      "```python\n",
      "from langchain_google_vertexai.gemma import GemmaVertexAIModelGarden\n",
      "```\n",
      "--------------------\n",
      "integrations/providers/gpt4all.mdx-0-177\n",
      "Introduction to using the GPT4All LangChain wrapper.\n",
      "\n",
      "# GPT4All\n",
      "\n",
      "This page covers how to use the `GPT4All` wrapper within LangChain. The tutorial is divided into two parts: installation and setup, followed by usage with an example.\n",
      "--------------------\n",
      "integrations/providers/baseten.md-1443-1567\n",
      "This section details how to use Baseten LLMs with LangChain, providing a code example and link to a more detailed usage example.\n",
      "\n",
      "## LLMs\n",
      "\n",
      "See a [usage example](/docs/integrations/llms/baseten).\n",
      "\n",
      "```python\n",
      "from langchain_community.llms import Baseten\n",
      "```\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for node in retr_nodes:\n",
    "    print(node.node.node_id)\n",
    "    print(node.node.metadata['context'])\n",
    "    print(node.node.text)\n",
    "    print('-'*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_engine = vector_index.as_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Prompt Token Usage: 508\n",
      "LLM Completion Token Usage: 52\n",
      "** Messages: **\n",
      "system: You are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.\n",
      "\n",
      "## Tools\n",
      "\n",
      "You have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\n",
      "This may require breaking the task into subtasks and using different tools to complete each subtask.\n",
      "\n",
      "You have access to the following tools:\n",
      "> Tool Name: query_engine_tool\n",
      "Tool Description: Useful for running a natural language query\n",
      "against a knowledge base and get back a natural language response.\n",
      "\n",
      "Tool Args: {\"properties\": {\"input\": {\"title\": \"Input\", \"type\": \"string\"}}, \"required\": [\"input\"], \"type\": \"object\"}\n",
      "\n",
      "\n",
      "\n",
      "## Output Format\n",
      "\n",
      "Please answer in the same language as the question and use the following format:\n",
      "\n",
      "```\n",
      "Thought: The current language of the user is: (user's language). I need to use a tool to help me answer the question.\n",
      "Action: tool name (one of query_engine_tool) if using a tool.\n",
      "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\"input\": \"hello world\", \"num_beams\": 5})\n",
      "```\n",
      "\n",
      "Please ALWAYS start with a Thought.\n",
      "\n",
      "NEVER surround your response with markdown code markers. You may use code markers within your response if you need to.\n",
      "\n",
      "Please use a valid JSON format for the Action Input. Do NOT do this {'input': 'hello world', 'num_beams': 5}.\n",
      "\n",
      "If this format is used, the tool will respond in the following format:\n",
      "\n",
      "```\n",
      "Observation: tool response\n",
      "```\n",
      "\n",
      "You should keep repeating the above format till you have enough information to answer the question without using any more tools. At that point, you MUST respond in one of the following two formats:\n",
      "\n",
      "```\n",
      "Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: [your answer here (In the same language as the user's question)]\n",
      "```\n",
      "\n",
      "```\n",
      "Thought: I cannot answer the question with the provided tools.\n",
      "Answer: [your answer here (In the same language as the user's question)]\n",
      "```\n",
      "\n",
      "## Current Conversation\n",
      "\n",
      "Below is the current conversation consisting of interleaving human and assistant messages.\n",
      "\n",
      "user: how to use gemini in langchain?\n",
      "**************************************************\n",
      "** Response: **\n",
      "assistant: Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: query_engine_tool\n",
      "Action Input: {\"input\": \"How to use Gemini with Langchain\"}\n",
      "\n",
      "**************************************************\n",
      "\n",
      "\n",
      "Embedding Token Usage: 7\n",
      "LLM Prompt Token Usage: 357\n",
      "LLM Completion Token Usage: 72\n",
      "** Messages: **\n",
      "system: You are an expert Q&A system that is trusted around the world.\n",
      "Always answer the query using the provided context information, and not prior knowledge.\n",
      "Some rules to follow:\n",
      "1. Never directly reference the given context in your answer.\n",
      "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
      "user: Context information is below.\n",
      "---------------------\n",
      "[Excerpt from document]\n",
      "file_path: integrations/providers/google.mdx\n",
      "header_path: /Google/LLMs/\n",
      "context: LLMs section:  Describes how to access Google AI Gemini models using the GoogleGenerativeAI class within the Langchain framework.\n",
      "Excerpt:\n",
      "-----\n",
      "### Google Generative AI\n",
      "\n",
      "Access GoogleAI `Gemini` models such as `gemini-pro` and `gemini-pro-vision` through the `GoogleGenerativeAI` class.\n",
      "\n",
      "Install python package.\n",
      "\n",
      "```bash\n",
      "pip install langchain-google-genai\n",
      "```\n",
      "\n",
      "See a [usage example](/docs/integrations/llms/google_ai).\n",
      "\n",
      "```python\n",
      "from langchain_google_genai import GoogleGenerativeAI\n",
      "```\n",
      "-----\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: integrations/providers/google.mdx\n",
      "header_path: /Google/\n",
      "context: Section outlining different Large Language Models (LLMs) available for integration with Langchain, including Google's Gemini and models accessible through Vertex AI.\n",
      "Excerpt:\n",
      "-----\n",
      "## LLMs\n",
      "-----\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: How to use Gemini with Langchain\n",
      "Answer: \n",
      "**************************************************\n",
      "** Response: **\n",
      "assistant: To use Google's Gemini models with Langchain, you can access them through the `GoogleGenerativeAI` class. First, install the `langchain-google-genai` Python package. Then, you can import and use the `GoogleGenerativeAI` class in your code. A usage example is available in the documentation.\n",
      "\n",
      "**************************************************\n",
      "\n",
      "\n",
      "LLM Prompt Token Usage: 637\n",
      "LLM Completion Token Usage: 97\n",
      "** Messages: **\n",
      "system: You are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.\n",
      "\n",
      "## Tools\n",
      "\n",
      "You have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\n",
      "This may require breaking the task into subtasks and using different tools to complete each subtask.\n",
      "\n",
      "You have access to the following tools:\n",
      "> Tool Name: query_engine_tool\n",
      "Tool Description: Useful for running a natural language query\n",
      "against a knowledge base and get back a natural language response.\n",
      "\n",
      "Tool Args: {\"properties\": {\"input\": {\"title\": \"Input\", \"type\": \"string\"}}, \"required\": [\"input\"], \"type\": \"object\"}\n",
      "\n",
      "\n",
      "\n",
      "## Output Format\n",
      "\n",
      "Please answer in the same language as the question and use the following format:\n",
      "\n",
      "```\n",
      "Thought: The current language of the user is: (user's language). I need to use a tool to help me answer the question.\n",
      "Action: tool name (one of query_engine_tool) if using a tool.\n",
      "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\"input\": \"hello world\", \"num_beams\": 5})\n",
      "```\n",
      "\n",
      "Please ALWAYS start with a Thought.\n",
      "\n",
      "NEVER surround your response with markdown code markers. You may use code markers within your response if you need to.\n",
      "\n",
      "Please use a valid JSON format for the Action Input. Do NOT do this {'input': 'hello world', 'num_beams': 5}.\n",
      "\n",
      "If this format is used, the tool will respond in the following format:\n",
      "\n",
      "```\n",
      "Observation: tool response\n",
      "```\n",
      "\n",
      "You should keep repeating the above format till you have enough information to answer the question without using any more tools. At that point, you MUST respond in one of the following two formats:\n",
      "\n",
      "```\n",
      "Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: [your answer here (In the same language as the user's question)]\n",
      "```\n",
      "\n",
      "```\n",
      "Thought: I cannot answer the question with the provided tools.\n",
      "Answer: [your answer here (In the same language as the user's question)]\n",
      "```\n",
      "\n",
      "## Current Conversation\n",
      "\n",
      "Below is the current conversation consisting of interleaving human and assistant messages.\n",
      "\n",
      "user: how to use gemini in langchain?\n",
      "assistant: Thought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: query_engine_tool\n",
      "Action Input: {'input': 'How to use Gemini with Langchain'}\n",
      "user: Observation: To use Google's Gemini models with Langchain, you can access them through the `GoogleGenerativeAI` class. First, install the `langchain-google-genai` Python package. Then, you can import and use the `GoogleGenerativeAI` class in your code. A usage example is available in the documentation.\n",
      "\n",
      "**************************************************\n",
      "** Response: **\n",
      "assistant: Thought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: To use Google's Gemini models with Langchain, you can access them through the `GoogleGenerativeAI` class. First, install the `langchain-google-genai` Python package. Then, you can import and use the `GoogleGenerativeAI` class in your code. A usage example is available in the documentation.\n",
      "\n",
      "**************************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "resp = chat_engine.chat(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Google's Gemini models with Langchain, you can access them through the `GoogleGenerativeAI` class. First, install the `langchain-google-genai` Python package. Then, you can import and use the `GoogleGenerativeAI` class in your code. A usage example is available in the documentation.\n"
     ]
    }
   ],
   "source": [
    "print(resp.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[NodeWithScore(node=TextNode(id_='additional_resources/tutorials.mdx-38-149', embedding=None, metadata={'file_path': 'additional_resources/tutorials.mdx', 'file_name': 'tutorials.mdx', 'file_size': 3498, 'creation_date': '2025-02-21', 'last_modified_date': '2025-02-21', 'header_path': '/3rd Party Tutorials/ Tutorials/', 'context': 'This is a YouTube playlist tutorial on LangChain v0.1 from LangChain.ai, listed under the \"Tutorials\" section of a document compiling various LangChain learning resources.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='additional_resources/tutorials.mdx', node_type='4', metadata={'file_path': 'additional_resources/tutorials.mdx', 'file_name': 'tutorials.mdx', 'file_size': 3498, 'creation_date': '2025-02-21', 'last_modified_date': '2025-02-21'}, hash='f799f710a55a4b78e9c1cc050ee88da721debbf3b7eceb07bec1ca708c9958fd'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='7848650a-ca19-4b27-8c4b-98c97aec9942', node_type='1', metadata={'file_path': 'additional_resources/tutorials.mdx', 'file_name': 'tutorials.mdx', 'file_size': 3498, 'creation_date': '2025-02-21', 'last_modified_date': '2025-02-21', 'header_path': '/3rd Party Tutorials/'}, hash='8b657919e980924f22027e8a186663973f2c734da7aa0d90c2940bcaa6bb5677'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='74d440d0-e273-4105-b98b-72108d09842b', node_type='1', metadata={'header_path': '/3rd Party Tutorials/ Tutorials/'}, hash='2f3edc36e0c1ee5cd220cb92d6a005d7c19795175089ff73861373ab0e107500')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='### [LangChain v 0.1 by LangChain.ai](https://www.youtube.com/playlist?list=PLfaIDFEXuae0gBSJ9T0w7cu7iJZbH3T31)', mimetype='text/plain', start_char_idx=38, end_char_idx=149, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.6604187931016724),\n",
       " NodeWithScore(node=TextNode(id_='introduction.mdx-5372-5649', embedding=None, metadata={'file_path': 'introduction.mdx', 'file_name': 'introduction.mdx', 'file_size': 6143, 'creation_date': '2025-02-21', 'last_modified_date': '2025-02-21', 'header_path': '/Introduction/Ecosystem/', 'context': 'This section is part of the \"Ecosystem\" section of the LangChain documentation, describing LangGraph as a tool for building stateful LLM applications.\\n'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='introduction.mdx', node_type='4', metadata={'file_path': 'introduction.mdx', 'file_name': 'introduction.mdx', 'file_size': 6143, 'creation_date': '2025-02-21', 'last_modified_date': '2025-02-21'}, hash='c05d1880fd55d5189fde491b6a9134a306c81e41f9589e06ee2831574c661497'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='35c83c22-aadb-4a28-acda-6da7ef9a1cb8', node_type='1', metadata={'file_path': 'introduction.mdx', 'file_name': 'introduction.mdx', 'file_size': 6143, 'creation_date': '2025-02-21', 'last_modified_date': '2025-02-21', 'header_path': '/Introduction/Ecosystem/'}, hash='8d10dcf3dee47e0fd685f55016c8ef57e0a8d0a13e3495f0ac1eaf9f79f90393'), <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='65d22abf-fdaf-427a-915f-807fdfd16cf0', node_type='1', metadata={'header_path': '/Introduction/'}, hash='553eabdf5dac79fa1b1cf367198b3d45585b3c5847a9ffec3c8faec319aa3d94')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='### [ü¶úüï∏Ô∏è LangGraph](https://langchain-ai.github.io/langgraph)\\nBuild stateful, multi-actor applications with LLMs. Integrates smoothly with LangChain, but can be used without it. LangGraph powers production-grade agents, trusted by Linkedin, Uber, Klarna, GitLab, and many more.', mimetype='text/plain', start_char_idx=5372, end_char_idx=5649, metadata_seperator='\\n', text_template='[Excerpt from document]\\n{metadata_str}\\nExcerpt:\\n-----\\n{content}\\n-----\\n'), score=0.6516497350224891)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.source_nodes#[0].node.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.660 doc: additional_resources/tutorials.mdx pos: 1% start 38 end 149\n",
      "score: 0.652 doc: introduction.mdx pos: 87% start 5372 end 5649\n"
     ]
    }
   ],
   "source": [
    "for source_node in resp.source_nodes:\n",
    "    print('score:', f\"{source_node.score:.3f}\", 'doc:', source_node.node.ref_doc_id, 'pos:', f\"{source_node.node.start_char_idx*100/source_node.node.metadata['file_size'] :.0f}%\", 'start', source_node.node.start_char_idx, 'end', source_node.node.end_char_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='what is Langchain?')]),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='Langchain is a framework that seems to be used for building LLM applications. One resource mentions a YouTube playlist tutorial on LangChain v0.1. Another resource mentions a tool called LangGraph that integrates with LangChain and helps build stateful LLM applications.')])]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "staring query\n",
      "Embedding Token Usage: 1\n",
      "LLM Prompt Token Usage: 239\n",
      "LLM Completion Token Usage: 17\n",
      "** Messages: **\n",
      "system: You are an expert Q&A system that is trusted around the world.\n",
      "Always answer the query using the provided context information, and not prior knowledge.\n",
      "Some rules to follow:\n",
      "1. Never directly reference the given context in your answer.\n",
      "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
      "user: Context information is below.\n",
      "---------------------\n",
      "[Excerpt from document]\n",
      "file_path: integrations/text_embedding/index.mdx\n",
      "header_path: /\n",
      "context: Frontmatter metadata for the document.\n",
      "Excerpt:\n",
      "-----\n",
      "---\n",
      "sidebar_position: 0\n",
      "sidebar_class_name: hidden\n",
      "---\n",
      "-----\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: introduction.mdx\n",
      "header_path: /\n",
      "context: Frontmatter metadata for the document's introduction.\n",
      "Excerpt:\n",
      "-----\n",
      "---\n",
      "sidebar_position: 0\n",
      "sidebar_class_name: hidden\n",
      "---\n",
      "-----\n",
      "---------------------\n",
      "Given the context information and not prior knowledge, answer the query.\n",
      "Query: q\n",
      "Answer: \n",
      "**************************************************\n",
      "** Response: **\n",
      "assistant: I am sorry, I cannot answer this question as the query is empty.\n",
      "**************************************************\n",
      "\n",
      "\n",
      "I am sorry, I cannot answer this question as the query is empty.\n",
      "finished\n",
      "yo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query Data from the persisted index\n",
    "query_engine = vector_index.as_query_engine()\n",
    "\n",
    "print('staring query')\n",
    "\n",
    "query = input('Enter query (press q to quit): ')\n",
    "response = query_engine.query(query)\n",
    "print(response)\n",
    "\n",
    "\n",
    "print('finished')\n",
    "print('yo')\n",
    "# logger.info('hey')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, get_response_synthesizer\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.chat_engine import ContextChatEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(index=vector_index, similarity_top_k=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Token Usage: 11\n",
      "LLM Prompt Token Usage: 3174\n",
      "LLM Completion Token Usage: 450\n",
      "** Messages: **\n",
      "system: You are an expert Q&A system that is trusted around the world.\n",
      "Always answer the query using the provided context information, and not prior knowledge.\n",
      "Some rules to follow:\n",
      "1. Never directly reference the given context in your answer.\n",
      "2. Avoid statements like 'Based on the context, ...' or 'The context information ...' or anything along those lines.\n",
      "user: Context information from multiple sources is below.\n",
      "---------------------\n",
      "[Excerpt from document]\n",
      "file_path: integrations/providers/google.mdx\n",
      "header_path: /Google/LLMs/\n",
      "context: LLMs section:  Describes how to access Google AI Gemini models using the GoogleGenerativeAI class within the Langchain framework.\n",
      "Excerpt:\n",
      "-----\n",
      "### Google Generative AI\n",
      "\n",
      "Access GoogleAI `Gemini` models such as `gemini-pro` and `gemini-pro-vision` through the `GoogleGenerativeAI` class.\n",
      "\n",
      "Install python package.\n",
      "\n",
      "```bash\n",
      "pip install langchain-google-genai\n",
      "```\n",
      "\n",
      "See a [usage example](/docs/integrations/llms/google_ai).\n",
      "\n",
      "```python\n",
      "from langchain_google_genai import GoogleGenerativeAI\n",
      "```\n",
      "-----\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: integrations/providers/google.mdx\n",
      "header_path: /Google/\n",
      "context: Section outlining different Large Language Models (LLMs) available for integration with Langchain, including Google's Gemini and models accessible through Vertex AI.\n",
      "Excerpt:\n",
      "-----\n",
      "## LLMs\n",
      "-----\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: integrations/providers/google.mdx\n",
      "header_path: /Google/Chat models/\n",
      "context: This section describes how to access Google's Gemini chat models via the Vertex AI platform using the `langchain-google-vertexai` package.\n",
      "Excerpt:\n",
      "-----\n",
      "### Vertex AI\n",
      "\n",
      "Access chat models like `Gemini` via Google Cloud.\n",
      "\n",
      "We need to install `langchain-google-vertexai` python package.\n",
      "\n",
      "```bash\n",
      "pip install langchain-google-vertexai\n",
      "```\n",
      "\n",
      "See a [usage example](/docs/integrations/chat/google_vertex_ai_palm).\n",
      "\n",
      "```python\n",
      "from langchain_google_vertexai import ChatVertexAI\n",
      "```\n",
      "-----\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: integrations/providers/google.mdx\n",
      "header_path: /Google/Chat models/\n",
      "context: This section details how to use the Google Generative AI (Gemini) models, including `gemini-pro` and `gemini-pro-vision`, via the `ChatGoogleGenerativeAI` class, showing code examples for text and image inputs.\n",
      "Excerpt:\n",
      "-----\n",
      "### Google Generative AI\n",
      "\n",
      "Access GoogleAI `Gemini` models such as `gemini-pro` and `gemini-pro-vision` through the `ChatGoogleGenerativeAI` class.\n",
      "\n",
      "```bash\n",
      "pip install -U langchain-google-genai\n",
      "```\n",
      "\n",
      "Configure your API key.\n",
      "\n",
      "```bash\n",
      "export GOOGLE_API_KEY=your-api-key\n",
      "```\n",
      "\n",
      "```python\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "\n",
      "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
      "llm.invoke(\"Sing a ballad of LangChain.\")\n",
      "```\n",
      "\n",
      "Gemini vision model supports image inputs when providing a single chat message.\n",
      "\n",
      "```python\n",
      "from langchain_core.messages import HumanMessage\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "\n",
      "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
      "\n",
      "message = HumanMessage(\n",
      "    content=[\n",
      "        {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"What's in this image?\",\n",
      "        },  # You can optionally provide text parts\n",
      "        {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"},\n",
      "    ]\n",
      ")\n",
      "llm.invoke([message])\n",
      "```\n",
      "\n",
      "The value of image_url can be any of the following:\n",
      "\n",
      "- A public image URL\n",
      "- A gcs file (e.g., \"gcs://path/to/file.png\")\n",
      "- A local file path\n",
      "- A base64 encoded image (e.g., data:image/png;base64,abcd124)\n",
      "- A PIL image\n",
      "-----\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: additional_resources/dependents.mdx\n",
      "header_path: /\n",
      "context: Table of Langchain dependent repositories with more than 100 stars, sorted by stars.\n",
      "Excerpt:\n",
      "-----\n",
      "com/nftblackmagic/flask-langchain) | 121 |\n",
      "|[vishwasg217/finsight](https://github.com/vishwasg217/finsight) | 120 |\n",
      "|[snap-stanford/MLAgentBench](https://github.com/snap-stanford/MLAgentBench) | 120 |\n",
      "|[Azure/app-service-linux-docs](https://github.com/Azure/app-service-linux-docs) | 120 |\n",
      "|[nyanp/chat2plot](https://github.com/nyanp/chat2plot) | 120 |\n",
      "|[ant4g0nist/polar](https://github.com/ant4g0nist/polar) | 119 |\n",
      "|[aws-samples/cdk-eks-blueprints-patterns](https://github.com/aws-samples/cdk-eks-blueprints-patterns) | 119 |\n",
      "|[aws-samples/amazon-kendra-langchain-extensions](https://github.com/aws-samples/amazon-kendra-langchain-extensions) | 119 |\n",
      "|[Xueheng-Li/SynologyChatbotGPT](https://github.com/Xueheng-Li/SynologyChatbotGPT) | 119 |\n",
      "|[CodeAlchemyAI/ViLT-GPT](https://github.com/CodeAlchemyAI/ViLT-GPT) | 117 |\n",
      "|[Lin-jun-xiang/docGPT-langchain](https://github.com/Lin-jun-xiang/docGPT-langchain) | 117 |\n",
      "|[ademakdogan/ChatSQL](https://github.com/ademakdogan/ChatSQL) | 116 |\n",
      "|[aniketmaurya/llm-inference](https://github.com/aniketmaurya/llm-inference) | 115 |\n",
      "|[xuwenhao/mactalk-ai-course](https://github.com/xuwenhao/mactalk-ai-course) | 115 |\n",
      "|[cmooredev/RepoReader](https://github.com/cmooredev/RepoReader) | 115 |\n",
      "|[abi/autocommit](https://github.com/abi/autocommit) | 115 |\n",
      "|[MIDORIBIN/langchain-gpt4free](https://github.com/MIDORIBIN/langchain-gpt4free) | 114 |\n",
      "|[finaldie/auto-news](https://github.com/finaldie/auto-news) | 114 |\n",
      "|[Anil-matcha/Youtube-to-chatbot](https://github.com/Anil-matcha/Youtube-to-chatbot) | 114 |\n",
      "|[avrabyt/MemoryBot](https://github.com/avrabyt/MemoryBot) | 114 |\n",
      "|[Capsize-Games/airunner](https://github.com/Capsize-Games/airunner) | 113 |\n",
      "|[atisharma/llama_farm](https://github.com/atisharma/llama_farm) | 113 |\n",
      "|[mbchang/data-driven-characters](https://github.com/mbchang/data-driven-characters) | 112 |\n",
      "|[fiddler-labs/fiddler-auditor](https://github.com/fiddler-labs/fiddler-auditor) | 112 |\n",
      "|[dirkjbreeuwer/gpt-automated-web-scraper](https://github.com/dirkjbreeuwer/gpt-automated-web-scraper) | 111 |\n",
      "|[Appointat/Chat-with-Document-s-using-ChatGPT-API-and-Text-Embedding](https://github.com/Appointat/Chat-with-Document-s-using-ChatGPT-API-and-Text-Embedding) | 111 |\n",
      "|[hwchase17/langchain-gradio-template](https://github.com/hwchase17/langchain-gradio-template) | 111 |\n",
      "|[artas728/spelltest](https://github.com/artas728/spelltest) | 110 |\n",
      "|[NVIDIA/GenerativeAIExamples](https://github.com/NVIDIA/GenerativeAIExamples) | 109 |\n",
      "|[Azure/aistudio-copilot-sample](https://github.com/Azure/aistudio-copilot-sample) | 108 |\n",
      "|[codefuse-ai/codefuse-chatbot](https://github.com/codefuse-ai/codefuse-chatbot) | 108 |\n",
      "|[apirrone/Memento](https://github.com/apirrone/Memento) | 108 |\n",
      "|[e-johnstonn/GPT-Doc-Summarizer](https://github.com/e-johnstonn/GPT-Doc-Summarizer) | 108 |\n",
      "|[salesforce/BOLAA](https://github.com/salesforce/BOLAA) | 107 |\n",
      "|[Erol444/gpt4-openai-api](https://github.com/Erol444/gpt4-openai-api) | 106 |\n",
      "|[linjungz/chat-with-your-doc](https://github.com/linjungz/chat-with-your-doc) | 106 |\n",
      "|[crosleythomas/MirrorGPT](https://github.com/crosleythomas/MirrorGPT) | 106 |\n",
      "|[panaverse/learn-generative-ai](https://github.com/panaverse/learn-generative-ai) | 105 |\n",
      "|[Azure/azure-sdk-tools](https://github.com/Azure/azure-sdk-tools) | 105 |\n",
      "|[malywut/gpt_examples](https://github.com/malywut/gpt_examples) | 105 |\n",
      "|[ritun16/chain-of-verification](https://github.com/ritun16/chain-of-verification) | 104 |\n",
      "|[langchain-ai/langchain-benchmarks](https://github.com/langchain-ai/langchain-benchmarks) | 104 |\n",
      "|[lightninglabs/LangChainBitcoin](https://github.com/lightninglabs/LangChainBitcoin) | 104 |\n",
      "|[flepied/second-brain-agent](https://github.com/flepied/second-brain-agent) | 103 |\n",
      "|[llmapp/openai.mini](https://github.com/llmapp/openai.mini) | 102 |\n",
      "|[gimlet-ai/tddGPT](https://github.com/gimlet-ai/tddGPT) | 102 |\n",
      "|[jlonge4/gpt_chatwithPDF](https://github.com/jlonge4/gpt_chatwithPDF) | 102 |\n",
      "|[agentification/RAFA_code](https://github.com/agentification/RAFA_code) | 101 |\n",
      "|[pacman100/DHS-LLM-Workshop](https://github.com/pacman100/DHS-LLM-Workshop) | 101 |\n",
      "|[aws-samples/private-llm-qa-bot](https://github.com/aws-samples/private-llm-qa-bot) | 101 |\n",
      "\n",
      "\n",
      "_Generated by [github-dependents-info](https://github.com/nvuillam/github-dependents-info)_\n",
      "\n",
      "`github-dependents-info --repo \"langchain-ai/langchain\" --markdownfile dependents.md --minstars 100 --sort stars`\n",
      "-----\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: integrations/providers/google.mdx\n",
      "header_path: /Google/LLMs/\n",
      "context: This code snippet shows how to use the Gemma large language model from the Vertex AI Model Garden using the `langchain-google-vertexai` library.\n",
      "Excerpt:\n",
      "-----\n",
      "### Gemma on Vertex AI Model Garden\n",
      "\n",
      "We need to install `langchain-google-vertexai` python package.\n",
      "\n",
      "```bash\n",
      "pip install langchain-google-vertexai\n",
      "```\n",
      "\n",
      "```python\n",
      "from langchain_google_vertexai.gemma import GemmaVertexAIModelGarden\n",
      "```\n",
      "-----\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: integrations/providers/google.mdx\n",
      "header_path: /Google/LLMs/\n",
      "context: This code snippet shows how to load a local Gemma model from Hugging Face using the `langchain-google-vertexai` library.  It's part of the larger section on Google LLMs.\n",
      "Excerpt:\n",
      "-----\n",
      "### Gemma local from Hugging Face\n",
      "\n",
      ">Local `Gemma` model loaded from `HuggingFace`.\n",
      "\n",
      "We need to install `langchain-google-vertexai` python package.\n",
      "\n",
      "```bash\n",
      "pip install langchain-google-vertexai\n",
      "```\n",
      "\n",
      "```python\n",
      "from langchain_google_vertexai.gemma import GemmaLocalHF\n",
      "```\n",
      "-----\n",
      "\n",
      "[Excerpt from document]\n",
      "file_path: integrations/providers/littlellm.md\n",
      "header_path: /LiteLLM/\n",
      "context: This section details how to install the LiteLLM Python package using pip.\n",
      "Excerpt:\n",
      "-----\n",
      "## Installation and setup\n",
      "\n",
      "Install the `litellm` python package.\n",
      "\n",
      "```bash\n",
      "pip install litellm\n",
      "```\n",
      "-----\n",
      "---------------------\n",
      "Given the information from multiple sources and not prior knowledge, answer the query.\n",
      "Query: how to use google gemini as the llm?\n",
      "Answer: \n",
      "**************************************************\n",
      "** Response: **\n",
      "assistant: To use Google's Gemini models as your LLM, you can use the `GoogleGenerativeAI` class to access models like `gemini-pro` and `gemini-pro-vision`. First, install the `langchain-google-genai` Python package.\n",
      "\n",
      "```bash\n",
      "pip install langchain-google-genai\n",
      "```\n",
      "\n",
      "Then, you can access the models:\n",
      "\n",
      "```python\n",
      "from langchain_google_genai import GoogleGenerativeAI\n",
      "```\n",
      "\n",
      "You can also access `Gemini` chat models via Google Cloud using the `ChatVertexAI` class after installing the `langchain-google-vertexai` package.\n",
      "\n",
      "```bash\n",
      "pip install langchain-google-vertexai\n",
      "```\n",
      "\n",
      "```python\n",
      "from langchain_google_vertexai import ChatVertexAI\n",
      "```\n",
      "\n",
      "Alternatively, you can use the `ChatGoogleGenerativeAI` class to access `gemini-pro` and `gemini-pro-vision`.  You'll need to configure your API key.\n",
      "\n",
      "```bash\n",
      "export GOOGLE_API_KEY=your-api-key\n",
      "```\n",
      "\n",
      "To use the Gemini models with `ChatGoogleGenerativeAI`:\n",
      "\n",
      "```python\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "\n",
      "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
      "llm.invoke(\"Sing a ballad of LangChain.\")\n",
      "```\n",
      "\n",
      "The Gemini vision model supports image inputs when providing a single chat message.\n",
      "\n",
      "```python\n",
      "from langchain_core.messages import HumanMessage\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "\n",
      "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
      "\n",
      "message = HumanMessage(\n",
      "    content=[\n",
      "        {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"What's in this image?\",\n",
      "        },  # You can optionally provide text parts\n",
      "        {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"},\n",
      "    ]\n",
      ")\n",
      "llm.invoke([message])\n",
      "```\n",
      "\n",
      "**************************************************\n",
      "\n",
      "\n",
      "To use Google's Gemini models as your LLM, you can use the `GoogleGenerativeAI` class to access models like `gemini-pro` and `gemini-pro-vision`. First, install the `langchain-google-genai` Python package.\n",
      "\n",
      "```bash\n",
      "pip install langchain-google-genai\n",
      "```\n",
      "\n",
      "Then, you can access the models:\n",
      "\n",
      "```python\n",
      "from langchain_google_genai import GoogleGenerativeAI\n",
      "```\n",
      "\n",
      "You can also access `Gemini` chat models via Google Cloud using the `ChatVertexAI` class after installing the `langchain-google-vertexai` package.\n",
      "\n",
      "```bash\n",
      "pip install langchain-google-vertexai\n",
      "```\n",
      "\n",
      "```python\n",
      "from langchain_google_vertexai import ChatVertexAI\n",
      "```\n",
      "\n",
      "Alternatively, you can use the `ChatGoogleGenerativeAI` class to access `gemini-pro` and `gemini-pro-vision`.  You'll need to configure your API key.\n",
      "\n",
      "```bash\n",
      "export GOOGLE_API_KEY=your-api-key\n",
      "```\n",
      "\n",
      "To use the Gemini models with `ChatGoogleGenerativeAI`:\n",
      "\n",
      "```python\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "\n",
      "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
      "llm.invoke(\"Sing a ballad of LangChain.\")\n",
      "```\n",
      "\n",
      "The Gemini vision model supports image inputs when providing a single chat message.\n",
      "\n",
      "```python\n",
      "from langchain_core.messages import HumanMessage\n",
      "from langchain_google_genai import ChatGoogleGenerativeAI\n",
      "\n",
      "llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\")\n",
      "\n",
      "message = HumanMessage(\n",
      "    content=[\n",
      "        {\n",
      "            \"type\": \"text\",\n",
      "            \"text\": \"What's in this image?\",\n",
      "        },  # You can optionally provide text parts\n",
      "        {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"},\n",
      "    ]\n",
      ")\n",
      "llm.invoke([message])\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# configure response synthesizer\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    ")\n",
    "\n",
    "# assemble query engine\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    ")\n",
    "\n",
    "# query\n",
    "response = query_engine.query(\"how to use google gemini as the llm?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1143"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(sn.node.text) for sn in response.source_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "317"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode('\\n'.join([sn.node.text for sn in response.source_nodes]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(response.get_formatted_sources()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.679 doc: additional_resources/tutorials.mdx pos: 1% start 38 end 149\n",
      "score: 0.666 doc: changes/changelog/core.mdx pos: 0% start 0 end 16\n",
      "score: 0.661 doc: introduction.mdx pos: 87% start 5372 end 5649\n",
      "score: 0.658 doc: changes/changelog/langchain.mdx pos: 0% start 13 end 21\n",
      "score: 0.657 doc: additional_resources/tutorials.mdx pos: 8% start 278 end 383\n",
      "score: 0.656 doc: concepts/chat_models.mdx pos: 0% start 0 end 13\n",
      "score: 0.655 doc: concepts/why_langchain.mdx pos: 0% start 0 end 603\n",
      "score: 0.654 doc: versions/v0_2/overview.mdx pos: 0% start 29 end 39\n"
     ]
    }
   ],
   "source": [
    "for source_node in response.source_nodes:\n",
    "    print('score:', f\"{source_node.score:.3f}\", 'doc:', source_node.node.ref_doc_id, 'pos:', f\"{source_node.node.start_char_idx*100/source_node.node.metadata['file_size'] :.0f}%\", 'start', source_node.node.start_char_idx, 'end', source_node.node.end_char_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = retriever.retrieve(\"how to download a webpage as an image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.653 doc: Core/Link_Media.md pos: 60% start 8880 end 10869\n",
      "score: 0.647 doc: Advanced/Overview.md pos: 41% start 6926 end 8449\n",
      "score: 0.644 doc: Core/Link_Media.md pos: 75% start 11231 end 11573\n",
      "score: 0.617 doc: Core/Crawler_Result.md pos: 86% start 12569 end 12887\n",
      "score: 0.613 doc: Core/Content_Selection.md pos: 75% start 13747 end 16261\n",
      "score: 0.610 doc: Advanced/File_Downloading.md pos: 54% start 5020 end 5446\n",
      "score: 0.603 doc: Core/Crawler_Result.md pos: 84% start 12253 end 12565\n",
      "score: 0.602 doc: Core/Link_Media.md pos: 29% start 4250 end 5104\n"
     ]
    }
   ],
   "source": [
    "for source_node in ret:\n",
    "    print('score:', f\"{source_node.score:.3f}\", 'doc:', source_node.node.ref_doc_id, 'pos:', f\"{source_node.node.start_char_idx*100/source_node.node.metadata['file_size'] :.0f}%\", 'start', source_node.node.start_char_idx, 'end', source_node.node.end_char_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat engine with the retriever\n",
    "chat_engine = ContextChatEngine.from_defaults(\n",
    "    retriever=retriever,\n",
    "    system_prompt=\"You are a helpful AI assistant. Use the provided context to answer questions.\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can download a webpage as an image (screenshot) using Crawl4AI by setting the `screenshot` parameter to `True` in your crawler configuration. The screenshot will be stored as a base64-encoded PNG string in the `result.screenshot` attribute of the crawl result.\n",
      "\n",
      "Here's an example:\n",
      "\n",
      "```python\n",
      "import os, asyncio\n",
      "from base64 import b64decode\n",
      "from crawl4ai import AsyncWebCrawler, CacheMode\n",
      "async def main():\n",
      "  async with AsyncWebCrawler() as crawler:\n",
      "    result = await crawler.arun(\n",
      "      url=\"https://en.wikipedia.org/wiki/List_of_common_misconceptions\",\n",
      "      cache_mode=CacheMode.BYPASS,\n",
      "      screenshot=True\n",
      "    )\n",
      "    if result.success:\n",
      "      # Save screenshot\n",
      "      if result.screenshot:\n",
      "        with open(\"wikipedia_screenshot.png\", \"wb\") as f:\n",
      "          f.write(b64decode(result.screenshot))\n",
      "      print(\"[OK] Screenshot captured.\")\n",
      "    else:\n",
      "      print(\"[ERROR]\", result.error_message)\n",
      "if __name__ == \"__main__\":\n",
      "  asyncio.run(main())\n",
      "```\n",
      "\n",
      "This code snippet demonstrates how to capture a screenshot of a webpage and save it as a PNG file.  The `b64decode` function from the `base64` module is used to decode the base64-encoded string into raw bytes, which are then written to the file.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"how to download a webpage as an image\"\n",
    "response = chat_engine.chat()\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_nodes = response.source_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.653 doc: Core/Link_Media.md pos: 60% start 8880 end 10869\n",
      "score: 0.647 doc: Advanced/Overview.md pos: 41% start 6926 end 8449\n",
      "score: 0.644 doc: Core/Link_Media.md pos: 75% start 11231 end 11573\n",
      "score: 0.617 doc: Core/Crawler_Result.md pos: 86% start 12569 end 12887\n",
      "score: 0.613 doc: Core/Content_Selection.md pos: 75% start 13747 end 16261\n",
      "score: 0.610 doc: Advanced/File_Downloading.md pos: 54% start 5020 end 5446\n",
      "score: 0.603 doc: Core/Crawler_Result.md pos: 84% start 12253 end 12565\n",
      "score: 0.602 doc: Core/Link_Media.md pos: 29% start 4250 end 5104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for source_node in ref_nodes:\n",
    "    print('score:', f\"{source_node.score:.3f}\", 'doc:', source_node.node.ref_doc_id, 'pos:', f\"{source_node.node.start_char_idx*100/source_node.node.metadata['file_size'] :.0f}%\", 'start', source_node.node.start_char_idx, 'end', source_node.node.end_char_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hybrid:\n",
    "# from llama_index.retrievers import BM25Retriever, VectorIndexRetriever\n",
    "\n",
    "# bm25_retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=10)\n",
    "# vector_retriever = VectorIndexRetriever(index)\n",
    "# from llama_index.retrievers import BaseRetriever\n",
    "\n",
    "# class HybridRetriever(BaseRetriever):\n",
    "# def __init__(self, vector_retriever, bm25_retriever):\n",
    "# self.vector_retriever = vector_retriever\n",
    "# self.bm25_retriever = bm25_retriever\n",
    "\n",
    "# def _retrieve(self, query, **kwargs):\n",
    "# bm25_nodes = self.bm25_retriever.retrieve(query, **kwargs)\n",
    "# vector_nodes = self.vector_retriever.retrieve(query, **kwargs)\n",
    "# all_nodes = []\n",
    "# node_ids = set()\n",
    "# for n in bm25_nodes + vector_nodes:\n",
    "# if n.node.node_id not in node_ids:\n",
    "# all_nodes.append(n)\n",
    "# node_ids.add(n.node.node_id)\n",
    "# return all_nodes\n",
    "\n",
    "# hybrid_retriever = HybridRetriever(vector_retriever, bm25_retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "cohere_rerank = CohereRerank(\n",
    "    top_n=2, model=\"rerank-v3.5\", api_key=os.environ['COHERE_API_KEY']\n",
    ")\n",
    "\n",
    "sorted_nodes = cohere_rerank.postprocess_nodes(nodes=ref_nodes, query_str=\"how to download a webpage as an image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.243 doc: Advanced/Overview.md pos: 41% start 6926 end 8449\n",
      "score: 0.242 doc: Core/Link_Media.md pos: 75% start 11231 end 11573\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for source_node in sorted_nodes:\n",
    "    print('score:', f\"{source_node.score:.3f}\", 'doc:', source_node.node.ref_doc_id, 'pos:', f\"{source_node.node.start_char_idx*100/source_node.node.metadata['file_size'] :.0f}%\", 'start', source_node.node.start_char_idx, 'end', source_node.node.end_char_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import os, asyncio\n",
      "from base64 import b64decode\n",
      "from crawl4ai import AsyncWebCrawler, CacheMode\n",
      "async def main():\n",
      "  async with AsyncWebCrawler() as crawler:\n",
      "    result = await crawler.arun(\n",
      "      url=\"https://en.wikipedia.org/wiki/List_of_common_misconceptions\",\n",
      "      cache_mode=CacheMode.BYPASS,\n",
      "      screenshot=True\n",
      "    )\n",
      "    if result.success:\n",
      "      # Save screenshot\n",
      "      if result.screenshot:\n",
      "        with open(\"wikipedia_screenshot.png\", \"wb\") as f:\n",
      "          f.write(b64decode(result.screenshot))\n",
      "      print(\"[OK] Screenshot captured.\")\n",
      "      print(result) # Print the crawl result\n",
      "    else:\n",
      "      print(\"[ERROR]\", result.error_message)\n",
      "if __name__ == \"__main__\":\n",
      "  asyncio.run(main())\n",
      "```\n",
      "\n",
      "This code will now print the entire `CrawlResult` object to the console, allowing you to inspect all its attributes, including `url`, `status_code`, `html`, `markdown_v2`, `media`, `links`, `screenshot`, and more.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat_engine.chat(\"now print the crawl result\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.778 doc: API_Reference/CrawlResult.md pos: 82% start 14476 end 15782\n",
      "score: 0.774 doc: API_Reference/CrawlResult.md pos: 33% start 5861 end 6017\n",
      "score: 0.756 doc: Core/Crawler_Result.md pos: 89% start 13055 end 13688\n",
      "score: 0.750 doc: Core/Simple_Crawling.md pos: 76% start 6673 end 6975\n",
      "score: 0.746 doc: API_Reference/CrawlResult.md pos: 36% start 6429 end 6633\n",
      "score: 0.745 doc: API_Reference/CrawlResult.md pos: 34% start 6021 end 6237\n",
      "score: 0.744 doc: Core/Crawler_Result.md pos: 42% start 6084 end 8277\n",
      "score: 0.739 doc: Core/Crawler_Result.md pos: 84% start 12253 end 12565\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for source_node in response.source_nodes:\n",
    "    print('score:', f\"{source_node.score:.3f}\", 'doc:', source_node.node.ref_doc_id, 'pos:', f\"{source_node.node.start_char_idx*100/source_node.node.metadata['file_size'] :.0f}%\", 'start', source_node.node.start_char_idx, 'end', source_node.node.end_char_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='how to download a webpage as an image')]),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='You can download a webpage as an image (screenshot) using Crawl4AI by setting the `screenshot` parameter to `True` in your crawler configuration. The screenshot will be stored as a base64-encoded PNG string in the `result.screenshot` attribute of the crawl result.\\n\\nHere\\'s an example:\\n\\n```python\\nimport os, asyncio\\nfrom base64 import b64decode\\nfrom crawl4ai import AsyncWebCrawler, CacheMode\\nasync def main():\\n  async with AsyncWebCrawler() as crawler:\\n    result = await crawler.arun(\\n      url=\"https://en.wikipedia.org/wiki/List_of_common_misconceptions\",\\n      cache_mode=CacheMode.BYPASS,\\n      screenshot=True\\n    )\\n    if result.success:\\n      # Save screenshot\\n      if result.screenshot:\\n        with open(\"wikipedia_screenshot.png\", \"wb\") as f:\\n          f.write(b64decode(result.screenshot))\\n      print(\"[OK] Screenshot captured.\")\\n    else:\\n      print(\"[ERROR]\", result.error_message)\\nif __name__ == \"__main__\":\\n  asyncio.run(main())\\n```\\n\\nThis code snippet demonstrates how to capture a screenshot of a webpage and save it as a PNG file.  The `b64decode` function from the `base64` module is used to decode the base64-encoded string into raw bytes, which are then written to the file.\\n')]),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='now print the crawl result')]),\n",
       " ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='```python\\nimport os, asyncio\\nfrom base64 import b64decode\\nfrom crawl4ai import AsyncWebCrawler, CacheMode\\nasync def main():\\n  async with AsyncWebCrawler() as crawler:\\n    result = await crawler.arun(\\n      url=\"https://en.wikipedia.org/wiki/List_of_common_misconceptions\",\\n      cache_mode=CacheMode.BYPASS,\\n      screenshot=True\\n    )\\n    if result.success:\\n      # Save screenshot\\n      if result.screenshot:\\n        with open(\"wikipedia_screenshot.png\", \"wb\") as f:\\n          f.write(b64decode(result.screenshot))\\n      print(\"[OK] Screenshot captured.\")\\n      print(result) # Print the crawl result\\n    else:\\n      print(\"[ERROR]\", result.error_message)\\nif __name__ == \"__main__\":\\n  asyncio.run(main())\\n```\\n\\nThis code will now print the entire `CrawlResult` object to the console, allowing you to inspect all its attributes, including `url`, `status_code`, `html`, `markdown_v2`, `media`, `links`, `screenshot`, and more.\\n')])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_engine.chat_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from pydantic import BaseModel\n",
      "from crawl4ai.extraction_strategy import LLMExtractionStrategy\n",
      "# Define schema\n",
      "class ProductData(BaseModel):\n",
      "  title: str\n",
      "  content: str\n",
      "# Create strategy\n",
      "strategy = LLMExtractionStrategy(\n",
      "  provider=\"ollama/llama2\",\n",
      "  schema=ProductData.schema(),\n",
      "  instruction=\"Extract article details\"\n",
      ")\n",
      "# Use with crawler\n",
      "result = await crawler.arun(\n",
      "  url=\"https://example.com/article\",\n",
      "  extraction_strategy=strategy\n",
      ")\n",
      "# Access extracted data\n",
      "data = json.loads(result.extracted_content)\n",
      "```"
     ]
    }
   ],
   "source": [
    "query_engine = vector_index.as_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    node_postprocessors=[cohere_rerank],\n",
    "    streaming=True,\n",
    ")\n",
    "streaming_response = query_engine.query(\n",
    "    \"define a pydantic class 'ProductData' and extract the list of Product from a webpage by getting the title and content. write code.\",\n",
    ")\n",
    "streaming_response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the chromadb server process\n",
    "process.terminate()\n",
    "process.kill()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
